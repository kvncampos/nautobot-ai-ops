{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"AI Ops","text":"<p>    An App for Nautobot. </p>"},{"location":"index.html#overview","title":"Overview","text":"<p>The AI Ops plugin brings advanced artificial intelligence capabilities to Nautobot through a flexible multi-provider architecture and the Model Context Protocol (MCP). This app provides an intelligent chat assistant that can interact with your Nautobot environment, external MCP servers, and other integrated systems to help automate operational tasks, answer questions, and provide insights based on your network infrastructure data.</p> <p>At its core, AI Ops leverages LangGraph and LangChain to orchestrate conversations with Large Language Models (LLMs) from multiple providers (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, and custom providers), maintaining conversation context through checkpointed sessions stored in Redis. The plugin supports flexible LLM provider and model management, allowing administrators to define multiple providers and models for various use cases. A powerful middleware system enables request/response processing with features like caching, logging, validation, and retry logic. The multi-MCP server architecture enables the AI assistant to connect to both internal and external MCP servers, providing extensible tool access for network automation, data retrieval, and operational workflows. Enterprise features include automated health monitoring for MCP servers, middleware cache management, automatic status tracking, conversation persistence, and scheduled maintenance tasks to maintain optimal performance.</p>"},{"location":"index.html#key-features","title":"Key Features","text":"<ul> <li>Multi-Provider LLM Support: Use models from Ollama (local), OpenAI, Azure AI, Anthropic, HuggingFace, or implement custom providers</li> <li>LLM Provider Management: Configure and manage multiple LLM providers with provider-specific settings and handler classes</li> <li>LLM Model Management: Configure multiple models from different providers with varying capabilities, temperature settings, and configurations</li> <li>Middleware System: Apply middleware chains to models for caching, logging, validation, retry logic, rate limiting, and custom processing</li> <li>Priority-Based Middleware Execution: Control middleware execution order (1-100) with pre and post-processing phases</li> <li>AI Chat Assistant: Interactive chat interface that understands and responds to natural language queries about your Nautobot environment</li> <li>MCP Server Integration: Connect to internal and external Model Context Protocol servers to extend capabilities with custom tools</li> <li>Automated Health Monitoring: Scheduled health checks for MCP servers with retry logic and automatic cache invalidation</li> <li>Conversation Persistence: Checkpoint-based conversation management using Redis ensures context is maintained across sessions</li> <li>Secure Configuration: API keys and credentials managed through Nautobot's Secret objects, never stored directly</li> <li>Scheduled Tasks: Background jobs for checkpoint cleanup, MCP server health monitoring, and middleware cache management</li> <li>RESTful API: Full API support for programmatic access to all models (providers, models, middleware, MCP servers)</li> <li>Environment-Aware: Supports LAB (local development with Ollama), NONPROD, and PROD environments</li> </ul> <p>More screenshots and detailed use cases can be found in the Using the App page in the documentation.</p>"},{"location":"index.html#requirements","title":"Requirements","text":"<ul> <li>Nautobot 2.4.22+</li> <li>Python 3.10 - 3.12</li> <li>Redis (for conversation checkpointing and caching)</li> <li>At least one LLM provider:</li> <li>Ollama (local, free): For development and testing</li> <li>OpenAI API: For OpenAI models (requires API key)</li> <li>Azure OpenAI: For Azure-hosted models (requires subscription)</li> <li>Anthropic API: For Claude models (requires API key)</li> <li>HuggingFace: For HuggingFace models (requires API key)</li> <li>Custom: Implement your own provider handler</li> <li>Optional: MCP servers for extended functionality</li> </ul>"},{"location":"index.html#documentation","title":"Documentation","text":"<p>Full documentation for this App can be found over on the Nautobot Docs website:</p> <ul> <li>User Guide - Overview, Using the App, Getting Started.</li> <li>Administrator Guide - How to Install, Configure, Upgrade, or Uninstall the App.</li> <li>Developer Guide - Extending the App, Code Reference, Contribution Guide.</li> <li>Release Notes / Changelog.</li> <li>Frequently Asked Questions.</li> </ul>"},{"location":"index.html#contributing-to-the-documentation","title":"Contributing to the Documentation","text":"<p>You can find all the Markdown source for the App documentation under the <code>docs</code> folder in this repository. For simple edits, a Markdown capable editor is sufficient: clone the repository and edit away.</p> <p>If you need to view the fully-generated documentation site, you can build it with MkDocs. A container hosting the documentation can be started using the <code>invoke</code> commands (details in the Development Environment Guide) on http://localhost:8001. Using this container, as your changes to the documentation are saved, they will be automatically rebuilt and any pages currently being viewed will be reloaded in your browser.</p> <p>Any PRs with fixes or improvements are very welcome!</p>"},{"location":"index.html#questions","title":"Questions","text":"<p>For any questions or comments, please check the FAQ first. Feel free to also swing by the Network to Code Slack (channel <code>#nautobot</code>), sign up here if you don't have an account.</p>"},{"location":"admin/compatibility_matrix.html","title":"Compatibility Matrix","text":"<p>The AI Ops app is compatible with the following Nautobot versions:</p> AI Ops Version Nautobot Minimum Nautobot Maximum Status 1.0.X 2.4.0 3.x.x Tested up to 3.x.x"},{"location":"admin/compatibility_matrix.html#support-policy","title":"Support Policy","text":"<ul> <li>Minimum Version: Nautobot 2.4.0 or higher is required</li> <li>Tested Versions: Tested and verified with Nautobot 2.4.0 through 3.x.x</li> <li>Untested Versions: Compatibility with Nautobot 4.0.0 and higher is pending testing</li> </ul>"},{"location":"admin/compatibility_matrix.html#deprecation","title":"Deprecation","text":"<p>As new versions of Nautobot are released, older versions of the AI Ops app may be deprecated. Support timelines will be announced with each release.</p>"},{"location":"admin/health_checks.html","title":"MCP Server Health Checks","text":"<p>The AI Ops app includes automated health check monitoring for MCP (Model Context Protocol) servers to ensure reliable agent operations.</p>"},{"location":"admin/health_checks.html#overview","title":"Overview","text":"<p>Health checks automatically verify the operational status of HTTP MCP servers by periodically querying their health endpoints. Servers that fail health checks are automatically marked as \"Unhealthy\" and excluded from agent operations until they recover.</p>"},{"location":"admin/health_checks.html#features","title":"Features","text":"<ul> <li>Automated Scheduling: Health checks run every minute (configurable)</li> <li>Intelligent Retry Logic: Status changes require verification to prevent false positives</li> <li>Parallel Execution: Multiple servers checked simultaneously for efficiency</li> <li>Cache Invalidation: MCP client cache automatically cleared when server statuses change</li> <li>Selective Checking: Skips servers with \"Vulnerable\" status or \"stdio\" protocol</li> </ul>"},{"location":"admin/health_checks.html#health-check-process","title":"Health Check Process","text":""},{"location":"admin/health_checks.html#basic-flow","title":"Basic Flow","text":"<ol> <li>Query: Fetch all HTTP MCP servers (excluding \"Vulnerable\" status)</li> <li>Check: Send HTTP GET request to each server's health endpoint</li> <li>Evaluate: HTTP 200 response = Healthy, otherwise = Unhealthy</li> <li>Update: Change server status if needed (with verification)</li> <li>Invalidate: Clear MCP client cache if any status changed</li> </ol>"},{"location":"admin/health_checks.html#retry-logic","title":"Retry Logic","text":"<p>To prevent false positives from temporary network issues, the health check uses a verification process before changing server status:</p> <p>No Retry Scenarios (Status matches check result): - Server status is \"Healthy\" AND health check passes \u2192 No change needed - Server status is \"Unhealthy\" AND health check fails \u2192 No change needed</p> <p>Verification Required (Status differs from check result): 1. Initial health check indicates status should change 2. Wait 5 seconds, perform verification check #1 3. Wait 5 seconds, perform verification check #2 4. Evaluate all 3 checks (initial + 2 verifications) 5. If 2 out of 3 checks confirm the new status, flip the status 6. Otherwise, keep current status</p> <p>Example: If a \"Healthy\" server fails the initial check: - Wait 5s \u2192 Check again - Wait 5s \u2192 Check again - If 2 out of 3 checks failed \u2192 Mark as \"Unhealthy\" - If only 1 out of 3 checks failed \u2192 Keep \"Healthy\"</p>"},{"location":"admin/health_checks.html#parallel-execution","title":"Parallel Execution","text":"<p>Health checks use parallel processing to efficiently handle multiple servers:</p> <ul> <li>Worker Count: Minimum 2 workers, maximum 50% of available CPU cores</li> <li>Execution: Uses Python's <code>ThreadPoolExecutor</code> for concurrent checks</li> <li>Timeout: Each individual check times out after 5 seconds</li> </ul> <p>Performance Impact: - 10 servers with 4 CPU cores \u2192 Uses 2 workers (50% of 4 = 2) - 20 servers with 16 CPU cores \u2192 Uses 8 workers (50% of 16 = 8) - Worst-case runtime per server: 15 seconds (initial + 2\u00d75s verification)</p>"},{"location":"admin/health_checks.html#configuration","title":"Configuration","text":""},{"location":"admin/health_checks.html#scheduling","title":"Scheduling","text":"<p>The health check job is automatically scheduled during app migrations and runs every minute by default.</p> <p>Current Schedule (POC/Testing): <pre><code>Crontab: * * * * *\nDescription: Every minute\n</code></pre></p> <p>Production Recommendation: For production environments, update the scheduled job to run less frequently:</p> <ol> <li>Navigate to Jobs &gt; Scheduled Jobs</li> <li>Find \"MCP Server Health Check\"</li> <li>Edit the schedule</li> <li>Change crontab to: <code>*/5 * * * *</code> (every 5 minutes)</li> <li>Save changes</li> </ol> <p>Alternative Schedules: - Every 5 minutes: <code>*/5 * * * *</code> - Every 10 minutes: <code>*/10 * * * *</code> - Every 15 minutes: <code>*/15 * * * *</code> - Every 30 minutes: <code>*/30 * * * *</code></p>"},{"location":"admin/health_checks.html#server-status-configuration","title":"Server Status Configuration","text":"<p>Health checks only process servers meeting these criteria:</p> <ul> <li>Protocol: Must be <code>http</code> (STDIO servers are skipped)</li> <li>Status: Must NOT be \"Vulnerable\" (these are excluded)</li> <li>Health Endpoint: Defaults to <code>/health</code> (configurable per server)</li> </ul> <p>Status Meanings: - Healthy: Server is operational and responding to health checks - Unhealthy: Server failed health checks and is excluded from agent operations - Vulnerable: Manually set status to exclude server from health checks entirely (e.g., known security issues)</p>"},{"location":"admin/health_checks.html#health-check-endpoint","title":"Health Check Endpoint","text":"<p>Each MCP server can specify its health check endpoint path:</p> <ul> <li>Default: <code>/health</code></li> <li>Configurable: Set custom path in MCP Server configuration</li> <li>URL Construction: <code>{server.url.rstrip('/')}{server.health_check}</code></li> </ul> <p>Example: <pre><code>Server URL: https://mcp-server.internal.com\nHealth Check Path: /health\nFinal URL: https://mcp-server.internal.com/health\n</code></pre></p>"},{"location":"admin/health_checks.html#ssl-verification","title":"SSL Verification","text":"<p>SSL certificate verification behavior depends on server type:</p> <ul> <li>Internal Servers (<code>mcp_type=\"internal\"</code>): SSL verification disabled (for self-signed certs)</li> <li>External Servers (<code>mcp_type=\"external\"</code>): SSL verification enabled</li> </ul>"},{"location":"admin/health_checks.html#monitoring","title":"Monitoring","text":""},{"location":"admin/health_checks.html#job-execution","title":"Job Execution","text":"<p>Monitor health check execution through Nautobot's job system:</p> <ol> <li>Navigate to Jobs &gt; Job Results</li> <li>Filter by job name: \"MCP Server Health Check\"</li> <li>View execution logs and results</li> </ol> <p>Log Messages: - \u2705 Success: <code>Health check completed: X checked, Y changed, Z failed</code> - \u26a0\ufe0f Status Change: <code>MCP Server status changed: {name} (Healthy \u2192 Unhealthy)</code> - \u274c Failure: <code>Health check failed: {error}</code></p>"},{"location":"admin/health_checks.html#cache-invalidation","title":"Cache Invalidation","text":"<p>When server statuses change, the MCP client cache is automatically invalidated:</p> <pre><code>MCP client cache cleared due to status changes (was tracking X server(s))\n</code></pre> <p>This ensures the agent immediately picks up the new server configuration without requiring a manual cache clear.</p>"},{"location":"admin/health_checks.html#manual-health-checks","title":"Manual Health Checks","text":"<p>In addition to automated health checks, you can manually check individual servers:</p> <ol> <li>Navigate to AI Platform &gt; Configuration &gt; MCP Servers</li> <li>Click on a specific MCP server</li> <li>Click the Check Health button</li> <li>View immediate health check results</li> </ol> <p>Manual health checks: - Do NOT trigger status changes - Do NOT clear the MCP client cache - Provide immediate feedback for troubleshooting</p>"},{"location":"admin/health_checks.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin/health_checks.html#health-check-not-running","title":"Health Check Not Running","text":"<p>Verify Scheduled Job: 1. Navigate to Jobs &gt; Scheduled Jobs 2. Find \"MCP Server Health Check\" 3. Verify:    - Status is \"Enabled\"    - Crontab is correct    - Start time is in the past    - User is \"JobRunner\"</p> <p>Check Job Registration: - Verify job appears in Jobs list - Module: <code>ai_ops.jobs.mcp_health_check</code> - Class: <code>MCPServerHealthCheckJob</code></p>"},{"location":"admin/health_checks.html#server-stuck-in-unhealthy-status","title":"Server Stuck in Unhealthy Status","text":"<p>Verify Server Accessibility: <pre><code>curl -i https://mcp-server.internal.com/health\n</code></pre></p> <p>Expected response: HTTP 200</p> <p>Common Issues: - Server is actually down - Network connectivity problems - Firewall blocking Nautobot \u2192 MCP server - Health endpoint path incorrect - SSL certificate issues (for external servers)</p> <p>Temporary Override: If server is actually healthy but status is stuck: 1. Edit the MCP server 2. Manually set status to \"Healthy\" 3. Save changes 4. Monitor next health check cycle</p>"},{"location":"admin/health_checks.html#all-health-checks-failing","title":"All Health Checks Failing","text":"<p>Check Nautobot Server: - Network connectivity working? - DNS resolution working? - Firewall rules correct?</p> <p>Check Celery Workers: <pre><code># View Celery worker status\nnautobot-server celery inspect active\n\n# Check for stuck tasks\nnautobot-server celery inspect scheduled\n</code></pre></p> <p>Review Logs: <pre><code># Check Nautobot logs for health check errors\ntail -f /var/log/nautobot/nautobot.log | grep \"health check\"\n</code></pre></p>"},{"location":"admin/health_checks.html#performance-tuning","title":"Performance Tuning","text":""},{"location":"admin/health_checks.html#adjust-worker-count","title":"Adjust Worker Count","text":"<p>Worker count is calculated automatically but can be influenced by system resources:</p> <pre><code># Current formula:\nmax_workers = max(2, min(cpu_count // 2, server_count))\n</code></pre> <p>To modify (requires code change): 1. Edit <code>ai_ops/celery_tasks.py</code> 2. Find <code>perform_mcp_health_checks()</code> function 3. Adjust the worker calculation formula 4. Restart Nautobot</p>"},{"location":"admin/health_checks.html#adjust-check-frequency","title":"Adjust Check Frequency","text":"<p>For environments with many servers or slow networks, consider:</p> <ol> <li>Increase schedule interval: Every 10-15 minutes instead of every minute</li> <li>Reduce verification checks: Modify retry logic (requires code change)</li> <li>Increase timeout: Modify 5-second timeout (requires code change)</li> </ol>"},{"location":"admin/health_checks.html#exclude-servers-from-checks","title":"Exclude Servers from Checks","text":"<p>To permanently exclude a server from automated health checks:</p> <ol> <li>Edit the MCP server</li> <li>Set status to \"Vulnerable\"</li> <li>Save changes</li> </ol> <p>The server will be skipped in all future automated health checks.</p>"},{"location":"admin/health_checks.html#api-integration","title":"API Integration","text":""},{"location":"admin/health_checks.html#celery-task-invocation","title":"Celery Task Invocation","text":"<p>You can programmatically trigger health checks:</p> <pre><code>from ai_ops.celery_tasks import perform_mcp_health_checks\n\n# Trigger health checks\nresult = perform_mcp_health_checks()\n\n# Check results\nif result['success']:\n    print(f\"Checked: {result['checked_count']}\")\n    print(f\"Changed: {result['changed_count']}\")\n    print(f\"Failed: {result['failed_count']}\")\n    print(f\"Cache cleared: {result['cache_cleared']}\")\n</code></pre>"},{"location":"admin/health_checks.html#single-server-check","title":"Single Server Check","text":"<p>Check a specific server:</p> <pre><code>from ai_ops.celery_tasks import check_mcp_server_health\n\n# Check server by ID\nresult = check_mcp_server_health(server_id='uuid-here')\n\nif result['success']:\n    if result['status_changed']:\n        print(f\"Status changed: {result['old_status']} \u2192 {result['new_status']}\")\n    else:\n        print(f\"Status unchanged: {result['new_status']}\")\n</code></pre>"},{"location":"admin/health_checks.html#best-practices","title":"Best Practices","text":"<ol> <li>Production Scheduling: Use 5-15 minute intervals instead of every minute</li> <li>Monitor Failures: Review job results regularly for persistent failures</li> <li>Use Vulnerable Status: For servers with known issues that shouldn't be checked</li> <li>Test Health Endpoints: Manually verify health endpoints work before registering servers</li> <li>SSL Certificates: Use valid certificates for external servers, or mark as internal</li> <li>Network Requirements: Ensure Nautobot can reach all MCP server URLs</li> <li>Resource Planning: More servers = more workers = more CPU usage during checks</li> </ol>"},{"location":"admin/health_checks.html#related-documentation","title":"Related Documentation","text":"<ul> <li>MCP Server Configuration</li> <li>External Interactions</li> <li>Job Scheduling</li> <li>Celery Configuration</li> </ul>"},{"location":"admin/install.html","title":"Installing the App in Nautobot","text":"<p>Here you will find detailed instructions on how to install and configure the AI Ops App within your Nautobot environment.</p>"},{"location":"admin/install.html#prerequisites","title":"Prerequisites","text":"<ul> <li>The app is compatible with Nautobot 2.4.0 and higher (tested up to 3.x.x).</li> <li>Databases supported: PostgreSQL, MySQL</li> <li>Redis instance required for conversation checkpointing and LangGraph integration</li> <li>At least one LLM provider configured (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, or Custom)</li> </ul> <p>Note</p> <p>Please check the dedicated page for a full compatibility matrix and the deprecation policy.</p>"},{"location":"admin/install.html#access-requirements","title":"Access Requirements","text":"<p>The app requires external access to:</p> <ul> <li>LLM Providers: Depending on your configured provider</li> <li>Azure OpenAI: <code>*.openai.azure.com</code> (HTTPS)</li> <li>OpenAI: <code>api.openai.com</code> (HTTPS)</li> <li>Anthropic: <code>api.anthropic.com</code> (HTTPS)</li> <li>HuggingFace: <code>api-inference.huggingface.co</code> (HTTPS)</li> <li>Ollama: Local or network-accessible Ollama instance</li> <li> <p>Custom: Your custom LLM provider endpoint</p> </li> <li> <p>MCP Servers (if configured): Depends on your MCP server setup</p> </li> <li>HTTP/STDIO based servers as configured</li> </ul>"},{"location":"admin/install.html#install-guide","title":"Install Guide","text":"<p>Note</p> <p>Apps can be installed from the Python Package Index or locally. See the Nautobot documentation for more details. The pip package name for this app is <code>ai-ops</code>.</p> <p>The app is available as a Python package via PyPI and can be installed with <code>pip</code>:</p> <pre><code>pip install ai-ops\n</code></pre> <p>To ensure AI Ops is automatically re-installed during future upgrades, create a file named <code>local_requirements.txt</code> (if not already existing) in the Nautobot root directory (alongside <code>requirements.txt</code>) and list the <code>ai-ops</code> package:</p> <pre><code>echo ai-ops &gt;&gt; local_requirements.txt\n</code></pre> <p>Once installed, the app needs to be enabled in your Nautobot configuration. The following block of code below shows the additional configuration required to be added to your <code>nautobot_config.py</code> file:</p> <ul> <li>Append <code>\"ai_ops\"</code> to the <code>PLUGINS</code> list.</li> <li>Append the <code>\"ai_ops\"</code> dictionary to the <code>PLUGINS_CONFIG</code> dictionary and override any defaults.</li> </ul> <pre><code># In your nautobot_config.py\nPLUGINS = [\"ai_ops\"]\n\n# PLUGINS_CONFIG = {\n#   \"ai_ops\": {\n#     ADD YOUR SETTINGS HERE\n#   }\n# }\n</code></pre> <p>Once the Nautobot configuration is updated, run the Post Upgrade command (<code>nautobot-server post_upgrade</code>) to run migrations and clear any cache:</p> <pre><code>nautobot-server post_upgrade\n</code></pre> <p>Then restart (if necessary) the Nautobot services which may include:</p> <ul> <li>Nautobot</li> <li>Nautobot Workers</li> <li>Nautobot Scheduler</li> </ul> <pre><code>sudo systemctl restart nautobot nautobot-worker nautobot-scheduler\n</code></pre>"},{"location":"admin/install.html#app-configuration","title":"App Configuration","text":"<p>The AI Ops App requires minimal configuration in <code>nautobot_config.py</code>. The app uses Nautobot's existing infrastructure (PostgreSQL, Redis) and doesn't require additional plugin settings.</p> <pre><code># In your nautobot_config.py\nPLUGINS = [\"ai_ops\"]\n\n# PLUGINS_CONFIG = {\n#   \"ai_ops\": {\n#     # No additional configuration required\n#   }\n# }\n</code></pre>"},{"location":"admin/install.html#database-configuration","title":"Database Configuration","text":"<p>The app automatically creates all required tables during the migration process. No manual database configuration is needed beyond Nautobot's standard setup.</p>"},{"location":"admin/install.html#redis-configuration","title":"Redis Configuration","text":"<p>The app uses Redis for conversation checkpointing through LangGraph. Ensure Redis is configured in your <code>nautobot_config.py</code>:</p> <pre><code># Redis configuration (shared with Nautobot)\nCACHES = {\n    \"default\": {\n        \"BACKEND\": \"django_redis.cache.RedisCache\",\n        \"LOCATION\": \"redis://localhost:6379/0\",\n        \"OPTIONS\": {\n            \"CLIENT_CLASS\": \"django_redis.client.DefaultClient\",\n        },\n    }\n}\n\n# Celery configuration (uses Redis)\nCELERY_BROKER_URL = \"redis://localhost:6379/1\"\n</code></pre>"},{"location":"admin/install.html#environment-variables","title":"Environment Variables","text":"<p>For production deployments, configure the following environment variables:</p> Variable Required Description Example <code>NAUTOBOT_REDIS_HOST</code> Yes Redis server hostname <code>localhost</code> or <code>redis.internal.com</code> <code>NAUTOBOT_REDIS_PORT</code> Yes Redis server port <code>6379</code> <code>NAUTOBOT_REDIS_PASSWORD</code> No Redis password (if required) <code>your-secure-password</code> <code>LANGGRAPH_REDIS_DB</code> No Redis database number for checkpoints <code>2</code> (default)"},{"location":"admin/install.html#lab-environment-variables-development-only","title":"LAB Environment Variables (Development Only)","text":"<p>For local development (LAB environment), you can use environment variables instead of database configuration:</p> Variable Required Description Example <code>AZURE_OPENAI_API_KEY</code> Yes (LAB) Azure OpenAI API key <code>your-api-key</code> <code>AZURE_OPENAI_ENDPOINT</code> Yes (LAB) Azure OpenAI endpoint URL <code>https://your-resource.openai.azure.com/</code> <code>AZURE_OPENAI_DEPLOYMENT_NAME</code> Yes (LAB) Model deployment name <code>gpt-4o</code> <code>AZURE_OPENAI_API_VERSION</code> No API version <code>2024-02-15-preview</code> <p>Production Configuration</p> <p>In production (NONPROD/PROD environments), LLM models should be configured through the Nautobot UI rather than environment variables. See Post-Installation Configuration for configuration steps.</p>"},{"location":"admin/install.html#post-installation-configuration","title":"Post-Installation Configuration","text":"<p>After installing the app, you need to configure the LLM providers and models. The configuration follows this hierarchy:</p> <ol> <li>LLM Providers - Define available provider types (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, Custom)</li> <li>LLM Models - Create specific model deployments using a provider</li> <li>Middleware Types (Optional) - Define middleware for request/response processing</li> <li>LLM Middleware (Optional) - Apply middleware to specific models</li> <li>MCP Servers (Optional) - Configure Model Context Protocol servers for extended capabilities</li> </ol>"},{"location":"admin/install.html#1-create-llm-provider","title":"1. Create LLM Provider","text":"<p>First, define which LLM provider you'll use:</p> <ol> <li>Navigate to AI Platform &gt; LLM &gt; LLM Providers</li> <li>Click + Add</li> <li>Select a provider:</li> <li>Name: Choose from Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, or Custom</li> <li>Description: Optional description of the provider setup</li> <li>Documentation URL: Optional link to provider documentation</li> <li>Config Schema: Provider-specific configuration (e.g., Azure API version, base URL)</li> <li>Is Enabled: Check to enable this provider</li> </ol>"},{"location":"admin/install.html#2-create-llm-model","title":"2. Create LLM Model","text":"<p>Create at least one LLM Model for your selected provider:</p> <ol> <li>Navigate to AI Platform &gt; LLM &gt; LLM Models</li> <li>Click + Add</li> <li>Configure:</li> <li>LLM Provider: Select the provider created in step 1</li> <li>Name: Model name (e.g., <code>gpt-4o</code>, <code>llama2</code>, or your Azure deployment name)</li> <li>Description: Optional description of the model's capabilities</li> <li>Model Secret Key: (Production only) Name of Nautobot Secret containing API key</li> <li>Endpoint: Model endpoint URL (required for some providers)</li> <li>API Version: API version string (e.g., <code>2024-02-15-preview</code> for Azure)</li> <li>Is Default: Check to make this the default model</li> <li>Temperature: Model temperature setting (0.0-2.0, default 0.0)</li> <li>Cache TTL: Cache duration in seconds (minimum 60)</li> </ol>"},{"location":"admin/install.html#3-create-secrets-production-only","title":"3. Create Secrets (Production Only)","text":"<p>For production deployments, store API keys securely in Nautobot Secrets:</p> <ol> <li>Navigate to Secrets &gt; Secrets</li> <li>Click + Add</li> <li>Create a new Secret:</li> <li>Name: <code>azure_gpt4_api_key</code> (or your chosen name - must match Model Secret Key)</li> <li>Provider: Choose appropriate provider</li> <li>Value: Your Azure OpenAI API key (or other provider credentials)</li> <li>Reference this Secret name in your LLM Model's Model Secret Key field</li> </ol>"},{"location":"admin/install.html#4-configure-middleware-types-optional","title":"4. Configure Middleware Types (Optional)","text":"<p>To add request/response processing capabilities:</p> <ol> <li>Navigate to AI Platform &gt; Middleware &gt; Middleware Types</li> <li>Click + Add</li> <li>Define middleware:</li> <li>Name: Middleware class name (e.g., <code>SummarizationMiddleware</code>, auto-suffixed with \"Middleware\")</li> <li>Is Custom: Check if this is a custom middleware (unchecked for built-in LangChain middleware)</li> <li>Description: What this middleware does</li> </ol>"},{"location":"admin/install.html#5-apply-llm-middleware-optional","title":"5. Apply LLM Middleware (Optional)","text":"<p>Apply middleware to specific models:</p> <ol> <li>Navigate to AI Platform &gt; Middleware &gt; LLM Middleware</li> <li>Click + Add</li> <li>Configure:</li> <li>LLM Model: Select the model to apply middleware to</li> <li>Middleware: Select the middleware type</li> <li>Config: JSON configuration for the middleware</li> <li>Config Version: LangChain version compatibility (default: 1.1.0)</li> <li>Is Active: Check to enable this middleware</li> <li>Is Critical: Check if initialization should fail if this middleware fails</li> <li>Priority: Execution order (1-100, lower executes first)</li> <li>TTL: Time-to-live for middleware data in seconds</li> </ol>"},{"location":"admin/install.html#6-configure-mcp-servers-optional","title":"6. Configure MCP Servers (Optional)","text":"<p>To extend agent capabilities with Model Context Protocol servers:</p> <ol> <li>Navigate to AI Platform &gt; Middleware &gt; MCP Servers</li> <li>Click + Add</li> <li>Configure:</li> <li>Name: Unique name for the server</li> <li>Status: Set to Active (or use other status options)</li> <li>Protocol: STDIO or HTTP</li> <li>URL: Base URL for the MCP server</li> <li>MCP Endpoint: Path to MCP endpoint (default: <code>/mcp</code>)</li> <li>Health Check: Path to health check endpoint (default: <code>/health</code>)</li> <li>Description: Optional description</li> <li>MCP Type: Internal or External</li> <li>Click the health check button to verify server connectivity</li> </ol>"},{"location":"admin/install.html#7-schedule-cleanup-job-recommended","title":"7. Schedule Cleanup Job (Recommended)","text":"<p>To prevent Redis checkpoint storage from growing indefinitely:</p> <ol> <li>Navigate to Jobs &gt; Jobs</li> <li>Find AI Agents &gt; Cleanup Old Checkpoints</li> <li>Click Schedule Job</li> <li>Configure to run daily or weekly</li> </ol>"},{"location":"admin/install.html#configuration-examples","title":"Configuration Examples","text":""},{"location":"admin/install.html#local-development-lab-environment","title":"Local Development (LAB Environment)","text":"<p>For local development, you can skip database configuration and use environment variables:</p> <pre><code># .env file\nAZURE_OPENAI_API_KEY=your-api-key\nAZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\nAZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o\nAZURE_OPENAI_API_VERSION=2024-02-15-preview\n</code></pre> <p>The app automatically detects LAB environment based on hostname and uses these variables.</p>"},{"location":"admin/install.html#production-nonprodprod-environment","title":"Production (NONPROD/PROD Environment)","text":"<p>For production deployments: - Configure LLM Providers and Models through the Nautobot UI - Store API keys in Nautobot Secrets (not environment variables) - Configure Redis for conversation checkpointing - Set up multiple models for redundancy</p> <p>See Getting Started for additional configuration instructions and best practices.</p>"},{"location":"admin/uninstall.html","title":"Uninstall the App from Nautobot","text":"<p>Here you will find any steps necessary to cleanly remove the App from your Nautobot environment.</p>"},{"location":"admin/uninstall.html#database-cleanup","title":"Database Cleanup","text":"<p>Prior to removing the app from the <code>nautobot_config.py</code>, run the following command to roll back any migration specific to this app.</p> <pre><code>nautobot-server migrate ai_ops zero\n</code></pre> <p>Database Objects Removed</p> <p>The migration rollback will remove all AI Ops-specific database objects including: - LLM Providers, Models, and Middleware configurations - MCP Server configurations - Conversation checkpoints stored in Redis - Any custom webhooks or relationships tied to AI Ops models</p>"},{"location":"admin/uninstall.html#clear-redis-cache","title":"Clear Redis Cache","text":"<p>After the migration, clear any remaining conversation checkpoints from Redis:</p> <pre><code># Clear the LangGraph checkpoint database\nredis-cli -n 2 FLUSHDB\n</code></pre>"},{"location":"admin/uninstall.html#remove-app-configuration","title":"Remove App configuration","text":"<p>Remove the configuration you added in <code>nautobot_config.py</code> from <code>PLUGINS</code> &amp; <code>PLUGINS_CONFIG</code>.</p>"},{"location":"admin/uninstall.html#uninstall-the-package","title":"Uninstall the package","text":"<pre><code>$ pip3 uninstall ai-ops\n</code></pre>"},{"location":"admin/upgrade.html","title":"Upgrading the App","text":"<p>Here you will find any steps necessary to upgrade the App in your Nautobot environment.</p>"},{"location":"admin/upgrade.html#upgrade-guide","title":"Upgrade Guide","text":"<p>When a new release of the AI Ops app is available, follow these steps to upgrade:</p>"},{"location":"admin/upgrade.html#1-update-the-package","title":"1. Update the Package","text":"<p>Upgrade the <code>ai-ops</code> package using pip:</p> <pre><code>pip install --upgrade ai-ops\n</code></pre>"},{"location":"admin/upgrade.html#2-run-migrations","title":"2. Run Migrations","text":"<p>Execute the post-upgrade command to run any database migrations:</p> <pre><code>nautobot-server post_upgrade\n</code></pre> <p>This command will: - Apply any new database migrations - Update the schema for LLM models, middleware, and MCP configurations - Clear caches and regenerate static files</p>"},{"location":"admin/upgrade.html#3-restart-services","title":"3. Restart Services","text":"<p>Restart the Nautobot services to load the updated app:</p> <pre><code>sudo systemctl restart nautobot nautobot-worker nautobot-scheduler\n</code></pre>"},{"location":"admin/upgrade.html#4-verify-configuration","title":"4. Verify Configuration","text":"<p>After upgrading, verify that: - All LLM models are still configured correctly - Middleware configurations are intact - MCP servers are still responding to health checks - The cleanup job is still scheduled (if it was previously configured)</p>"},{"location":"admin/upgrade.html#breaking-changes","title":"Breaking Changes","text":"<p>Refer to the release notes for any breaking changes or required configuration updates for the specific version you're upgrading to.</p>"},{"location":"admin/release_notes/index.html","title":"Release Notes","text":"<p>All the published release notes can be found via the navigation menu. All patch releases are included in the same minor release (e.g. <code>v1.2</code>) document.</p>"},{"location":"admin/release_notes/version_1.0.html","title":"v1.0 Release Notes","text":"<p>This document describes all new features and changes in the release. The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"admin/release_notes/version_1.0.html#release-overview","title":"Release Overview","text":"<p>AI Ops v1.0.0 is the inaugural stable release of the AI Operations application for Nautobot, featuring comprehensive LLM provider integration and multi-agent capabilities.</p>"},{"location":"admin/release_notes/version_1.0.html#key-features","title":"Key Features","text":"<ul> <li>Multi-Provider LLM Support: Seamless integration with Azure OpenAI, OpenAI, Anthropic, HuggingFace, Ollama, and custom providers</li> <li>LLM Provider Configuration: Database-driven provider and model management with dynamic configuration</li> <li>Middleware Architecture: Request/response processing middleware with priority-based execution</li> <li>MCP Server Integration: Model Context Protocol server support for extended agent capabilities</li> <li>Conversation Checkpointing: Redis-backed conversation state persistence using LangGraph</li> <li>Rest API: Comprehensive REST API for LLM operations and model management</li> <li>Health Checks: Built-in health check system for MCP servers and LLM availability</li> </ul>"},{"location":"admin/release_notes/version_1.0.html#compatibility","title":"Compatibility","text":"<ul> <li>Nautobot: 2.4.0 to 3.x.x (tested up to 3.x.x, 4.0.0+ pending testing)</li> <li>Python: 3.10, 3.11, 3.12, 3.13</li> <li>Databases: PostgreSQL, MySQL</li> </ul>"},{"location":"admin/release_notes/version_1.0.html#v100-2025-12-19","title":"v1.0.0 (2025-12-19)","text":"<p>No significant changes.</p>"},{"location":"dev/arch_decision.html","title":"Architecture Decision Records","text":"<p>This section documents architectural decisions and design patterns used in the AI Ops app that deviate from standard patterns or require explanation.</p>"},{"location":"dev/architecture.html","title":"Architecture Overview","text":"<p>This document provides a comprehensive overview of the AI Ops App architecture.</p>"},{"location":"dev/architecture.html#high-level-architecture","title":"High-Level Architecture","text":"<p>The AI Ops App integrates multiple LLM providers with Nautobot through a multi-layered architecture with middleware support:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        User Interface                         \u2502\n\u2502  (Web UI / REST API / Chat Interface)                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Nautobot Plugin Layer                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   Views      \u2502  \u2502     API      \u2502  \u2502    Jobs      \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                  \u2502                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Application Layer                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502  AI Agents   \u2502  \u2502   Models     \u2502  \u2502   Helpers    \u2502       \u2502\n\u2502  \u2502  (LangGraph) \u2502  \u2502 (LLMProvider,\u2502  \u2502(get_llm_     \u2502       \u2502\n\u2502  \u2502  + Middleware\u2502  \u2502  LLMModel,   \u2502  \u2502 model,       \u2502       \u2502\n\u2502  \u2502              \u2502  \u2502  Middleware, \u2502  \u2502 middleware)  \u2502       \u2502\n\u2502  \u2502              \u2502  \u2502  MCPServer)  \u2502  \u2502              \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                  \u2502                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Integration Layer                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502  LangChain   \u2502  \u2502    Redis     \u2502  \u2502  PostgreSQL  \u2502       \u2502\n\u2502  \u2502   (MCP)      \u2502  \u2502(Checkpoints, \u2502  \u2502  (Models)    \u2502       \u2502\n\u2502  \u2502              \u2502  \u2502  Middleware  \u2502  \u2502              \u2502       \u2502\n\u2502  \u2502              \u2502  \u2502   Cache)     \u2502  \u2502              \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   External Services                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502     LLM Providers (Multi-Provider Support)      \u2502          \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502          \u2502\n\u2502  \u2502  \u2502 Ollama \u2502  \u2502 OpenAI \u2502  \u2502Azure AI\u2502  \u2502Anthropic\u2502          \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502          \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502          \u2502\n\u2502  \u2502  \u2502HuggingF\u2502  \u2502 Custom \u2502                        \u2502          \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                             \u2502\n\u2502  \u2502 MCP Servers  \u2502                                             \u2502\n\u2502  \u2502 (Tools/Ctx)  \u2502                                             \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"dev/architecture.html#middleware-architecture","title":"Middleware Architecture","text":"<p>The app supports a flexible middleware system that processes requests before and after they reach the LLM:</p> <pre><code>User Request\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        Middleware Chain               \u2502\n\u2502  (Executed in Priority Order 1-100)  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Priority 10: LoggingMiddleware      \u2502 \u2190 Log request\n\u2502  Priority 20: CacheMiddleware        \u2502 \u2190 Check cache\n\u2502  Priority 30: RetryMiddleware        \u2502 \u2190 Retry logic\n\u2502  Priority 40: ValidationMiddleware   \u2502 \u2190 Validate input\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              LLM Model                \u2502 \u2190 Process request\n\u2502      (Ollama/OpenAI/Azure/etc)       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Priority 40: ValidationMiddleware   \u2502 \u2190 Validate output\n\u2502  Priority 30: RetryMiddleware        \u2502 \u2190 (if needed)\n\u2502  Priority 20: CacheMiddleware        \u2502 \u2190 Store in cache\n\u2502  Priority 10: LoggingMiddleware      \u2502 \u2190 Log response\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\nResponse to User\n</code></pre>"},{"location":"dev/architecture.html#component-architecture","title":"Component Architecture","text":""},{"location":"dev/architecture.html#1-user-interface-layer","title":"1. User Interface Layer","text":""},{"location":"dev/architecture.html#web-ui","title":"Web UI","text":"<ul> <li>Chat Interface: <code>/plugins/ai-ops/chat/</code> - Interactive chat widget</li> <li>Provider Management: List, create, edit, delete LLM providers</li> <li>Model Management: List, create, edit, delete LLM models</li> <li>Middleware Management: Configure middleware for models</li> <li>Server Management: Configure and monitor MCP servers</li> <li>Navigation: Integrated into Nautobot's navigation menu</li> </ul>"},{"location":"dev/architecture.html#rest-api","title":"REST API","text":"<ul> <li>LLM Providers API: <code>/api/plugins/ai-ops/llm-providers/</code></li> <li>LLM Models API: <code>/api/plugins/ai-ops/llm-models/</code></li> <li>Middleware Types API: <code>/api/plugins/ai-ops/middleware-types/</code></li> <li>LLM Middleware API: <code>/api/plugins/ai-ops/llm-middleware/</code></li> <li>MCP Servers API: <code>/api/plugins/ai-ops/mcp-servers/</code></li> <li>Chat API: <code>/plugins/ai-ops/api/chat/</code> - Programmatic chat access</li> </ul>"},{"location":"dev/architecture.html#2-application-layer","title":"2. Application Layer","text":""},{"location":"dev/architecture.html#ai-agents","title":"AI Agents","text":"<p>Multi-MCP Agent (<code>ai_ops/agents/multi_mcp_agent.py</code>): - Production-ready agent implementation - Supports multiple MCP servers simultaneously - Application-level caching for performance - Health-based server selection - LangGraph state management - Middleware integration</p> <p>Single-MCP Agent (<code>ai_ops/agents/single_mcp_agent.py</code>): - Simplified single-server implementation - Development and testing scenarios</p> <p>Agent Features: - Conversation history via checkpointing - Tool discovery from MCP servers - Multi-provider LLM support (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace) - Middleware chain execution - Async/await architecture</p>"},{"location":"dev/architecture.html#models","title":"Models","text":"<p>LLMProvider: - Defines available LLM providers (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, Custom) - Stores provider-specific configuration in JSON schema - Has corresponding provider handler classes - Enable/disable providers without deletion</p> <p>LLMModel: - Stores model configurations for any supported provider - Environment-aware (LAB/NONPROD/PROD) - Integrates with Nautobot Secrets - Supports default model selection - Can have multiple middleware configurations - References LLMProvider via foreign key</p> <p>MiddlewareType: - Defines middleware types (built-in LangChain or custom) - Reusable across multiple models - Name validation and formatting</p> <p>LLMMiddleware: - Configures middleware instances for specific models - Priority-based execution order (1-100) - JSON configuration for flexibility - Active/inactive toggle - Critical flag for initialization requirements</p> <p>MCPServer: - Stores MCP server configurations - Health status tracking with automated checks - Protocol support (HTTP/STDIO) - Type classification (Internal/External)</p>"},{"location":"dev/architecture.html#helpers","title":"Helpers","text":"<p>get_llm_model: - Environment detection - Model configuration retrieval - Azure OpenAI client creation - Sync and async variants</p> <p>get_info: - Status retrieval utilities - Default value providers</p> <p>Serializers: - LangGraph checkpoint serialization - Custom data type handling</p>"},{"location":"dev/architecture.html#3-integration-layer","title":"3. Integration Layer","text":""},{"location":"dev/architecture.html#langchain-langgraph","title":"LangChain &amp; LangGraph","text":"<p>LangChain: - Azure OpenAI integration - Message handling - Tool abstraction</p> <p>LangGraph: - State graph workflow - Checkpointing system - Conditional routing - Tool node execution</p> <p>MCP Integration: - <code>langchain-mcp-adapters</code>: MCP client library - <code>MultiServerMCPClient</code>: Multi-server support - Tool discovery and execution</p>"},{"location":"dev/architecture.html#redis","title":"Redis","text":"<p>Purpose: Conversation checkpoint storage</p> <p>Configuration: - Database: Separate from cache/Celery (default DB 2) - Key Pattern: <code>checkpoint:{thread_id}:{checkpoint_id}</code> - TTL: Managed by cleanup job</p> <p>Data Stored: - Conversation messages - Agent state - Metadata (timestamps, user info)</p>"},{"location":"dev/architecture.html#postgresql","title":"PostgreSQL","text":"<p>Purpose: Application data storage</p> <p>Tables: - <code>ai_ops_llmmodel</code>: LLM configurations - <code>ai_ops_mcpserver</code>: MCP server configurations - Plus standard Nautobot tables (secrets, statuses, etc.)</p>"},{"location":"dev/architecture.html#4-external-services","title":"4. External Services","text":""},{"location":"dev/architecture.html#azure-openai","title":"Azure OpenAI","text":"<p>Service: Microsoft Azure OpenAI Service</p> <p>LLM Provider Support: - Ollama: Local open-source models - OpenAI: GPT-4, GPT-4o, GPT-3.5-turbo - Azure AI: Azure OpenAI Service - Anthropic: Claude models - HuggingFace: HuggingFace Hub models - Custom: Extensible provider system</p> <p>Communication: - HTTPS REST API - API Key authentication (via Secrets) - Provider-specific endpoints - Handler-based initialization</p>"},{"location":"dev/architecture.html#mcp-servers","title":"MCP Servers","text":"<p>Purpose: Extend agent capabilities with custom tools</p> <p>Types: - Internal: Hosted within infrastructure - External: Third-party services</p> <p>Protocols: - HTTP: RESTful MCP servers - STDIO: Process-based servers</p> <p>Health Monitoring: - Automatic health checks via scheduled job - Status field in database - Failed servers excluded from operations - Parallel health checking with retry logic</p>"},{"location":"dev/architecture.html#data-flow","title":"Data Flow","text":""},{"location":"dev/architecture.html#chat-message-flow-with-middleware","title":"Chat Message Flow with Middleware","text":"<pre><code>1. User submits message\n   \u2193\n2. ChatMessageView receives request\n   \u2193\n3. Session ID retrieved (thread_id)\n   \u2193\n4. process_message() called\n   \u2193\n5. MCP client cache checked/created\n   \u2193\n6. LLM model configuration retrieved\n   \u2193\n7. Middleware cache checked\n   \u2193\n8. Middleware chain initialized (priority order)\n   \u2193\n9. LangGraph state graph created with middleware\n   \u2193\n10. Message added to state\n    \u2193\n11. Middleware pre-processing (Priority 1 \u2192 100)\n    \u2193\n12. Agent processes message\n    \u2193\n13. LLM provider handler creates model instance\n    \u2193\n14. Model processes request\n    \u2193\n15. Tools invoked if needed (via MCP)\n    \u2193\n16. Middleware post-processing (Priority 100 \u2192 1)\n    \u2193\n17. Response generated by LLM\n    \u2193\n18. State persisted to Redis\n    \u2193\n19. Response returned to user\n</code></pre>"},{"location":"dev/architecture.html#provider-selection-flow","title":"Provider Selection Flow","text":"<pre><code>1. Get LLM model (by name or default)\n   \u2193\n2. Load model's provider relationship\n   \u2193\n3. Get provider handler from registry\n   \u2193\n4. Retrieve provider config_schema from database\n   \u2193\n5. Get model's API key from Secret\n   \u2193\n6. Provider handler initializes LLM\n   \u2502\n   \u251c\u2500 Ollama: ChatOllama(base_url, model_name)\n   \u251c\u2500 OpenAI: ChatOpenAI(api_key, model_name)\n   \u251c\u2500 Azure AI: AzureChatOpenAI(api_key, endpoint, deployment)\n   \u251c\u2500 Anthropic: ChatAnthropic(api_key, model_name)\n   \u251c\u2500 HuggingFace: ChatHuggingFace(api_key, model_name)\n   \u2514\u2500 Custom: CustomHandler(config, api_key)\n   \u2193\n7. Return initialized chat model instance\n</code></pre>"},{"location":"dev/architecture.html#middleware-execution-flow","title":"Middleware Execution Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Request from Agent                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Load Model's Middleware Configurations    \u2502\n\u2502  - Query LLMMiddleware.objects             \u2502\n\u2502  - Filter: is_active=True                  \u2502\n\u2502  - Order by: priority, middleware__name    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Initialize Middleware Chain               \u2502\n\u2502  For each middleware (priority order):     \u2502\n\u2502    1. Load middleware type                 \u2502\n\u2502    2. Get configuration JSON               \u2502\n\u2502    3. Initialize middleware instance       \u2502\n\u2502    4. Add to chain                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Pre-Processing Phase                      \u2502\n\u2502  (Priority 1 \u2192 100)                        \u2502\n\u2502                                            \u2502\n\u2502  Priority 10: LoggingMiddleware            \u2502\n\u2502    - Log request timestamp                 \u2502\n\u2502    - Log user info and message             \u2502\n\u2502                                            \u2502\n\u2502  Priority 20: CacheMiddleware              \u2502\n\u2502    - Check if response cached              \u2502\n\u2502    - If cached: return cached response     \u2502\n\u2502    - If not: continue chain                \u2502\n\u2502                                            \u2502\n\u2502  Priority 30: ValidationMiddleware         \u2502\n\u2502    - Validate input format                 \u2502\n\u2502    - Check for malicious content           \u2502\n\u2502    - Sanitize input if needed              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LLM Processing                            \u2502\n\u2502  - Model generates response                \u2502\n\u2502  - Tools invoked if needed                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Post-Processing Phase                     \u2502\n\u2502  (Priority 100 \u2192 1)                        \u2502\n\u2502                                            \u2502\n\u2502  Priority 30: ValidationMiddleware         \u2502\n\u2502    - Validate output format                \u2502\n\u2502    - Check for sensitive data              \u2502\n\u2502    - Filter response if needed             \u2502\n\u2502                                            \u2502\n\u2502  Priority 20: CacheMiddleware              \u2502\n\u2502    - Store response in cache               \u2502\n\u2502    - Set TTL from middleware config        \u2502\n\u2502                                            \u2502\n\u2502  Priority 10: LoggingMiddleware            \u2502\n\u2502    - Log response timestamp                \u2502\n\u2502    - Log token usage and latency           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Response to User                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"dev/architecture.html#chat-message-flow","title":"Chat Message Flow","text":"<p>\u2193 10. Tools invoked if needed (via MCP)     \u2193 11. Response generated by LLM     \u2193 12. State persisted to Redis     \u2193 13. Response returned to user <pre><code>### Model Configuration Flow\n\n**LAB Environment**:\n</code></pre> get_llm_model()   \u2193 Detect environment \u2192 \"LAB\"   \u2193 Read environment variables   \u2193 Create model via default provider (typically Ollama)   \u2193 Return model <pre><code>**Production Environment**:\n</code></pre> get_llm_model()   \u2193 Detect environment \u2192 \"PROD\"   \u2193 Query LLMModel.get_default_model()   \u2193 Load model's provider relationship   \u2193 Get provider handler from registry   \u2193 Retrieve Secret for API key   \u2193 Build configuration dict from provider config_schema   \u2193 Provider handler creates model instance   \u2193 Return model <pre><code>### MCP Server Discovery Flow\n</code></pre> 1. App startup or cache expiry    \u2193 2. Query MCPServer.objects.filter(status__name=\"Active\")    \u2193 3. Build connections dict    \u2193 4. Create MultiServerMCPClient    \u2193 5. Discover tools from each server    \u2193 6. Cache client and tools (5 min TTL)    \u2193 7. Tools available to agent <pre><code>### Middleware Cache Flow\n</code></pre> 1. Agent needs to process request    \u2193 2. Check middleware cache for model    \u2193 3. If cache miss:    \u2502  a. Query LLMMiddleware for model    \u2502  b. Filter: is_active=True    \u2502  c. Order by: priority, middleware__name    \u2502  d. Initialize each middleware with config    \u2502  e. Store in cache with TTL    \u2193 4. If cache hit:    \u2502  a. Validate cache not expired    \u2502  b. Return cached middleware chain    \u2193 5. Apply middleware chain to request <pre><code>### Health Check Flow (Scheduled Job)\n</code></pre> 1. MCPServerHealthCheckJob triggered (scheduled)    \u2193 2. Query all HTTP MCP servers (exclude STDIO, Vulnerable)    \u2193 3. Parallel execution (ThreadPoolExecutor, max 4 workers)    \u2502  For each server:    \u2502    a. Send GET to {url}{health_check}    \u2502    b. If status differs from database:    \u2502       - Wait 5 seconds    \u2502       - Verify (check #1)    \u2502       - Wait 5 seconds    \u2502       - Verify (check #2)    \u2502       - If both confirm: update database    \u2193 4. If any status changed:    \u2502  a. Clear MCP client cache    \u2502  b. Log cache invalidation    \u2193 5. Return summary:    \u2502  - checked_count    \u2502  - changed_count    \u2502  - failed_count    \u2502  - cache_cleared <pre><code>## State Management\n\n### Conversation State\n\n**Storage**: Redis checkpoints via LangGraph\n\n**State Structure**:\n```python\n{\n    \"messages\": [\n        HumanMessage(content=\"User question\"),\n        AIMessage(content=\"AI response\"),\n        ...\n    ]\n}\n</code></pre></p> <p>Thread Isolation: - Each session has unique thread_id - Sessions don't interfere - Parallel conversations supported</p> <p>Persistence: - Automatic via LangGraph checkpointer - Survives application restarts - Cleaned up by maintenance job</p>"},{"location":"dev/architecture.html#application-state","title":"Application State","text":"<p>MCP Client Cache: <pre><code>{\n    \"client\": MultiServerMCPClient,\n    \"tools\": [Tool1, Tool2, ...],\n    \"timestamp\": datetime,\n    \"server_count\": int\n}\n</code></pre></p> <p>MCP Cache Invalidation: - Time-based (5 minute TTL) - Manual refresh available - Server status changes trigger refresh (via health check job)</p> <p>Middleware Cache: <pre><code>{\n    model_id: {\n        \"middlewares\": [Middleware1, Middleware2, ...],\n        \"timestamp\": datetime,\n        \"config_hashes\": {middleware_id: hash, ...}\n    }\n}\n</code></pre></p> <p>Middleware Cache Invalidation: - Automatic via JobHookReceiver on LLMMiddleware changes - Cache cleared when middleware created/updated/deleted - Cache warmed when new default model set - Ensures middleware changes take effect immediately</p>"},{"location":"dev/architecture.html#security-architecture","title":"Security Architecture","text":""},{"location":"dev/architecture.html#authentication-authorization","title":"Authentication &amp; Authorization","text":"<p>User Authentication: - Nautobot's built-in authentication - LDAP/SAML support via Nautobot - Session management</p> <p>API Authentication: - Token-based (Nautobot API tokens) - Per-request authentication - Token permissions enforced</p> <p>Permissions: - <code>ai_ops.view_llmprovider</code> - <code>ai_ops.add_llmprovider</code> - <code>ai_ops.change_llmprovider</code> - <code>ai_ops.delete_llmprovider</code> - <code>ai_ops.view_llmmodel</code> - <code>ai_ops.add_llmmodel</code> - <code>ai_ops.change_llmmodel</code> - <code>ai_ops.delete_llmmodel</code> - <code>ai_ops.view_middlewaretype</code> - <code>ai_ops.add_middlewaretype</code> - <code>ai_ops.change_middlewaretype</code> - <code>ai_ops.delete_middlewaretype</code> - <code>ai_ops.view_llmmiddleware</code> - <code>ai_ops.add_llmmiddleware</code> - <code>ai_ops.change_llmmiddleware</code> - <code>ai_ops.delete_llmmiddleware</code> - <code>ai_ops.view_mcpserver</code> - <code>ai_ops.add_mcpserver</code> - <code>ai_ops.change_mcpserver</code> - <code>ai_ops.delete_mcpserver</code></p>"},{"location":"dev/architecture.html#secrets-management","title":"Secrets Management","text":"<p>API Keys: - Stored in Nautobot Secrets - Never in code or database directly - Provider-agnostic (environment, HashiCorp Vault, etc.)</p> <p>Access Control: - Secrets retrieved at runtime - Minimal exposure - Audit trail via Nautobot</p>"},{"location":"dev/architecture.html#data-security","title":"Data Security","text":"<p>In Transit: - HTTPS for all LLM providers (Ollama, OpenAI, Azure AI, Anthropic, etc.) - HTTPS for MCP servers (recommended) - TLS for Redis connections (optional)</p> <p>At Rest: - PostgreSQL encryption (via deployment) - Redis encryption (via deployment) - Nautobot Secrets encryption</p>"},{"location":"dev/architecture.html#network-security","title":"Network Security","text":"<p>Firewall Rules: - Outbound to LLM provider APIs (443)   - Ollama: Configurable port (default 11434)   - OpenAI: api.openai.com:443   - Azure AI: *.openai.azure.com:443   - Anthropic: api.anthropic.com:443   - HuggingFace: huggingface.co:443 - Outbound to MCP servers (various) - Inbound to Nautobot (80/443)</p> <p>Internal Communication: - PostgreSQL: local or internal network - Redis: local or internal network - MCP servers: internal network (typically)</p>"},{"location":"dev/architecture.html#scalability-performance","title":"Scalability &amp; Performance","text":""},{"location":"dev/architecture.html#caching-strategy","title":"Caching Strategy","text":"<p>MCP Client Cache: - Application-level - 5-minute TTL - Reduces initialization overhead - Thread-safe</p> <p>Model Instances: - Reusable within agent lifecycle - Not cached globally (created per request) - Stateless between requests</p>"},{"location":"dev/architecture.html#async-architecture","title":"Async Architecture","text":"<p>Benefits: - Non-blocking I/O - Concurrent request handling - Better resource utilization</p> <p>Implementation: - Django async views - Async agent processing - Async MCP client operations</p>"},{"location":"dev/architecture.html#database-optimization","title":"Database Optimization","text":"<p>Indexes: - Primary keys (UUID) - Status field (MCPServer) - is_default field (LLMModel)</p> <p>Queries: - Filtered by status for MCP servers - Default model query optimized - Minimal database round trips</p>"},{"location":"dev/architecture.html#redis-optimization","title":"Redis Optimization","text":"<p>Checkpoint Cleanup: - Scheduled job removes old data - Prevents unbounded growth - Configurable retention period</p> <p>Key Structure: - Efficient key patterns - SCAN for safe iteration - Separate database number</p>"},{"location":"dev/architecture.html#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"dev/architecture.html#logging","title":"Logging","text":"<p>Log Levels: - INFO: Normal operations - WARNING: Degraded conditions - ERROR: Failures</p> <p>Log Locations: - Nautobot logs directory - Application-specific logger: <code>ai_ops</code></p> <p>Key Events: - Agent message processing - MCP cache operations - Model configuration retrieval - Health check failures</p>"},{"location":"dev/architecture.html#metrics","title":"Metrics","text":"<p>Application Metrics: - Request count - Response times - Error rates</p> <p>System Metrics: - Redis memory usage - PostgreSQL connection pool - Azure OpenAI API usage</p>"},{"location":"dev/architecture.html#health-checks","title":"Health Checks","text":"<p>MCP Server Health: - Automatic health check requests - Status field updated - Failed servers excluded</p> <p>System Health: - Redis connectivity - PostgreSQL connectivity - Azure OpenAI API accessibility</p>"},{"location":"dev/architecture.html#error-handling","title":"Error Handling","text":""},{"location":"dev/architecture.html#graceful-degradation","title":"Graceful Degradation","text":"<p>No MCP Servers: - Agent continues without tools - Basic conversation capabilities maintained - Warning logged</p> <p>Model Configuration Missing: - Error raised early - Clear error message - LAB fallback to environment variables</p> <p>Azure API Failures: - Errors propagated to user - Rate limit handling recommended - Retry logic (external implementation)</p>"},{"location":"dev/architecture.html#error-recovery","title":"Error Recovery","text":"<p>Transient Failures: - MCP server temporarily unavailable \u2192 Excluded until healthy - Redis connection issue \u2192 Conversation history unavailable but agent works - Azure API timeout \u2192 User notified, can retry</p> <p>Permanent Failures: - Invalid API key \u2192 Configuration error, admin must fix - Model not found \u2192 Create model or use default - Server consistently failing \u2192 Update status to Maintenance</p>"},{"location":"dev/architecture.html#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"dev/architecture.html#environment-requirements","title":"Environment Requirements","text":"<p>Minimum: - Python 3.10+ - PostgreSQL 12+ or MySQL 8+ - Redis 6+ - Nautobot 2.4.22+</p> <p>Recommended: - Python 3.11+ - PostgreSQL 14+ - Redis 7+ - Dedicated Redis for checkpoints</p>"},{"location":"dev/architecture.html#scaling","title":"Scaling","text":"<p>Horizontal Scaling: - Multiple Nautobot workers - Shared Redis for checkpoints - Shared PostgreSQL database - MCP client cache per worker</p> <p>Vertical Scaling: - More CPU for LLM processing - More memory for Redis checkpoints - More connections to PostgreSQL</p>"},{"location":"dev/architecture.html#high-availability","title":"High Availability","text":"<p>Components: - Nautobot: Load balanced, multiple workers - PostgreSQL: Primary/replica setup - Redis: Redis Sentinel or Cluster - MCP Servers: Multiple instances with health checks</p> <p>Failure Scenarios: - Single worker failure \u2192 Other workers handle requests - Redis failure \u2192 Conversation history lost, functionality continues - PostgreSQL failure \u2192 Application unavailable (required) - MCP server failure \u2192 Other servers continue, failed server excluded</p>"},{"location":"dev/architecture.html#future-architecture-enhancements","title":"Future Architecture Enhancements","text":""},{"location":"dev/architecture.html#planned-improvements","title":"Planned Improvements","text":"<ol> <li>PostgreSQL Checkpointing: Replace Redis with PostgreSQL for persistence</li> <li>Conversation History UI: View and manage past conversations</li> <li>Model Performance Metrics: Track model usage and performance</li> <li>Advanced Caching: Redis caching for model responses</li> <li>Streaming Responses: Real-time streaming of AI responses</li> <li>Multi-Tenancy: Tenant-specific models and configurations</li> <li>Custom Agent Types: Support for specialized agent implementations</li> <li>Tool Usage Analytics: Track and visualize tool invocations</li> </ol>"},{"location":"dev/architecture.html#integration-opportunities","title":"Integration Opportunities","text":"<ul> <li>ITSM Integration: ServiceNow, Jira ticket creation</li> <li>Monitoring Systems: Integration with Prometheus, Grafana</li> <li>ChatOps: Slack, Teams integration</li> <li>Workflow Automation: Ansible, Terraform integration</li> </ul>"},{"location":"dev/architecture.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Models - Database models</li> <li>Agents - AI agent implementations</li> <li>Helpers - Helper modules</li> <li>Jobs - Background jobs</li> <li>External Interactions - External system integrations</li> </ul>"},{"location":"dev/contributing.html","title":"Contributing to the App","text":"<p>The project is packaged with a light development environment based on <code>docker-compose</code> to help with the local development of the project and to run tests.</p> <p>The project is following Network to Code software development guidelines and is leveraging the following:</p> <ul> <li>Python linting and formatting: <code>pylint</code> and <code>ruff</code>.</li> <li>YAML linting is done with <code>yamllint</code>.</li> <li>Django unit test to ensure the app is working properly.</li> <li>Django Template linting: <code>djlint</code></li> <li>Django Template formatting: <code>djhtml</code></li> </ul> <p>Documentation is built using mkdocs. The Docker based development environment automatically starts a container hosting a live version of the documentation website on http://localhost:8001 that auto-refreshes when you make any changes to your local files.</p>"},{"location":"dev/contributing.html#creating-changelog-fragments","title":"Creating Changelog Fragments","text":"<p>All pull requests to <code>next</code> or <code>develop</code> must include a changelog fragment file in the <code>./changes</code> directory. To create a fragment, use your GitHub issue number and fragment type as the filename. For example, <code>2362.added</code>. Valid fragment types are <code>added</code>, <code>changed</code>, <code>deprecated</code>, <code>fixed</code>, <code>removed</code>, and <code>security</code>. The change summary is added to the file in plain text. Change summaries should be complete sentences, starting with a capital letter and ending with a period, and be in past tense. Each line of the change fragment will generate a single change entry in the release notes. Use multiple lines in the same file if your change needs to generate multiple release notes in the same category. If the change needs to create multiple entries in separate categories, create multiple files.</p> <p>Example</p> <p>Wrong changes/1234.fixed<pre><code>fix critical bug in documentation\n</code></pre></p> <p>Right changes/1234.fixed<pre><code>Fixed critical bug in documentation.\n</code></pre></p> <p>Multiple Entry Example</p> <p>This will generate 2 entries in the <code>fixed</code> category and one entry in the <code>changed</code> category.</p> changes/1234.fixed<pre><code>Fixed critical bug in documentation.\nFixed release notes generation.\n</code></pre> changes/1234.changed<pre><code>Changed release notes generation.\n</code></pre>"},{"location":"dev/contributing.html#branching-policy","title":"Branching Policy","text":"<p>The branching policy includes the following tenets:</p> <ul> <li>The <code>develop</code> branch is the branch of the next major and minor paired version planned.</li> <li>PRs intended to add new features should be sourced from the <code>develop</code> branch.</li> <li>PRs intended to fix issues in the Nautobot LTM compatible release should be sourced from the latest <code>ltm-&lt;major.minor&gt;</code> branch instead of <code>develop</code>.</li> </ul> <p>AI Ops will observe semantic versioning, as of 1.0. This may result in a quick turnaround in minor versions to keep pace with an ever-growing feature set.</p>"},{"location":"dev/contributing.html#backporting-to-older-releases","title":"Backporting to Older Releases","text":"<p>If you are backporting any fixes to a prior major or minor version of this app, please open an issue, comment on an existing issue, or post in the Network to Code Slack (channel <code>#nautobot</code>).</p> <p>We will create a <code>release-X.Y</code> branch for you to open your PR against and cut a new release once the PR is successfully merged.</p>"},{"location":"dev/contributing.html#release-policy","title":"Release Policy","text":"<p>AI Ops has currently no intended scheduled release schedule, and will release new features in minor versions.</p> <p>The steps taken by maintainers when creating a new release are documented in the release checklist.</p>"},{"location":"dev/dev_environment.html","title":"Building Your Development Environment","text":""},{"location":"dev/dev_environment.html#quickstart-guide","title":"Quickstart Guide","text":"<p>The development environment can be used in two ways:</p> <ol> <li>(Recommended) All services, including Nautobot, are spun up using Docker containers and a volume mount so you can develop locally.</li> <li>With a local Poetry environment if you wish to develop outside of Docker, with the caveat of using external services provided by Docker for the database (PostgreSQL by default, MySQL optionally) and Redis services.</li> </ol> <p>This is a quick reference guide if you're already familiar with the development environment provided, which you can read more about later in this document.</p>"},{"location":"dev/dev_environment.html#invoke","title":"Invoke","text":"<p>The Invoke library is used to provide some helper commands based on the environment. There are a few configuration parameters which can be passed to Invoke to override the default configuration:</p> <ul> <li><code>nautobot_ver</code>: the version of Nautobot to use as a base for any built docker containers (default: 3.0.0)</li> <li><code>project_name</code>: the default docker compose project name (default: <code>ai-ops</code>)</li> <li><code>python_ver</code>: the version of Python to use as a base for any built docker containers (default: 3.12)</li> <li><code>local</code>: a boolean flag indicating if invoke tasks should be run on the host or inside the docker containers (default: False, commands will be run in docker containers)</li> <li><code>compose_dir</code>: the full path to a directory containing the project compose files</li> <li><code>compose_files</code>: a list of compose files applied in order (see Multiple Compose files for more information)</li> </ul> <p>Using Invoke these configuration options can be overridden using several methods. Perhaps the simplest is setting an environment variable <code>INVOKE_AI_OPS_VARIABLE_NAME</code> where <code>VARIABLE_NAME</code> is the variable you are trying to override. The only exception is <code>compose_files</code>, because it is a list it must be overridden in a YAML file. There is an example <code>invoke.yml</code> (<code>invoke.example.yml</code>) in this directory which can be used as a starting point.</p>"},{"location":"dev/dev_environment.html#docker-development-environment","title":"Docker Development Environment","text":"<p>Tip</p> <p>This is the recommended option for development.</p> <p>This project is managed by Python Poetry and has a few requirements to setup your development environment:</p> <ol> <li>Install Poetry, see the Poetry documentation for your operating system.</li> <li>Install Docker, see the Docker documentation for your operating system.</li> <li>Install Docker-compose, see the Docker-compose documentation for your operation system.</li> </ol> <p>Once you have Poetry and Docker installed you can run the following commands (in the root of the repository) to install all other development dependencies in an isolated Python virtual environment:</p> <pre><code>poetry self add poetry-plugin-shell\npoetry shell\npoetry install\ninvoke build\ninvoke start\n</code></pre> <p>The Nautobot server can now be accessed at http://localhost:8080 and the live documentation at http://localhost:8001.</p> <p>To either stop or destroy the development environment use the following options.</p> <ul> <li>invoke stop - Stop the containers, but keep all underlying systems intact</li> <li>invoke destroy - Stop and remove all containers, volumes, etc. (This results in data loss due to the volume being deleted)</li> </ul>"},{"location":"dev/dev_environment.html#local-poetry-development-environment","title":"Local Poetry Development Environment","text":"<ul> <li>Create an <code>invoke.yml</code> file with the following contents at the root of the repo and edit as necessary</li> </ul> <pre><code>---\nai_ops:\n  local: true\n</code></pre> <p>Run the following commands:</p> <pre><code>poetry self add poetry-plugin-shell\npoetry shell\npoetry install --extras nautobot\nexport $(cat development/development.env | xargs)\nexport $(cat development/creds.env | xargs)\ninvoke start &amp;&amp; sleep 5\nnautobot-server migrate\n</code></pre> <p>Note</p> <p>If you want to develop on the latest develop branch of Nautobot, run the following command: <code>poetry add --optional git+https://github.com/nautobot/nautobot@develop</code>. After the <code>@</code> symbol must match either a branch or a tag.</p> <p>You can now run <code>nautobot-server</code> commands as you would from the Nautobot documentation for example to start the development server:</p> <pre><code>nautobot-server runserver 0.0.0.0:8080 --insecure\n</code></pre> <p>Nautobot server can now be accessed at http://localhost:8080.</p> <p>It is typically recommended to launch the Nautobot runserver command in a separate shell so you can keep developing and manage the webserver separately.</p>"},{"location":"dev/dev_environment.html#updating-the-documentation","title":"Updating the Documentation","text":"<p>Documentation dependencies are pinned to exact versions to ensure consistent results. For the development environment, they are defined in the <code>pyproject.toml</code> file.</p>"},{"location":"dev/dev_environment.html#cli-helper-commands","title":"CLI Helper Commands","text":"<p>The project features a CLI helper based on Invoke to help setup the development environment. The commands are listed below in 3 categories:</p> <ul> <li><code>dev environment</code></li> <li><code>utility</code></li> <li><code>testing</code></li> </ul> <p>Each command can be executed with <code>invoke &lt;command&gt;</code>. All commands support the arguments <code>--nautobot-ver</code> and <code>--python-ver</code> if you want to manually define the version of Python and Nautobot to use. Each command also has its own help <code>invoke &lt;command&gt; --help</code></p>"},{"location":"dev/dev_environment.html#local-development-environment","title":"Local Development Environment","text":"<pre><code>  build            Build all docker images.\n  debug            Start Nautobot and its dependencies in debug mode.\n  destroy          Destroy all containers and volumes.\n  restart          Restart Nautobot and its dependencies in detached mode.\n  start            Start Nautobot and its dependencies in detached mode.\n  stop             Stop Nautobot and its dependencies.\n</code></pre>"},{"location":"dev/dev_environment.html#utility","title":"Utility","text":"<pre><code>  cli              Launch a bash shell inside the running Nautobot container.\n  create-user      Create a new user in django (default: admin), will prompt for password.\n  makemigrations   Run Make Migration in Django.\n  nbshell          Launch a nbshell session.\n</code></pre>"},{"location":"dev/dev_environment.html#testing","title":"Testing","text":"<pre><code>  ruff             Run ruff to perform code formatting and/or linting.\n  pylint           Run pylint code analysis.\n  markdownlint     Run pymarkdown linting.\n  tests            Run all tests for this app.\n  unittest         Run Django unit tests for the app.\n  djlint           Run djlint to perform django template linting.\n  djhtml           Run djhtml to perform django template formatting.\n</code></pre>"},{"location":"dev/dev_environment.html#project-overview","title":"Project Overview","text":"<p>This project provides the ability to develop and manage the Nautobot server locally (with supporting services being Dockerized) or by using only Docker containers to manage Nautobot. The main difference between the two environments is the ability to debug and use pdb when developing locally. Debugging with pdb within the Docker container is more complicated, but can still be accomplished by either entering into the container (via <code>docker exec</code>) or attaching your IDE to the container and running the Nautobot service manually within the container.</p> <p>The upside to having the Nautobot service handled by Docker rather than locally is that you do not have to manage the Nautobot server. The Docker logs provide the majority of the information you will need to help troubleshoot, while getting started quickly and not requiring you to perform several manual steps and remembering to have the Nautobot server running in a separate terminal while you develop.</p> <p>Note</p> <p>The local environment still uses Docker containers for the supporting services (Postgres, Redis, and RQ Worker), but the Nautobot server is handled locally by you, the developer.</p> <p>Follow the directions below for the specific development environment that you choose.</p>"},{"location":"dev/dev_environment.html#poetry","title":"Poetry","text":"<p>Poetry is used in lieu of the \"virtualenv\" commands and is leveraged in both environments. The virtual environment will provide all of the Python packages required to manage the development environment such as Invoke. See the Local Development Environment section to see how to install Nautobot if you're going to be developing locally (i.e. not using the Docker container).</p> <p>The <code>pyproject.toml</code> file outlines all of the relevant dependencies for the project:</p> <ul> <li><code>tool.poetry.dependencies</code> - the main list of dependencies.</li> <li><code>tool.poetry.group.dev.dependencies</code> - development dependencies, to facilitate linting, testing, and documentation building.</li> </ul> <p>The <code>poetry shell</code> command is used to create and enable a virtual environment managed by Poetry, so all commands ran going forward are executed within the virtual environment. This is similar to running the <code>source venv/bin/activate</code> command with virtualenvs. To install project dependencies in the virtual environment, you should run <code>poetry install</code> - this will install both project and development dependencies.</p> <p>For more details about Poetry and its commands please check out its online documentation.</p> <p>In Poetry version 2, the shell command was moved out of the main Poetry project and into a plugin. For more details about the Poetry shell plugin, refer to its GitHub repository.</p>"},{"location":"dev/dev_environment.html#full-docker-development-environment","title":"Full Docker Development Environment","text":"<p>This project is set up with a number of Invoke tasks consumed as simple CLI commands to get developing fast. You'll use a few <code>invoke</code> commands to get your environment up and running.</p>"},{"location":"dev/dev_environment.html#copy-the-credentials-file-for-nautobot","title":"Copy the credentials file for Nautobot","text":"<p>First, you may create/overwrite the <code>development/creds.env</code> file - it stores a bunch of private information such as passwords and tokens for your local Nautobot install. You can make a copy of the <code>development/creds.example.env</code> and modify it to suit you.</p> <pre><code>cp development/creds.example.env development/creds.env\n</code></pre>"},{"location":"dev/dev_environment.html#invoke-building-the-docker-image","title":"Invoke - Building the Docker Image","text":"<p>The first thing you need to do is build the necessary Docker image for Nautobot that installs the specific <code>nautobot_ver</code>. The image is used for Nautobot and the Celery worker service used by Docker Compose.</p> <pre><code>\u279c invoke build\n... &lt;omitted for brevity&gt;\n#14 exporting to image\n#14 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n#14 exporting layers\n#14 exporting layers 1.2s done\n#14 writing image sha256:2d524bc1665327faa0d34001b0a9d2ccf450612bf8feeb969312e96a2d3e3503 done\n#14 naming to docker.io/ai-ops/nautobot:3.0.0-py3.12 done\n</code></pre>"},{"location":"dev/dev_environment.html#invoke-starting-the-development-environment","title":"Invoke - Starting the Development Environment","text":"<p>Next, you need to start up your Docker containers.</p> <pre><code>\u279c invoke start\nStarting Nautobot in detached mode...\nRunning docker-compose command \"up --detach\"\nCreating network \"ai_ops_default\" with the default driver\nCreating volume \"ai_ops_postgres_data\" with default driver\nCreating ai_ops_redis_1 ...\nCreating ai_ops_docs_1  ...\nCreating ai_ops_postgres_1 ...\nCreating ai_ops_postgres_1 ... done\nCreating ai_ops_redis_1    ... done\nCreating ai_ops_nautobot_1 ...\nCreating ai_ops_docs_1     ... done\nCreating ai_ops_nautobot_1 ... done\nCreating ai_ops_worker_1   ...\nCreating ai_ops_worker_1   ... done\nDocker Compose is now in the Docker CLI, try `docker compose up`\n</code></pre> <p>This will start all of the Docker containers used for hosting Nautobot. You should see the following containers running after <code>invoke start</code> is finished.</p> <pre><code>\u279c docker ps\n****CONTAINER ID   IMAGE                            COMMAND                  CREATED          STATUS          PORTS                                       NAMES\nee90fbfabd77   ai-ops/nautobot:3.0.0-py3.12  \"nautobot-server rqw\u2026\"   16 seconds ago   Up 13 seconds                                               ai_ops_worker_1\nb8adb781d013   ai-ops/nautobot:3.0.0-py3.12  \"/docker-entrypoint.\u2026\"   20 seconds ago   Up 15 seconds   0.0.0.0:8080-&gt;8080/tcp, :::8080-&gt;8080/tcp   ai_ops_nautobot_1\nd64ebd60675d   ai-ops/nautobot:3.0.0-py3.12  \"mkdocs serve -v -a \u2026\"   25 seconds ago   Up 18 seconds   0.0.0.0:8001-&gt;8080/tcp, :::8001-&gt;8080/tcp   ai_ops_docs_1\ne72d63129b36   postgres:13-alpine               \"docker-entrypoint.s\u2026\"   25 seconds ago   Up 19 seconds   0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp   ai_ops_postgres_1\n96c6ff66997c   redis:6-alpine                   \"docker-entrypoint.s\u2026\"   25 seconds ago   Up 21 seconds   0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp   ai_ops_redis_1\n</code></pre> <p>Once the containers are fully up, you should be able to open up a web browser, and view:</p> <ul> <li>The Nautobot homepage at http://localhost:8080</li> <li>A live version of the documentation at http://localhost:8001</li> </ul> <p>Note</p> <p>Sometimes the containers take a minute to fully spin up. If the page doesn't load right away, wait a minute and try again.</p>"},{"location":"dev/dev_environment.html#invoke-creating-a-superuser","title":"Invoke - Creating a Superuser","text":"<p>The Nautobot development image will automatically provision a super user when specifying the following variables within <code>creds.env</code> which is the default when copying <code>creds.example.env</code> to <code>creds.env</code>.</p> <ul> <li><code>NAUTOBOT_CREATE_SUPERUSER=true</code></li> <li><code>NAUTOBOT_SUPERUSER_API_TOKEN=0123456789abcdef0123456789abcdef01234567</code></li> <li><code>NAUTOBOT_SUPERUSER_PASSWORD=admin</code></li> </ul> <p>Note</p> <p>The default username is admin, but can be overridden by specifying NAUTOBOT_SUPERUSER_USERNAME.</p> <p>If you need to create additional superusers, run the follow commands.</p> <pre><code>\u279c invoke createsuperuser\nRunning docker-compose command \"ps --services --filter status=running\"\nRunning docker-compose command \"exec nautobot nautobot-server createsuperuser --username admin\"\nError: That username is already taken.\nUsername: ntc\nEmail address: ntc@networktocode.com\nPassword:\nPassword (again):\nSuperuser created successfully.\n</code></pre>"},{"location":"dev/dev_environment.html#invoke-stopping-the-development-environment","title":"Invoke - Stopping the Development Environment","text":"<p>The last command to know for now is <code>invoke stop</code>.</p> <pre><code>\u279c invoke stop\nStopping Nautobot...\nRunning docker-compose command \"down\"\nStopping ai_ops_worker_1   ...\nStopping ai_ops_nautobot_1 ...\nStopping ai_ops_docs_1     ...\nStopping ai_ops_redis_1    ...\nStopping ai_ops_postgres_1 ...\nStopping ai_ops_worker_1   ... done\nStopping ai_ops_nautobot_1 ... done\nStopping ai_ops_postgres_1 ... done\nStopping ai_ops_redis_1    ... done\nStopping ai_ops_docs_1     ... done\nRemoving ai_ops_worker_1   ...\nRemoving ai_ops_nautobot_1 ...\nRemoving ai_ops_docs_1     ...\nRemoving ai_ops_redis_1    ...\nRemoving ai_ops_postgres_1 ...\nRemoving ai_ops_postgres_1 ... done\nRemoving ai_ops_docs_1     ... done\nRemoving ai_ops_worker_1   ... done\nRemoving ai_ops_redis_1    ... done\nRemoving ai_ops_nautobot_1 ... done\nRemoving network ai_ops_default\n</code></pre> <p>This will safely shut down all of your running Docker containers for this project. When you are ready to spin containers back up, it is as simple as running <code>invoke start</code> again as seen previously.</p> <p>Warning</p> <p>If you're wanting to reset the database and configuration settings, you can use the <code>invoke destroy</code> command, but you will lose any data stored in those containers, so make sure that is what you want to do.</p>"},{"location":"dev/dev_environment.html#real-time-updates-how-cool","title":"Real-Time Updates? How Cool!","text":"<p>Your environment should now be fully setup, all necessary Docker containers are created and running, and you're logged into Nautobot in your web browser. Now what?</p> <p>Now you can start developing your app in the project folder!</p> <p>The magic here is the root directory is mounted inside your Docker containers when built and ran, so any changes made to the files in here are directly updated to the Nautobot app code running in Docker. This means that as you modify the code in your app folder, the changes will be instantly updated in Nautobot.</p> <p>Warning</p> <p>There are a few exceptions to this, as outlined in the section To Rebuild or Not To Rebuild.</p> <p>The back-end Django process is setup to automatically reload itself (it only takes a couple of seconds) every time a file is updated (saved). So for example, if you were to update one of the files like <code>tables.py</code>, then save it, the changes will be visible right away in the web browser!</p> <p>Note</p> <p>You may get connection refused while Django reloads, but it should be refreshed fairly quickly.</p>"},{"location":"dev/dev_environment.html#docker-logs","title":"Docker Logs","text":"<p>When trying to debug an issue, one helpful thing you can look at are the logs within the Docker containers.</p> <pre><code>\u279c docker logs &lt;name of container&gt; -f\n</code></pre> <p>Note</p> <p>The <code>-f</code> tag will keep the logs open, and output them in realtime as they are generated.</p> <p>Info</p> <p>Want to limit the log output even further? Use the <code>--tail &lt;#&gt;</code> command line argument in conjunction with <code>-f</code>.</p> <p>So for example, our app is named <code>ai-ops</code>, the command would most likely be <code>docker logs ai_ops_nautobot_1 -f</code>. You can find the name of all running containers via <code>docker ps</code>.</p> <p>If you want to view the logs specific to the worker container, simply use the name of that container instead.</p>"},{"location":"dev/dev_environment.html#to-rebuild-or-not-to-rebuild","title":"To Rebuild or Not to Rebuild","text":"<p>Most of the time, you will not need to rebuild your images. Simply running <code>invoke start</code> and <code>invoke stop</code> is enough to keep your environment going.</p> <p>However there are a couple of instances when you will want to.</p>"},{"location":"dev/dev_environment.html#updating-environment-variables","title":"Updating Environment Variables","text":"<p>To add environment variables to your containers, thus allowing Nautobot to use them, you will update/add them in the <code>development/development.env</code> file. However, doing so is considered updating the underlying container shell, instead of Django (which auto restarts itself on changes).</p> <p>To get new environment variables to take effect, you will need stop any running images, rebuild the images, then restart them. This can easily be done with 3 commands:</p> <pre><code>\u279c invoke stop\n\u279c invoke build\n\u279c invoke start\n</code></pre> <p>Once completed, the new/updated environment variables should now be live.</p>"},{"location":"dev/dev_environment.html#installing-additional-python-packages","title":"Installing Additional Python Packages","text":"<p>If you want your app to leverage another available Nautobot app or another Python package, you can easily add them into your Docker environment.</p> <pre><code>\u279c poetry add &lt;package_name&gt;\n</code></pre> <p>Once the dependencies are resolved, stop the existing containers, rebuild the Docker image, and then start all containers again.</p> <pre><code>\u279c invoke stop\n\u279c invoke build\n\u279c invoke start\n</code></pre>"},{"location":"dev/dev_environment.html#installing-additional-nautobot-apps","title":"Installing Additional Nautobot Apps","text":"<p>Let's say for example you want the new app you're creating to integrate into Slack. To do this, you will want to integrate into the existing Nautobot ChatOps App.</p> <pre><code>\u279c poetry add nautobot-chatops\n</code></pre> <p>Once you activate the virtual environment via Poetry, you then tell Poetry to install the new app.</p> <p>Before you continue, you'll need to update the file <code>development/nautobot_config.py</code> accordingly with the name of the new app under <code>PLUGINS</code> and any relevant settings as necessary for the app under <code>PLUGINS_CONFIG</code>. Since you're modifying the underlying OS (not just Django files), you need to rebuild the image. This is a similar process to updating environment variables, which was explained earlier.</p> <pre><code>\u279c invoke stop\n\u279c invoke build\n\u279c invoke start\n</code></pre> <p>Once the containers are up and running, you should now see the new app installed in your Nautobot instance.</p> <p>Note</p> <p>You can even launch an <code>ngrok</code> service locally on your laptop, pointing to port 8080 (such as for chatops development), and it will point traffic directly to your Docker images.</p>"},{"location":"dev/dev_environment.html#updating-python-version","title":"Updating Python Version","text":"<p>To update the Python version, you can update it within <code>tasks.py</code>.</p> <pre><code>namespace = Collection(\"ai_ops\")\nnamespace.configure(\n    {\n        \"ai_ops\": {\n            ...\n            \"python_ver\": \"3.12\",\n        ...\n        }\n    }\n)\n</code></pre> <p>Or set the <code>INVOKE_AI_OPS_PYTHON_VER</code> variable.</p>"},{"location":"dev/dev_environment.html#updating-nautobot-version","title":"Updating Nautobot Version","text":"<p>To update the Nautobot version, you can update it within <code>tasks.py</code>.</p> <pre><code>namespace = Collection(\"ai_ops\")\nnamespace.configure(\n    {\n        \"ai_ops\": {\n            ...\n            \"nautobot_ver\": \"3.0.0\",\n        ...\n        }\n    }\n)\n</code></pre> <p>Or set the <code>INVOKE_AI_OPS_NAUTOBOT_VER</code> variable.</p>"},{"location":"dev/dev_environment.html#other-miscellaneous-commands-to-know","title":"Other Miscellaneous Commands To Know","text":""},{"location":"dev/dev_environment.html#python-shell","title":"Python Shell","text":"<p>To drop into a Django shell for Nautobot (in the Docker container) run:</p> <pre><code>\u279c invoke nbshell\n</code></pre> <p>This is the same as running:</p> <pre><code>\u279c invoke cli\n\u279c nautobot-server nbshell\n</code></pre>"},{"location":"dev/dev_environment.html#ipython-shell-plus","title":"iPython Shell Plus","text":"<p>Django also has a more advanced shell that uses iPython and that will automatically import all the models:</p> <pre><code>\u279c invoke shell-plus\n</code></pre> <p>This is the same as running:</p> <pre><code>\u279c invoke cli\n\u279c nautobot-server shell_plus\n</code></pre>"},{"location":"dev/dev_environment.html#tests","title":"Tests","text":"<p>To run tests against your code, you can run all of the tests that the CI runs against any new PR with:</p> <pre><code>\u279c invoke tests\n</code></pre> <p>To run an individual test, you can run any or all of the following:</p> <pre><code>\u279c invoke unittest\n\u279c invoke ruff\n\u279c invoke pylint\n</code></pre>"},{"location":"dev/dev_environment.html#app-configuration-schema","title":"App Configuration Schema","text":"<p>In the package source, there is the <code>ai_ops/app-config-schema.json</code> file, conforming to the JSON Schema format. This file is used to validate the configuration of the app in CI pipelines.</p> <p>If you make changes to <code>PLUGINS_CONFIG</code> or the configuration schema, you can run the following command to validate the schema:</p> <pre><code>invoke validate-app-config\n</code></pre> <p>To generate the <code>app-config-schema.json</code> file based on the current <code>PLUGINS_CONFIG</code> configuration, run the following command:</p> <pre><code>invoke generate-app-config-schema\n</code></pre> <p>This command can only guess the schema, so it's up to the developer to manually update the schema as needed.</p>"},{"location":"dev/extending.html","title":"Extending the App","text":"<p>This document provides guidance on extending the AI Ops App functionality.</p>"},{"location":"dev/extending.html#before-you-begin","title":"Before You Begin","text":"<p>Extending the application is welcome! However, it's best to open an issue first to:</p> <ul> <li>Ensure a PR would be accepted</li> <li>Discuss the proposed feature or design</li> <li>Get feedback from maintainers</li> <li>Avoid duplicate work</li> </ul>"},{"location":"dev/extending.html#extension-points","title":"Extension Points","text":"<p>The AI Ops App provides several extension points:</p>"},{"location":"dev/extending.html#1-custom-ai-agents","title":"1. Custom AI Agents","text":"<p>Create custom agents with specialized behavior:</p> <pre><code># ai_ops/agents/custom_agent.py\nfrom langgraph.graph import StateGraph\nfrom ai_ops.helpers.get_azure_model import get_azure_model_async\nfrom ai_ops.agents.multi_mcp_agent import MessagesState\n\nasync def create_custom_agent():\n    \"\"\"Create a custom agent with specialized behavior.\"\"\"\n\n    # Get LLM model\n    model = await get_azure_model_async(model_name=\"gpt-4o\")\n\n    # Define custom system prompt\n    CUSTOM_PROMPT = \"\"\"\n    You are a specialized network automation assistant.\n    Focus on: ...\n    \"\"\"\n\n    # Create state graph\n    workflow = StateGraph(MessagesState)\n\n    # Add custom nodes and edges\n    # ...\n\n    return workflow.compile()\n</code></pre>"},{"location":"dev/extending.html#2-custom-mcp-servers","title":"2. Custom MCP Servers","text":"<p>Develop MCP servers to provide domain-specific tools:</p> <pre><code># Example MCP server structure\nfrom mcp.server import Server\nfrom mcp.types import Tool\n\nserver = Server(\"my-custom-mcp\")\n\n@server.tool()\nasync def custom_tool(param1: str, param2: int) -&gt; str:\n    \"\"\"Custom tool implementation.\"\"\"\n    # Your logic here\n    return result\n\n# Register in Nautobot:\n# AI Platform &gt; Configuration &gt; MCP Servers\n# Add your server URL\n</code></pre>"},{"location":"dev/extending.html#3-custom-system-prompts","title":"3. Custom System Prompts","text":"<p>Modify agent behavior by customizing prompts:</p> <pre><code># ai_ops/prompts/custom_prompt.py\n\nCUSTOM_SYSTEM_PROMPT = \"\"\"\nYou are an AI assistant specialized in [your domain].\n\nYour responsibilities:\n1. [Responsibility 1]\n2. [Responsibility 2]\n\nGuidelines:\n- [Guideline 1]\n- [Guideline 2]\n\nAvailable tools: {tool_names}\n\"\"\"\n</code></pre> <p>Then use in your agent:</p> <pre><code>from ai_ops.prompts.custom_prompt import CUSTOM_SYSTEM_PROMPT\n\n# In agent creation\nsystem_message = CUSTOM_SYSTEM_PROMPT.format(\n    tool_names=\", \".join(tool.name for tool in tools)\n)\n</code></pre>"},{"location":"dev/extending.html#4-additional-models","title":"4. Additional Models","text":"<p>Extend with custom database models:</p> <pre><code># ai_ops/models.py\nfrom nautobot.apps.models import PrimaryModel\n\n@extras_features(\"webhooks\", \"graphql\")\nclass CustomModel(PrimaryModel):\n    \"\"\"Your custom model.\"\"\"\n\n    name = models.CharField(max_length=100)\n    # Add your fields\n\n    class Meta:\n        ordering = [\"name\"]\n</code></pre> <p>Remember to create migrations:</p> <pre><code>nautobot-server makemigrations ai_ops\nnautobot-server migrate ai_ops\n</code></pre>"},{"location":"dev/extending.html#5-custom-views","title":"5. Custom Views","text":"<p>Add custom views for new functionality:</p> <pre><code># ai_ops/views.py\nfrom nautobot.apps.views import GenericView\nfrom django.shortcuts import render\n\nclass CustomView(GenericView):\n    \"\"\"Custom view for specialized functionality.\"\"\"\n\n    template_name = \"ai_ops/custom_template.html\"\n\n    def get(self, request):\n        context = {\n            # Your context data\n        }\n        return render(request, self.template_name, context)\n</code></pre> <p>Register in URLs:</p> <pre><code># ai_ops/urls.py\nfrom django.urls import path\nfrom ai_ops.views import CustomView\n\nurlpatterns = [\n    path(\"custom/\", CustomView.as_view(), name=\"custom_view\"),\n    # ...\n]\n</code></pre>"},{"location":"dev/extending.html#6-api-extensions","title":"6. API Extensions","text":"<p>Extend the REST API with custom endpoints:</p> <pre><code># ai_ops/api/views.py\nfrom rest_framework.decorators import action\nfrom rest_framework.response import Response\nfrom nautobot.apps.api import NautobotModelViewSet\n\nclass CustomModelViewSet(NautobotModelViewSet):\n    \"\"\"ViewSet with custom actions.\"\"\"\n\n    @action(detail=True, methods=['post'])\n    def custom_action(self, request, pk=None):\n        \"\"\"Custom API action.\"\"\"\n        obj = self.get_object()\n        # Your logic\n        return Response({\"status\": \"success\"})\n</code></pre>"},{"location":"dev/extending.html#7-background-jobs","title":"7. Background Jobs","text":"<p>Add custom Nautobot Jobs:</p> <pre><code># ai_ops/jobs/custom_job.py\nfrom nautobot.extras.jobs import Job\n\nclass CustomMaintenanceJob(Job):\n    \"\"\"Custom maintenance job.\"\"\"\n\n    class Meta:\n        name = \"Custom Maintenance\"\n        description = \"Performs custom maintenance tasks\"\n\n    def run(self):\n        \"\"\"Job implementation.\"\"\"\n        self.logger.info(\"Starting custom maintenance...\")\n        # Your logic\n        return \"Maintenance completed\"\n</code></pre> <p>Register the job:</p> <pre><code># ai_ops/jobs/__init__.py\nfrom .custom_job import CustomMaintenanceJob\n\njobs = [CleanupCheckpointsJob, CustomMaintenanceJob]\nregister_jobs(*jobs)\n</code></pre>"},{"location":"dev/extending.html#8-custom-filters","title":"8. Custom Filters","text":"<p>Add filtering capabilities:</p> <pre><code># ai_ops/filters.py\nimport django_filters\nfrom nautobot.apps.filters import NautobotFilterSet\n\nclass CustomModelFilterSet(NautobotFilterSet):\n    \"\"\"Custom filters for CustomModel.\"\"\"\n\n    custom_field = django_filters.CharFilter(\n        field_name=\"custom_field\",\n        lookup_expr=\"icontains\"\n    )\n\n    class Meta:\n        model = CustomModel\n        fields = [\"name\", \"custom_field\"]\n</code></pre>"},{"location":"dev/extending.html#9-signal-handlers","title":"9. Signal Handlers","text":"<p>React to model events:</p> <pre><code># ai_ops/signals.py\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom ai_ops.models import LLMModel\n\n@receiver(post_save, sender=LLMModel)\ndef on_llm_model_save(sender, instance, created, **kwargs):\n    \"\"\"React to LLM model changes.\"\"\"\n    if created:\n        logger.info(f\"New LLM model created: {instance.name}\")\n    else:\n        logger.info(f\"LLM model updated: {instance.name}\")\n\n    # Your logic (e.g., invalidate caches)\n</code></pre>"},{"location":"dev/extending.html#common-extension-patterns","title":"Common Extension Patterns","text":""},{"location":"dev/extending.html#pattern-1-specialized-agent","title":"Pattern 1: Specialized Agent","text":"<p>Create an agent for a specific domain:</p> <pre><code># ai_ops/agents/network_agent.py\n\"\"\"Network-focused AI agent.\"\"\"\n\nNETWORK_PROMPT = \"\"\"\nYou are a network operations assistant specializing in:\n- Device configuration\n- Troubleshooting connectivity\n- Network design recommendations\n\"\"\"\n\nasync def process_network_query(message: str, thread_id: str):\n    \"\"\"Process network-related queries.\"\"\"\n    model = await get_azure_model_async()\n    # Custom agent logic\n    return response\n</code></pre>"},{"location":"dev/extending.html#pattern-2-domain-specific-mcp-server","title":"Pattern 2: Domain-Specific MCP Server","text":"<p>Build an MCP server for your domain:</p> <pre><code># external_mcp_server/network_tools.py\nfrom mcp.server import Server\n\nserver = Server(\"network-tools\")\n\n@server.tool()\nasync def check_device_status(device_name: str) -&gt; dict:\n    \"\"\"Check network device status.\"\"\"\n    # Query Nautobot or network devices\n    return {\n        \"device\": device_name,\n        \"status\": \"active\",\n        \"uptime\": \"30 days\"\n    }\n\n@server.tool()\nasync def get_interface_stats(device_name: str, interface: str) -&gt; dict:\n    \"\"\"Get interface statistics.\"\"\"\n    # Gather stats\n    return stats\n</code></pre>"},{"location":"dev/extending.html#pattern-3-custom-workflow","title":"Pattern 3: Custom Workflow","text":"<p>Create a multi-step workflow:</p> <pre><code># ai_ops/workflows/deployment_workflow.py\n\"\"\"Deployment workflow automation.\"\"\"\n\nasync def automated_deployment_workflow(config: dict):\n    \"\"\"Multi-step deployment workflow.\"\"\"\n\n    # Step 1: Validate configuration\n    validation_result = await validate_config(config)\n\n    # Step 2: Get AI recommendations\n    recommendations = await get_ai_recommendations(config)\n\n    # Step 3: Apply changes\n    if validation_result[\"valid\"]:\n        result = await apply_deployment(config)\n\n    return result\n</code></pre>"},{"location":"dev/extending.html#pattern-4-custom-checkpointer","title":"Pattern 4: Custom Checkpointer","text":"<p>Implement alternative storage for checkpoints:</p> <pre><code># ai_ops/checkpointers/postgres_checkpointer.py\n\"\"\"PostgreSQL-based checkpointer.\"\"\"\n\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nasync def get_postgres_checkpointer():\n    \"\"\"Get PostgreSQL checkpointer.\"\"\"\n    connection_string = get_database_connection_string()\n    return PostgresSaver(connection_string)\n</code></pre>"},{"location":"dev/extending.html#development-workflow","title":"Development Workflow","text":""},{"location":"dev/extending.html#1-set-up-development-environment","title":"1. Set Up Development Environment","text":"<p>Follow the Development Environment guide to set up your environment.</p>"},{"location":"dev/extending.html#2-create-feature-branch","title":"2. Create Feature Branch","text":"<pre><code>git checkout -b feature/my-extension\n</code></pre>"},{"location":"dev/extending.html#3-implement-extension","title":"3. Implement Extension","text":"<p>Follow the patterns above and existing code style.</p>"},{"location":"dev/extending.html#4-add-tests","title":"4. Add Tests","text":"<pre><code># tests/test_custom_feature.py\nimport pytest\nfrom ai_ops.custom_module import custom_function\n\ndef test_custom_function():\n    \"\"\"Test custom functionality.\"\"\"\n    result = custom_function(param=\"value\")\n    assert result == expected_result\n\n@pytest.mark.asyncio\nasync def test_async_custom_function():\n    \"\"\"Test async functionality.\"\"\"\n    result = await async_custom_function()\n    assert result is not None\n</code></pre>"},{"location":"dev/extending.html#5-update-documentation","title":"5. Update Documentation","text":"<p>Add documentation for your extension:</p> <ul> <li>Update relevant <code>.md</code> files</li> <li>Add code examples</li> <li>Document configuration</li> <li>Include usage instructions</li> </ul>"},{"location":"dev/extending.html#6-run-tests-and-linting","title":"6. Run Tests and Linting","text":"<pre><code># Run tests\ninvoke tests\n\n# Run linting\ninvoke lint\n\n# Format code\ninvoke format\n</code></pre>"},{"location":"dev/extending.html#7-submit-pull-request","title":"7. Submit Pull Request","text":"<ul> <li>Push your branch to GitHub</li> <li>Open a pull request</li> <li>Describe your changes</li> <li>Reference related issues</li> </ul>"},{"location":"dev/extending.html#best-practices","title":"Best Practices","text":""},{"location":"dev/extending.html#code-style","title":"Code Style","text":"<ol> <li>Follow PEP 8: Python code style guidelines</li> <li>Use Type Hints: Add type annotations</li> <li>Write Docstrings: Document all public functions/classes</li> <li>Keep It Simple: KISS principle</li> </ol>"},{"location":"dev/extending.html#testing","title":"Testing","text":"<ol> <li>Write Unit Tests: Test individual components</li> <li>Write Integration Tests: Test component interactions</li> <li>Test Edge Cases: Handle error conditions</li> <li>Mock External Services: Use mocks for external APIs</li> </ol>"},{"location":"dev/extending.html#documentation","title":"Documentation","text":"<ol> <li>Document All Public APIs: Clear function/class documentation</li> <li>Provide Examples: Show how to use features</li> <li>Update User Docs: If user-facing changes</li> <li>Keep It Current: Update docs with code changes</li> </ol>"},{"location":"dev/extending.html#security","title":"Security","text":"<ol> <li>Validate Input: Never trust user input</li> <li>Use Secrets Properly: Store credentials securely</li> <li>Follow Least Privilege: Minimal permissions</li> <li>Audit Logging: Log security-relevant actions</li> </ol>"},{"location":"dev/extending.html#performance","title":"Performance","text":"<ol> <li>Profile First: Measure before optimizing</li> <li>Use Caching: Cache expensive operations</li> <li>Async When Possible: Use async for I/O operations</li> <li>Monitor Resources: Track memory and CPU usage</li> </ol>"},{"location":"dev/extending.html#common-tasks","title":"Common Tasks","text":""},{"location":"dev/extending.html#adding-a-new-field-to-llmmodel","title":"Adding a New Field to LLMModel","text":"<pre><code># 1. Update model\nclass LLMModel(PrimaryModel):\n    # ... existing fields ...\n    new_field = models.CharField(max_length=100, blank=True)\n\n# 2. Create migration\n# nautobot-server makemigrations ai_ops\n\n# 3. Update forms\nclass LLMModelForm(forms.NautobotModelForm):\n    class Meta:\n        fields = \"__all__\"  # Or add \"new_field\" explicitly\n\n# 4. Update serializer\nclass LLMModelSerializer(serializers.NautobotModelSerializer):\n    class Meta:\n        model = LLMModel\n        fields = \"__all__\"\n\n# 5. Update tests and documentation\n</code></pre>"},{"location":"dev/extending.html#adding-a-custom-api-endpoint","title":"Adding a Custom API Endpoint","text":"<pre><code># ai_ops/api/views.py\nfrom rest_framework.decorators import api_view\nfrom rest_framework.response import Response\n\n@api_view(['POST'])\ndef custom_endpoint(request):\n    \"\"\"Custom API endpoint.\"\"\"\n    data = request.data\n    # Process request\n    result = process_data(data)\n    return Response({\"result\": result})\n\n# ai_ops/api/urls.py\nfrom django.urls import path\nfrom .views import custom_endpoint\n\nurlpatterns = [\n    # ... existing patterns ...\n    path(\"custom-endpoint/\", custom_endpoint, name=\"custom_endpoint\"),\n]\n</code></pre>"},{"location":"dev/extending.html#adding-a-new-navigation-item","title":"Adding a New Navigation Item","text":"<pre><code># ai_ops/navigation.py\nfrom nautobot.apps.ui import NavMenuItem\n\nnew_item = NavMenuItem(\n    link=\"plugins:ai_ops:custom_view\",\n    name=\"Custom Feature\",\n    permissions=[\"ai_ops.view_custommodel\"],\n)\n\n# Add to appropriate group\nconfiguration_items = (\n    # ... existing items ...\n    new_item,\n)\n</code></pre>"},{"location":"dev/extending.html#extending-llm-providers","title":"Extending LLM Providers","text":"<p>The AI Ops App supports multiple LLM providers and provides a flexible system for adding support for new providers without modifying core code.</p>"},{"location":"dev/extending.html#supported-built-in-providers","title":"Supported Built-in Providers","text":"<p>The app includes built-in support for the following providers:</p> <ol> <li>Ollama - Local open-source LLM runtime (default)</li> <li>OpenAI - ChatGPT, GPT-4, and other OpenAI models</li> <li>Azure AI - Azure OpenAI deployments</li> <li>Anthropic - Claude models</li> <li>HuggingFace - Models hosted on HuggingFace Hub</li> </ol>"},{"location":"dev/extending.html#understanding-the-provider-architecture","title":"Understanding the Provider Architecture","text":"<p>The provider system consists of three components:</p> <ol> <li>Provider Model: Stores provider configuration in the database</li> <li>Provider Handler: Implements the actual LLM initialization logic</li> <li>Registry: Maps provider names to handler classes for dynamic lookup</li> </ol>"},{"location":"dev/extending.html#creating-a-custom-provider","title":"Creating a Custom Provider","text":"<p>To add support for a new LLM provider (e.g., Cohere, Replicate, etc.):</p>"},{"location":"dev/extending.html#step-1-create-a-provider-handler","title":"Step 1: Create a Provider Handler","text":"<p>Create a new handler class that inherits from <code>BaseLLMProviderHandler</code>:</p> <pre><code># in your application or plugin\nfrom ai_ops.helpers.providers.base import BaseLLMProviderHandler\n\nclass MyCustomProvider(BaseLLMProviderHandler):\n    \"\"\"Handler for MyCustomProvider LLM integration.\n\n    Reference: https://docs.langchain.com/oss/python/integrations/chat/my_custom_provider\n    \"\"\"\n\n    async def get_chat_model(\n        self,\n        model_name: str,\n        api_key: str | None = None,\n        temperature: float = 0.0,\n        **kwargs,\n    ):\n        \"\"\"Get a chat model instance for MyCustomProvider.\n\n        Args:\n            model_name: The model identifier (e.g., 'model-name')\n            api_key: API key for authentication\n            temperature: Temperature setting (0.0 to 2.0)\n            **kwargs: Additional provider-specific parameters\n\n        Returns:\n            A LangChain chat model instance\n\n        Raises:\n            ImportError: If required libraries are not installed\n            ValueError: If required configuration is missing\n        \"\"\"\n        try:\n            from langchain_my_custom import ChatMyCustom\n        except ImportError as e:\n            raise ImportError(\n                \"langchain-my-custom is required. \"\n                \"Install it with: pip install langchain-my-custom\"\n            ) from e\n\n        # Get required configuration from self.config or environment\n        api_endpoint = self.config.get(\"api_endpoint\")\n        if not api_endpoint:\n            raise ValueError(\"api_endpoint configuration is required\")\n\n        if not api_key:\n            raise ValueError(\"API key is required\")\n\n        # Initialize and return the chat model\n        return ChatMyCustom(\n            model=model_name,\n            api_key=api_key,\n            api_endpoint=api_endpoint,\n            temperature=temperature,\n            **kwargs,\n        )\n\n    def validate_config(self) -&gt; None:\n        \"\"\"Validate provider configuration (optional).\n\n        Called during handler initialization to validate that\n        required configuration values are present.\n\n        Raises:\n            ValueError: If configuration is invalid\n        \"\"\"\n        required_fields = [\"api_endpoint\"]\n        missing_fields = [f for f in required_fields if f not in self.config]\n        if missing_fields:\n            raise ValueError(\n                f\"Missing required configuration fields: {', '.join(missing_fields)}\"\n            )\n</code></pre>"},{"location":"dev/extending.html#step-2-register-the-provider","title":"Step 2: Register the Provider","text":"<p>Register your provider handler at application startup:</p> <pre><code># in your app's apps.py or initialization code\nfrom ai_ops.helpers.providers import register_provider\nfrom .my_providers import MyCustomProvider\n\nclass MyAppConfig(AppConfig):\n    \"\"\"Configuration for my app.\"\"\"\n\n    def ready(self):\n        \"\"\"Register custom providers when app is ready.\"\"\"\n        register_provider(\"my_custom\", MyCustomProvider)\n</code></pre>"},{"location":"dev/extending.html#step-3-create-a-provider-instance-in-the-database","title":"Step 3: Create a Provider Instance in the Database","text":"<p>Create the provider configuration through the Nautobot UI or Django admin:</p> <pre><code># Via Django shell or management command\nfrom ai_ops.models import Provider\n\nProvider.objects.create(\n    name=\"my_custom\",\n    description=\"My Custom LLM Provider\",\n    documentation_url=\"https://docs.example.com/llm\",\n    config_schema={\n        \"api_endpoint\": \"https://api.example.com/v1\",\n        \"additional_setting\": \"value\",\n    },\n    is_enabled=True,\n)\n</code></pre>"},{"location":"dev/extending.html#step-4-create-an-llm-model-using-your-provider","title":"Step 4: Create an LLM Model Using Your Provider","text":"<p>Create an LLM model that uses your new provider:</p> <pre><code>from ai_ops.models import LLMModel, Provider\n\nprovider = Provider.objects.get(name=\"my_custom\")\n\nLLMModel.objects.create(\n    name=\"my-model\",\n    provider=provider,\n    description=\"My custom model\",\n    model_secret_key=\"my-secret-api-key\",  # Name of Secret object in Nautobot\n    temperature=0.7,\n    cache_ttl=300,\n)\n</code></pre>"},{"location":"dev/extending.html#provider-configuration-schema","title":"Provider Configuration Schema","text":"<p>The <code>config_schema</code> JSONField in the Provider model stores provider-specific configuration. This allows admins to configure settings without code changes.</p> <p>Example configurations:</p> <pre><code># OpenAI\n{\n    \"organization\": \"my-org-id\",\n}\n\n# Azure AI\n{\n    \"api_version\": \"2024-02-15-preview\",\n    \"azure_endpoint\": \"https://my-resource.openai.azure.com/\",\n}\n\n# Custom Provider\n{\n    \"api_endpoint\": \"https://api.example.com/v1\",\n    \"max_retries\": 3,\n    \"timeout_seconds\": 30,\n}\n</code></pre> <p>The handler can access these values via <code>self.config</code>:</p> <pre><code>async def get_chat_model(self, ...):\n    api_endpoint = self.config.get(\"api_endpoint\")\n    max_retries = self.config.get(\"max_retries\", 3)\n    timeout = self.config.get(\"timeout_seconds\", 30)\n    # ...\n</code></pre>"},{"location":"dev/extending.html#provider-handler-best-practices","title":"Provider Handler Best Practices","text":"<ol> <li>Error Handling: Raise clear, actionable errors with helpful messages</li> <li>Logging: Log initialization and errors for debugging</li> <li>Configuration Validation: Validate required config in <code>validate_config()</code></li> <li>Documentation: Include docstrings and reference links</li> <li>Async: Implement <code>async def get_chat_model()</code> for proper async/await patterns</li> <li>Type Hints: Use Python 3.10+ type hints (e.g., <code>str | None</code> not <code>Optional[str]</code>)</li> <li>Secrets Management: Use the <code>model_secret_key</code> Secret object for API keys</li> <li>Kwargs Support: Accept <code>**kwargs</code> to pass through additional parameters</li> </ol>"},{"location":"dev/extending.html#using-custom-providers-in-chat","title":"Using Custom Providers in Chat","text":"<p>Once registered and configured, custom providers are available for:</p> <ol> <li>Default Model Selection: Set a model using your custom provider as the default</li> <li>Admin Provider Override: Admins can select from enabled providers per-conversation</li> <li>API Calls: Use the provider programmatically:</li> </ol> <pre><code>from ai_ops.helpers.get_llm_model import get_llm_model_async\n\n# Use specific model\nllm = await get_llm_model_async(model_name=\"my-model\")\n\n# Override provider\nllm = await get_llm_model_async(\n    model_name=\"my-model\",\n    provider=\"my_custom\"\n)\n\n# With temperature override\nllm = await get_llm_model_async(\n    model_name=\"my-model\",\n    temperature=0.5\n)\n</code></pre>"},{"location":"dev/extending.html#example-adding-cohere-support","title":"Example: Adding Cohere Support","text":"<p>Here's a complete example of adding Cohere provider support:</p> <pre><code># my_plugin/providers.py\nfrom ai_ops.helpers.providers.base import BaseLLMProviderHandler\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass CohereHandler(BaseLLMProviderHandler):\n    \"\"\"Handler for Cohere LLM provider.\"\"\"\n\n    async def get_chat_model(\n        self,\n        model_name: str,\n        api_key: str | None = None,\n        temperature: float = 0.0,\n        **kwargs,\n    ):\n        try:\n            from langchain_cohere import ChatCohere\n        except ImportError as e:\n            raise ImportError(\n                \"langchain-cohere is required. \"\n                \"Install it with: pip install langchain-cohere\"\n            ) from e\n\n        if not api_key:\n            raise ValueError(\"Cohere API key is required\")\n\n        logger.info(f\"Initializing ChatCohere with model={model_name}\")\n\n        return ChatCohere(\n            model=model_name,\n            cohere_api_key=api_key,\n            temperature=temperature,\n            **kwargs,\n        )\n\n# my_plugin/apps.py\nfrom django.apps import AppConfig\nfrom ai_ops.helpers.providers import register_provider\n\nclass MyPluginConfig(AppConfig):\n    default_auto_field = 'django.db.models.BigAutoField'\n    name = 'my_plugin'\n\n    def ready(self):\n        from .providers import CohereHandler\n        register_provider(\"cohere\", CohereHandler)\n</code></pre> <p>Then create the provider in the database:</p> <pre><code>Provider.objects.create(\n    name=\"cohere\",\n    description=\"Cohere language models\",\n    documentation_url=\"https://docs.cohere.com/\",\n    config_schema={},\n    is_enabled=True,\n)\n</code></pre>"},{"location":"dev/extending.html#resources","title":"Resources","text":"<ul> <li>Nautobot Apps Documentation</li> <li>LangGraph Documentation</li> <li>LangChain Documentation</li> <li>MCP Protocol Specification</li> <li>Django Documentation</li> </ul>"},{"location":"dev/extending.html#getting-help","title":"Getting Help","text":"<ul> <li>Open an Issue: For bugs or feature requests</li> <li>GitHub Discussions: For questions and ideas</li> <li>Code Review: Request review from maintainers</li> <li>Community: Join the Nautobot community</li> </ul>"},{"location":"dev/extending.html#contributing-guidelines","title":"Contributing Guidelines","text":"<p>See Contributing for detailed contribution guidelines including:</p> <ul> <li>Code of conduct</li> <li>Development process</li> <li>Pull request requirements</li> <li>Review process</li> </ul> <p>Thank you for contributing to the AI Ops App!</p>"},{"location":"dev/release_checklist.html","title":"Release Checklist","text":"<p>This document is intended for app maintainers and outlines the steps to perform when releasing a new version of the app.</p> <p>Important</p> <p>Before starting, make sure your local <code>develop</code>, <code>main</code>, and (if applicable) the current LTM branch are all up to date with upstream!</p> <pre><code>git fetch\ngit switch develop &amp;&amp; git pull # and repeat for main/ltm\n</code></pre> <p>Choose your own adventure:</p> <ul> <li>LTM release? Jump here.</li> <li>Patch release from <code>develop</code>? Jump here.</li> <li>Minor release? Continue with Minor Version Bumps and then All Releases from <code>develop</code>.</li> </ul>"},{"location":"dev/release_checklist.html#minor-version-bumps","title":"Minor Version Bumps","text":""},{"location":"dev/release_checklist.html#update-requirements","title":"Update Requirements","text":"<p>Every minor version release should refresh <code>poetry.lock</code>, so that it lists the most recent stable release of each package. To do this:</p> <ol> <li>Run <code>poetry update --dry-run</code> to have Poetry automatically tell you what package updates are available and the versions it would upgrade to. This requires an existing environment created from the lock file (i.e. via <code>poetry install</code>).</li> <li>Review each requirement's release notes for any breaking or otherwise noteworthy changes.</li> <li>Run <code>poetry update &lt;package&gt;</code> to update the package versions in <code>poetry.lock</code> as appropriate.</li> <li>If a required package requires updating to a new release not covered in the version constraints for a package as defined in <code>pyproject.toml</code>, (e.g. <code>Django ~3.1.7</code> would never install <code>Django &gt;=4.0.0</code>), update it manually in <code>pyproject.toml</code>.</li> <li>Run <code>poetry install</code> to install the refreshed versions of all required packages.</li> <li>Run all tests (<code>poetry run invoke tests</code>) and check that the UI and API function as expected.</li> </ol>"},{"location":"dev/release_checklist.html#update-documentation","title":"Update Documentation","text":"<p>If there are any changes to the compatibility matrix (such as a bump in the minimum supported Nautobot version), update it accordingly.</p> <p>Commit any resulting changes from the following sections to the documentation before proceeding with the release.</p> <p>Tip</p> <p>Fire up the documentation server in your development environment with <code>poetry run mkdocs serve</code>! This allows you to view the documentation site locally (the link is in the output of the command) and automatically rebuilds it as you make changes.</p>"},{"location":"dev/release_checklist.html#verify-the-installation-and-upgrade-steps","title":"Verify the Installation and Upgrade Steps","text":"<p>Follow the installation instructions to perform a new production installation of the app. If possible, also test the upgrade process from the previous released version.</p> <p>The goal of this step is to walk through the entire install process as documented to make sure nothing there needs to be changed or updated, to catch any errors or omissions in the documentation, and to ensure that it is current with each release.</p>"},{"location":"dev/release_checklist.html#all-releases-from-develop","title":"All Releases from <code>develop</code>","text":""},{"location":"dev/release_checklist.html#verify-ci-build-status","title":"Verify CI Build Status","text":"<p>Ensure that continuous integration testing on the <code>develop</code> branch is completing successfully.</p>"},{"location":"dev/release_checklist.html#bump-the-version","title":"Bump the Version","text":"<p>Update the package version using <code>poetry version</code> if necessary (poetry docs). This command shows the current version of the project or bumps the version of the project and writes the new version back to <code>pyproject.toml</code> if a valid bump rule is provided.</p> <p>The new version must be a valid semver string or a valid bump rule: <code>patch</code>, <code>minor</code>, <code>major</code>, <code>prepatch</code>, <code>preminor</code>, <code>premajor</code>, <code>prerelease</code>. Always try to use a bump rule when you can.</p> <p>Warning</p> <p>This guide uses <code>1.4.2</code> as the new version in its examples, so change it to match the version you bumped to in the previous step! Every. single. time. you. copy/paste commands!</p> <p>Display the current version with no arguments:</p> <pre><code>&gt; poetry version\nai-ops 1.0.0-beta.2\n</code></pre> <p>Bump pre-release versions using <code>prerelease</code>:</p> <pre><code>&gt; poetry version prerelease\nBumping version from 1.0.0-beta.2 to 1.0.0-beta.3\n</code></pre> <p>For major versions, use <code>major</code>:</p> <pre><code>&gt; poetry version major\nBumping version from 1.0.0-beta.2 to 1.0.0\n</code></pre> <p>For patch versions, use <code>minor</code>:</p> <pre><code>&gt; poetry version minor\nBumping version from 1.0.0 to 1.1.0\n</code></pre> <p>And lastly, for patch versions, you guessed it, use <code>patch</code>:</p> <pre><code>&gt; poetry version patch\nBumping version from 1.1.0 to 1.1.1\n</code></pre>"},{"location":"dev/release_checklist.html#update-the-changelog","title":"Update the Changelog","text":"<p>Note</p> <ul> <li>This project uses <code>towncrier</code> to track human readable changes, so all merged PRs will have one or more entries in the release notes.</li> <li>The changelog must adhere to the Keep a Changelog style guide for any manual changes you may need to make.</li> <li>You will need to have the project's poetry environment built at this stage, as the towncrier command runs locally only. If you don't have it, run <code>poetry install</code> first.</li> <li>You can also set the version explicitly with <code>invoke generate-release-notes --version 1.4.2</code> if it needs to be different from what's in <code>pyproject.toml</code>.</li> </ul> <p>First, create a release branch off of <code>develop</code> (<code>git switch -c release-1.4.2 develop</code>) and automatically generate release notes with <code>invoke generate-release-notes</code>.</p> <p>If you're releasing a new major or minor version, this will create a new <code>docs/admin/release_notes/version_{major}.{minor}.md</code> file. Please fill in the <code>Release Overview</code> section in that file manually with a user-friendly summary of the most notable changes!</p> <p>Stage any remaining files (e.g. <code>git add mkdocs.yml pyproject.toml</code>) and check the diffs to verify all of the changes are correct (<code>git diff --cached</code>). For a new release of <code>1.4.2</code>, this will update the release notes in <code>docs/admin/release_notes/version_1.4.md</code>, stage that file in git, and <code>git rm</code> all the fragments that have now been incorporated into the release notes.</p> <p>Commit <code>git commit -m \"Release v1.4.2\"</code> and <code>git push</code> the staged changes.</p>"},{"location":"dev/release_checklist.html#submit-release-pull-request","title":"Submit Release Pull Request","text":"<p>Submit a pull request titled <code>Release v1.4.2</code> to merge your release branch into <code>main</code>. Copy the documented release notes into the pull request's body.</p> <p>Important</p> <p>Do not squash merge this branch into <code>main</code>. Make sure to select <code>Create a merge commit</code> when merging in GitHub.</p> <p>Once CI has completed on the PR, merge it.</p>"},{"location":"dev/release_checklist.html#create-a-new-release-in-github","title":"Create a New Release in GitHub","text":"<p>Draft a new release with the following parameters.</p> <ul> <li>Tag: Input current version (e.g. <code>v1.4.2</code>) and select <code>Create new tag: v1.4.2 on publish</code></li> <li>Target: <code>main</code></li> <li>Title: Version and date (e.g. <code>v1.4.2 - 2024-04-02</code>)</li> </ul> <p>Click \"Generate Release Notes\" and edit the auto-generated content as follows:</p> <ul> <li>Change the entries generated by GitHub to only the usernames of the contributors. e.g. <code>* Updated dockerfile by @nautobot_user in https://github.com/kvncampos/nautobot-ai-ops/pull/123</code> -&gt; <code>* @nautobot_user</code>.<ul> <li>This should give you the list for the new <code>Contributors</code> section.</li> <li>Make sure there are no duplicated entries.</li> </ul> </li> <li>Replace the content of the <code>What's Changed</code> section with the description of changes from the release PR (what towncrier generated).</li> <li>If it exists, leave the <code>New Contributors</code> list as it is.</li> </ul> <p>The release notes should look as follows:</p> <pre><code>## What's Changed\n\n**Towncrier generated Changed/Fixed/Housekeeping etc. sections here**\n\n## Contributors\n\n* @alice\n* @bob\n\n## New Contributors\n\n* @bob\n\n**Full Changelog**: https://github.com/kvncampos/nautobot-ai-ops/compare/v1.4.1...v1.4.2\n</code></pre> <p>Publish the release!</p>"},{"location":"dev/release_checklist.html#create-a-pr-from-main-back-to-develop","title":"Create a PR from <code>main</code> back to <code>develop</code>","text":"<p>First, sync your <code>main</code> branch with upstream changes: <code>git switch main &amp;&amp; git pull</code>.</p> <p>Create a new branch from <code>main</code> called <code>release-1.4.2-to-develop</code> and use <code>poetry version prepatch</code> to bump the development version to the next release.</p> <p>For example, if you just released <code>v1.4.2</code>:</p> <pre><code>&gt; git switch -c release-1.4.2-to-develop main\nSwitched to a new branch 'release-1.4.2-to-develop'\n\n&gt; poetry version prepatch\nBumping version from 1.4.2 to 1.4.3a1\n\n&gt; git add pyproject.toml &amp;&amp; git commit -m \"Bump version\"\n\n&gt; git push\n</code></pre> <p>Important</p> <p>Do not squash merge this branch into <code>develop</code>. Make sure to select <code>Create a merge commit</code> when merging in GitHub.</p> <p>Open a new PR from <code>release-1.4.2-to-develop</code> against <code>develop</code>, wait for CI to pass, and merge it.</p>"},{"location":"dev/release_checklist.html#final-checks","title":"Final checks","text":"<p>At this stage, the CI should be running or finished for the <code>v1.4.2</code> tag and a package successfully published to PyPI and added into the GitHub Release. Double check that's the case.</p> <p>Documentation should also have been built for the tag on ReadTheDocs and if you're reading this page online, refresh it and look for the new version in the little version fly-out menu down at the bottom right of the page.</p> <p>All done!</p>"},{"location":"dev/release_checklist.html#ltm-releases","title":"LTM Releases","text":"<p>For projects maintaining a Nautobot LTM compatible release, all development and release management is done through the <code>ltm-x.y</code> branch. The <code>x.y</code> relates to the LTM version of Nautobot it's compatible with, for example <code>1.6</code>.</p> <p>The process is similar to releasing from <code>develop</code>, but there is no need for post-release branch syncing because you'll release directly from the LTM branch:</p> <ol> <li>Make sure your <code>ltm-1.6</code> branch is passing CI.</li> <li>Create a release branch from the <code>ltm-1.6</code> branch: <code>git switch -c release-1.2.3 ltm-1.6</code>.</li> <li>Bump up the patch version <code>poetry version patch</code>. If you're backporting a feature instead of bugfixes, bump the minor version instead with <code>poetry version minor</code>.</li> <li>Generate the release notes: <code>invoke generate-release-notes --version 1.2.3</code>.</li> <li>Move the release notes from the generated <code>docs/admin/release_notes/version_X.Y.md</code> to <code>docs/admin/release_notes/version_1.2.md</code>.</li> <li>Add all the changes and <code>git commit -m \"Release v1.2.3\"</code>, then <code>git push</code>.</li> <li>Open a new PR against <code>ltm-1.6</code>. Once CI is passing in the PR, <code>Create a merge commit</code> (don't squash!).</li> <li>Create a New Release in GitHub - use the same steps documented here.</li> <li>Open a separate PR against <code>develop</code> to synchronize all LTM release changelogs into the latest version of the docs for visibility.</li> </ol>"},{"location":"dev/code_reference/index.html","title":"Code Reference","text":"<p>This section contains auto-generated code reference documentation extracted from docstrings throughout the AI Ops application using mkdocstrings.</p>"},{"location":"dev/code_reference/index.html#available-modules","title":"Available Modules","text":"<ul> <li>API Serializers and Views - REST API components for AI Ops</li> <li>Helper Functions - Utility functions for LLM configuration and management</li> <li>Core Models - Django models for LLM providers, models, middleware, and MCP servers</li> <li>Package Documentation - Main package structure and components</li> </ul> <p>Each module includes detailed docstrings explaining parameters, return values, and usage examples.</p>"},{"location":"dev/code_reference/agents.html","title":"AI Agents","text":"<p>This page documents the AI agent implementations in the AI Ops App.</p>"},{"location":"dev/code_reference/agents.html#overview","title":"Overview","text":"<p>The AI Ops App uses LangGraph to create stateful AI agents that can interact with users and external systems. The app provides two agent implementations:</p> <ul> <li>Multi-MCP Agent: Production agent supporting multiple MCP servers (recommended)</li> <li>Single-MCP Agent: Simplified agent for single MCP server scenarios</li> </ul>"},{"location":"dev/code_reference/agents.html#multi-mcp-agent","title":"Multi-MCP Agent","text":"<p>The Multi-MCP Agent is the production-ready implementation that supports connecting to multiple Model Context Protocol servers simultaneously.</p>"},{"location":"dev/code_reference/agents.html#key-features","title":"Key Features","text":"<ul> <li>Multiple MCP Server Support: Connect to any number of MCP servers</li> <li>Application-Level Caching: Caches MCP client and tools for performance</li> <li>Health-Based Server Selection: Only uses servers with \"Healthy\" status</li> <li>Automatic Tool Discovery: Discovers tools from all healthy MCP servers</li> <li>Checkpointing: Maintains conversation history using Redis</li> <li>Graceful Degradation: Continues working even if some MCP servers fail</li> </ul>"},{"location":"dev/code_reference/agents.html#architecture","title":"Architecture","text":"<pre><code>User Message \u2192 Multi-MCP Agent \u2192 LangGraph State Graph\n                      \u2193\n                Azure OpenAI Model\n                      \u2193\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2193                      \u2193\n    MCP Server 1             MCP Server 2\n    (Tools A, B, C)          (Tools D, E, F)\n           \u2193                      \u2193\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2193\n                  Response\n</code></pre>"},{"location":"dev/code_reference/agents.html#core-functions","title":"Core Functions","text":""},{"location":"dev/code_reference/agents.html#get_or_create_mcp_client","title":"get_or_create_mcp_client","text":"<pre><code>async def get_or_create_mcp_client(\n    force_refresh: bool = False\n) -&gt; Tuple[Optional[MultiServerMCPClient], List]:\n    \"\"\"Get or create MCP client with application-level caching.\n\n    Args:\n        force_refresh: Force cache refresh even if not expired\n\n    Returns:\n        Tuple of (client, tools) or (None, []) if no healthy servers\n    \"\"\"\n</code></pre> <p>Cache Behavior: - Cache TTL: 5 minutes (300 seconds) - Thread-safe with asyncio lock - Invalidated on server status changes - Force refresh available when needed</p> <p>Server Selection: - Queries for servers with <code>status=\"Healthy\"</code> - Protocol must be <code>\"http\"</code> - Failed servers automatically excluded</p>"},{"location":"dev/code_reference/agents.html#warm_mcp_cache","title":"warm_mcp_cache","text":"<pre><code>async def warm_mcp_cache():\n    \"\"\"Warm the MCP client cache on application startup.\"\"\"\n</code></pre> <p>Called during app initialization to pre-populate the cache. Reduces first-request latency.</p>"},{"location":"dev/code_reference/agents.html#process_message","title":"process_message","text":"<pre><code>async def process_message(\n    user_message: str,\n    thread_id: str,\n    checkpointer=None\n) -&gt; str:\n    \"\"\"Process a user message through the multi-MCP agent.\n\n    Args:\n        user_message: The user's input message\n        thread_id: Unique identifier for conversation thread\n        checkpointer: LangGraph checkpointer for state persistence\n\n    Returns:\n        The agent's response as a string\n    \"\"\"\n</code></pre> <p>Message Processing Flow: 1. Get or create cached MCP client 2. Retrieve LLM model configuration 3. Create LangGraph state graph 4. Process message with conversation history 5. Return agent response</p>"},{"location":"dev/code_reference/agents.html#state-management","title":"State Management","text":"<p>The agent uses <code>MessagesState</code> for conversation tracking:</p> <pre><code>class MessagesState(TypedDict):\n    \"\"\"State for the agent graph.\"\"\"\n    messages: Annotated[List[BaseMessage], add_messages]\n</code></pre> <p>The <code>add_messages</code> reducer: - Properly accumulates messages - Works with checkpointers for persistence - Maintains conversation context</p>"},{"location":"dev/code_reference/agents.html#configuration","title":"Configuration","text":""},{"location":"dev/code_reference/agents.html#cache-settings","title":"Cache Settings","text":"<pre><code># Cache TTL: 5 minutes\nCACHE_TTL_SECONDS = 300\n\n# Cache structure\n_mcp_client_cache = {\n    \"client\": None,\n    \"tools\": None, \n    \"timestamp\": None,\n    \"server_count\": 0,\n}\n</code></pre>"},{"location":"dev/code_reference/agents.html#http-client-configuration","title":"HTTP Client Configuration","text":"<pre><code>def httpx_client_factory(**kwargs):\n    \"\"\"Factory for httpx client with SSL verification disabled.\n\n    Note: verify=False is intentional for internal MCP servers\n    with self-signed certificates.\n    \"\"\"\n    return httpx.AsyncClient(verify=False, timeout=30.0, **kwargs)\n</code></pre>"},{"location":"dev/code_reference/agents.html#usage-example","title":"Usage Example","text":"<pre><code>from ai_ops.agents.multi_mcp_agent import process_message\nfrom ai_ops.checkpointer import get_checkpointer\n\n# Process a message with conversation history\nasync with get_checkpointer() as checkpointer:\n    response = await process_message(\n        user_message=\"What is the status of my network?\",\n        thread_id=\"user-session-123\",\n        checkpointer=checkpointer\n    )\n    print(response)\n</code></pre>"},{"location":"dev/code_reference/agents.html#error-handling","title":"Error Handling","text":"<p>The agent handles various error scenarios:</p> <p>No Healthy MCP Servers: - Returns None for client - Agent continues without MCP tools - Logs warning message</p> <p>MCP Server Connection Failures: - Failed servers excluded from operations - Cache updated to reflect failures - Agent uses remaining healthy servers</p> <p>LLM API Errors: - Errors propagated to caller - Consider implementing retry logic - Check Azure OpenAI rate limits</p>"},{"location":"dev/code_reference/agents.html#single-mcp-agent","title":"Single-MCP Agent","text":"<p>The Single-MCP Agent is a simplified implementation for scenarios with only one MCP server.</p>"},{"location":"dev/code_reference/agents.html#key-features_1","title":"Key Features","text":"<ul> <li>Single Server Focus: Designed for one MCP server</li> <li>Simpler Configuration: Less complex than multi-server setup</li> <li>Same LangGraph Architecture: Uses LangGraph state management</li> <li>Production-Ready: Suitable for focused use cases</li> </ul>"},{"location":"dev/code_reference/agents.html#when-to-use","title":"When to Use","text":"<p>Use the Single-MCP Agent when: - You have only one MCP server - Simpler architecture is preferred - You want explicit server selection - Testing and development scenarios</p> <p>Use the Multi-MCP Agent when: - You have multiple MCP servers - Dynamic server management needed - Production deployment with scaling - Automatic failover desired</p>"},{"location":"dev/code_reference/agents.html#system-prompts","title":"System Prompts","text":"<p>The agents use system prompts to define their behavior:</p>"},{"location":"dev/code_reference/agents.html#multi-mcp-system-prompt","title":"Multi-MCP System Prompt","text":"<pre><code># ai_ops/prompts/multi_mcp_system_prompt.py\nSYSTEM_PROMPT = \"\"\"\nYou are a helpful AI assistant powered by Azure OpenAI...\n\"\"\"\n</code></pre> <p>The multi-MCP prompt: - Explains multi-server capabilities - Provides guidance on tool usage - Sets expectations for responses - Defines assistant personality</p>"},{"location":"dev/code_reference/agents.html#single-system-prompt","title":"Single System Prompt","text":"<pre><code># ai_ops/prompts/system_prompt.py  \nSYSTEM_PROMPT = \"\"\"\nYou are a helpful AI assistant...\n\"\"\"\n</code></pre> <p>Simpler prompt for single-server scenarios.</p>"},{"location":"dev/code_reference/agents.html#customizing-prompts","title":"Customizing Prompts","text":"<p>To customize agent behavior:</p> <ol> <li>Edit the prompt files</li> <li>Modify system instructions</li> <li>Add domain-specific guidance</li> <li>Restart Nautobot to apply changes</li> </ol> <p>Example customization: <pre><code>SYSTEM_PROMPT = \"\"\"\nYou are a network operations AI assistant.\n\nYour role is to help network engineers with:\n- Device configuration questions\n- Troubleshooting connectivity issues  \n- Best practices for network automation\n- Nautobot data queries\n\nAlways:\n- Provide step-by-step guidance\n- Cite sources when possible\n- Admit when you're unsure\n- Suggest consulting documentation or experts for critical decisions\n\nAvailable tools: {tool_names}\n\"\"\"\n</code></pre></p>"},{"location":"dev/code_reference/agents.html#langgraph-integration","title":"LangGraph Integration","text":""},{"location":"dev/code_reference/agents.html#state-graph-structure","title":"State Graph Structure","text":"<p>Both agents use LangGraph's StateGraph:</p> <pre><code>from langgraph.graph import StateGraph\n\n# Create graph\nworkflow = StateGraph(MessagesState)\n\n# Add nodes\nworkflow.add_node(\"agent\", agent_node)\nworkflow.add_node(\"tools\", tool_node)\n\n# Add edges\nworkflow.add_edge(START, \"agent\")\nworkflow.add_conditional_edges(\"agent\", should_continue)\nworkflow.add_edge(\"tools\", \"agent\")\n\n# Compile with checkpointer\ngraph = workflow.compile(checkpointer=checkpointer)\n</code></pre>"},{"location":"dev/code_reference/agents.html#message-flow","title":"Message Flow","text":"<ol> <li>START \u2192 agent: Initial message routing</li> <li>agent \u2192 tools (conditional): If tool calls needed</li> <li>tools \u2192 agent: Tool results fed back</li> <li>agent \u2192 END: Final response</li> </ol>"},{"location":"dev/code_reference/agents.html#checkpointing","title":"Checkpointing","text":"<p>Conversation state persisted using Redis:</p> <pre><code>from ai_ops.checkpointer import get_checkpointer\n\nasync with get_checkpointer() as checkpointer:\n    result = await graph.ainvoke(\n        {\"messages\": [HumanMessage(content=user_message)]},\n        config={\"configurable\": {\"thread_id\": thread_id}}\n    )\n</code></pre> <p>Thread IDs: - Unique identifier per conversation - Typically uses session ID - Enables multi-user support - Isolates conversations</p>"},{"location":"dev/code_reference/agents.html#mcp-client-integration","title":"MCP Client Integration","text":""},{"location":"dev/code_reference/agents.html#multiservermcpclient","title":"MultiServerMCPClient","text":"<p>The langchain-mcp-adapters library provides MCP integration:</p> <pre><code>from langchain_mcp_adapters.client import MultiServerMCPClient\n\n# Create client\nclient = MultiServerMCPClient(\n    connections={\n        \"server1\": {\"url\": \"https://mcp1.example.com\"},\n        \"server2\": {\"url\": \"https://mcp2.example.com\"},\n    },\n    httpx_client_factory=httpx_client_factory\n)\n\n# Get tools\nasync with client:\n    tools = await client.get_tools()\n</code></pre>"},{"location":"dev/code_reference/agents.html#tool-discovery","title":"Tool Discovery","text":"<p>Tools are automatically discovered from MCP servers:</p> <pre><code># Tools include metadata\nfor tool in tools:\n    print(f\"Tool: {tool.name}\")\n    print(f\"Description: {tool.description}\")\n    print(f\"Schema: {tool.args_schema}\")\n</code></pre>"},{"location":"dev/code_reference/agents.html#tool-execution","title":"Tool Execution","text":"<p>LangGraph automatically handles tool execution:</p> <ol> <li>Agent decides to call tool</li> <li>ToolNode executes tool call</li> <li>Results returned to agent</li> <li>Agent incorporates results in response</li> </ol>"},{"location":"dev/code_reference/agents.html#performance-considerations","title":"Performance Considerations","text":""},{"location":"dev/code_reference/agents.html#caching-strategy","title":"Caching Strategy","text":"<p>Why Cache? - MCP client initialization is expensive - Tool discovery requires network calls - Multiple users share the same servers</p> <p>Cache Invalidation: - Time-based (5 minute TTL) - Manual refresh via <code>force_refresh=True</code> - Server status changes (handled by health checks)</p>"},{"location":"dev/code_reference/agents.html#async-architecture","title":"Async Architecture","text":"<p>All agent operations are async:</p> <pre><code># Good - async/await\nasync def handle_message(message):\n    response = await process_message(message, thread_id)\n    return response\n\n# Bad - blocking\ndef handle_message(message):\n    # This won't work - process_message is async\n    response = process_message(message, thread_id)\n</code></pre>"},{"location":"dev/code_reference/agents.html#rate-limiting","title":"Rate Limiting","text":"<p>Consider Azure OpenAI rate limits:</p> <ul> <li>Monitor API usage</li> <li>Implement retry logic</li> <li>Use appropriate models for workload</li> <li>Request quota increases if needed</li> </ul>"},{"location":"dev/code_reference/agents.html#testing-agents","title":"Testing Agents","text":""},{"location":"dev/code_reference/agents.html#unit-testing","title":"Unit Testing","text":"<pre><code>import pytest\nfrom ai_ops.agents.multi_mcp_agent import get_or_create_mcp_client\n\n@pytest.mark.asyncio\nasync def test_mcp_client_cache():\n    # First call - cache miss\n    client1, tools1 = await get_or_create_mcp_client()\n\n    # Second call - cache hit\n    client2, tools2 = await get_or_create_mcp_client()\n\n    # Should return same client\n    assert client1 is client2\n</code></pre>"},{"location":"dev/code_reference/agents.html#integration-testing","title":"Integration Testing","text":"<pre><code>@pytest.mark.asyncio  \nasync def test_process_message():\n    from ai_ops.agents.multi_mcp_agent import process_message\n    from ai_ops.checkpointer import get_checkpointer\n\n    async with get_checkpointer() as checkpointer:\n        response = await process_message(\n            user_message=\"Hello\",\n            thread_id=\"test-thread\",\n            checkpointer=checkpointer\n        )\n\n        assert isinstance(response, str)\n        assert len(response) &gt; 0\n</code></pre>"},{"location":"dev/code_reference/agents.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"dev/code_reference/agents.html#agent-not-responding","title":"Agent Not Responding","text":"<p>Check these items:</p> <ol> <li>LLM Model Configuration: Verify default model exists</li> <li>Azure Connectivity: Test API endpoint access</li> <li>Logs: Review for error messages</li> <li>Permissions: Ensure proper API key permissions</li> </ol>"},{"location":"dev/code_reference/agents.html#mcp-tools-not-available","title":"MCP Tools Not Available","text":"<p>Verify:</p> <ol> <li>Server Health: Check MCP server status</li> <li>Cache State: Try force refresh</li> <li>Network: Test server URL accessibility</li> <li>Protocol: Ensure HTTP protocol selected</li> </ol>"},{"location":"dev/code_reference/agents.html#conversation-history-lost","title":"Conversation History Lost","text":"<p>Check:</p> <ol> <li>Redis Connection: Verify Redis is running</li> <li>Thread IDs: Ensure consistent thread_id usage</li> <li>Checkpointer: Confirm checkpointer passed correctly</li> <li>Cleanup Job: Check if cleanup removed history</li> </ol>"},{"location":"dev/code_reference/agents.html#best-practices","title":"Best Practices","text":""},{"location":"dev/code_reference/agents.html#agent-usage","title":"Agent Usage","text":"<ol> <li>Use Multi-MCP Agent: For production deployments</li> <li>Implement Error Handling: Wrap agent calls in try/except</li> <li>Monitor Performance: Track response times and errors</li> <li>Cache Awareness: Understand caching behavior</li> </ol>"},{"location":"dev/code_reference/agents.html#prompt-engineering","title":"Prompt Engineering","text":"<ol> <li>Be Specific: Clear instructions in system prompts</li> <li>Provide Context: Include relevant background</li> <li>Set Boundaries: Define what agent should/shouldn't do</li> <li>Test Thoroughly: Validate prompt changes</li> </ol>"},{"location":"dev/code_reference/agents.html#production-deployment","title":"Production Deployment","text":"<ol> <li>Scale Redis: Ensure adequate Redis capacity</li> <li>Monitor Rate Limits: Watch Azure OpenAI usage</li> <li>Health Checks: Regular MCP server monitoring</li> <li>Logging: Comprehensive logging for debugging</li> </ol>"},{"location":"dev/code_reference/agents.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Models - Database models documentation</li> <li>Helpers - Helper functions</li> <li>Checkpointer - Checkpoint configuration (see <code>ai_ops/checkpointer.py</code>)</li> </ul>"},{"location":"dev/code_reference/api.html","title":"AI Ops API Package","text":""},{"location":"dev/code_reference/api.html#ai_ops.api","title":"<code>ai_ops.api</code>","text":"<p>REST API module for ai_ops app.</p>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers","title":"<code>serializers</code>","text":"<p>API serializers for ai_ops.</p>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.LLMMiddlewareSerializer","title":"<code>LLMMiddlewareSerializer</code>","text":"<p>               Bases: <code>NautobotModelSerializer</code>, <code>TaggedModelSerializerMixin</code></p> <p>LLMMiddleware Serializer.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class LLMMiddlewareSerializer(NautobotModelSerializer, TaggedModelSerializerMixin):  # pylint: disable=too-many-ancestors\n    \"\"\"LLMMiddleware Serializer.\"\"\"\n\n    class Meta:\n        \"\"\"Meta attributes.\"\"\"\n\n        model = models.LLMMiddleware\n        fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.LLMMiddlewareSerializer.Meta","title":"<code>Meta</code>","text":"<p>Meta attributes.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class Meta:\n    \"\"\"Meta attributes.\"\"\"\n\n    model = models.LLMMiddleware\n    fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.LLMModelSerializer","title":"<code>LLMModelSerializer</code>","text":"<p>               Bases: <code>NautobotModelSerializer</code>, <code>TaggedModelSerializerMixin</code></p> <p>LLMModel Serializer.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class LLMModelSerializer(NautobotModelSerializer, TaggedModelSerializerMixin):  # pylint: disable=too-many-ancestors\n    \"\"\"LLMModel Serializer.\"\"\"\n\n    class Meta:\n        \"\"\"Meta attributes.\"\"\"\n\n        model = models.LLMModel\n        fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.LLMModelSerializer.Meta","title":"<code>Meta</code>","text":"<p>Meta attributes.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class Meta:\n    \"\"\"Meta attributes.\"\"\"\n\n    model = models.LLMModel\n    fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.LLMProviderSerializer","title":"<code>LLMProviderSerializer</code>","text":"<p>               Bases: <code>NautobotModelSerializer</code>, <code>TaggedModelSerializerMixin</code></p> <p>LLMProvider Serializer.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class LLMProviderSerializer(NautobotModelSerializer, TaggedModelSerializerMixin):  # pylint: disable=too-many-ancestors\n    \"\"\"LLMProvider Serializer.\"\"\"\n\n    class Meta:\n        \"\"\"Meta attributes.\"\"\"\n\n        model = models.LLMProvider\n        fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.LLMProviderSerializer.Meta","title":"<code>Meta</code>","text":"<p>Meta attributes.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class Meta:\n    \"\"\"Meta attributes.\"\"\"\n\n    model = models.LLMProvider\n    fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.MCPServerSerializer","title":"<code>MCPServerSerializer</code>","text":"<p>               Bases: <code>NautobotModelSerializer</code>, <code>TaggedModelSerializerMixin</code></p> <p>MCPServer Serializer.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class MCPServerSerializer(NautobotModelSerializer, TaggedModelSerializerMixin):  # pylint: disable=too-many-ancestors\n    \"\"\"MCPServer Serializer.\"\"\"\n\n    class Meta:\n        \"\"\"Meta attributes.\"\"\"\n\n        model = models.MCPServer\n        fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.MCPServerSerializer.Meta","title":"<code>Meta</code>","text":"<p>Meta attributes.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class Meta:\n    \"\"\"Meta attributes.\"\"\"\n\n    model = models.MCPServer\n    fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.MiddlewareTypeSerializer","title":"<code>MiddlewareTypeSerializer</code>","text":"<p>               Bases: <code>NautobotModelSerializer</code>, <code>TaggedModelSerializerMixin</code></p> <p>MiddlewareType Serializer.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class MiddlewareTypeSerializer(NautobotModelSerializer, TaggedModelSerializerMixin):  # pylint: disable=too-many-ancestors\n    \"\"\"MiddlewareType Serializer.\"\"\"\n\n    class Meta:\n        \"\"\"Meta attributes.\"\"\"\n\n        model = models.MiddlewareType\n        fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.serializers.MiddlewareTypeSerializer.Meta","title":"<code>Meta</code>","text":"<p>Meta attributes.</p> Source code in <code>ai_ops/api/serializers.py</code> <pre><code>class Meta:\n    \"\"\"Meta attributes.\"\"\"\n\n    model = models.MiddlewareType\n    fields = \"__all__\"\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.urls","title":"<code>urls</code>","text":"<p>Django API urlpatterns declaration for ai_ops app.</p>"},{"location":"dev/code_reference/api.html#ai_ops.api.views","title":"<code>views</code>","text":"<p>API views for ai_ops.</p>"},{"location":"dev/code_reference/api.html#ai_ops.api.views.LLMMiddlewareViewSet","title":"<code>LLMMiddlewareViewSet</code>","text":"<p>               Bases: <code>NautobotModelViewSet</code></p> <p>LLMMiddleware viewset.</p> Source code in <code>ai_ops/api/views.py</code> <pre><code>class LLMMiddlewareViewSet(NautobotModelViewSet):  # pylint: disable=too-many-ancestors\n    \"\"\"LLMMiddleware viewset.\"\"\"\n\n    queryset = models.LLMMiddleware.objects.all()\n    serializer_class = serializers.LLMMiddlewareSerializer\n    filterset_class = filters.LLMMiddlewareFilterSet\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.views.LLMModelViewSet","title":"<code>LLMModelViewSet</code>","text":"<p>               Bases: <code>NautobotModelViewSet</code></p> <p>LLMModel viewset.</p> Source code in <code>ai_ops/api/views.py</code> <pre><code>class LLMModelViewSet(NautobotModelViewSet):  # pylint: disable=too-many-ancestors\n    \"\"\"LLMModel viewset.\"\"\"\n\n    queryset = models.LLMModel.objects.all()\n    serializer_class = serializers.LLMModelSerializer\n    filterset_class = filters.LLMModelFilterSet\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.views.LLMProviderViewSet","title":"<code>LLMProviderViewSet</code>","text":"<p>               Bases: <code>NautobotModelViewSet</code></p> <p>LLMProvider viewset.</p> Source code in <code>ai_ops/api/views.py</code> <pre><code>class LLMProviderViewSet(NautobotModelViewSet):  # pylint: disable=too-many-ancestors\n    \"\"\"LLMProvider viewset.\"\"\"\n\n    queryset = models.LLMProvider.objects.all()\n    serializer_class = serializers.LLMProviderSerializer\n    filterset_class = filters.LLMProviderFilterSet\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.views.MCPServerViewSet","title":"<code>MCPServerViewSet</code>","text":"<p>               Bases: <code>NautobotModelViewSet</code></p> <p>MCPServer viewset.</p> Source code in <code>ai_ops/api/views.py</code> <pre><code>class MCPServerViewSet(NautobotModelViewSet):  # pylint: disable=too-many-ancestors\n    \"\"\"MCPServer viewset.\"\"\"\n\n    queryset = models.MCPServer.objects.all()\n    serializer_class = serializers.MCPServerSerializer\n    filterset_class = filters.MCPServerFilterSet\n\n    # Option for modifying the default HTTP methods:\n    # http_method_names = [\"get\", \"post\", \"put\", \"patch\", \"delete\", \"head\", \"options\", \"trace\"]\n\n    @action(detail=True, methods=[\"post\"], url_path=\"health-check\")\n    def health_check(self, request, pk=None):\n        \"\"\"Perform health check on MCP server.\"\"\"\n        mcp_server = self.get_object()\n\n        try:\n            # Build health check URL using base URL + health_check path\n            # Note: health check is NOT at the MCP endpoint, it's at the base URL\n            health_path = getattr(mcp_server, \"health_check\", \"/health\")\n            health_url = f\"{mcp_server.url.rstrip('/')}{health_path}\"\n\n            # Only disable SSL verification for internal MCP servers\n            verify_ssl = mcp_server.mcp_type != \"internal\"\n\n            # Perform health check\n            with httpx.Client(verify=verify_ssl, timeout=5.0) as client:\n                response = client.get(health_url)\n\n                if response.status_code == 200:\n                    return Response(\n                        {\n                            \"success\": True,\n                            \"message\": f\"MCP Server '{mcp_server.name}' is healthy\",\n                            \"details\": f\"Successfully connected to {health_url}\",\n                            \"url\": health_url,\n                        }\n                    )\n                else:\n                    return Response(\n                        {\n                            \"success\": False,\n                            \"message\": f\"MCP Server '{mcp_server.name}' health check failed\",\n                            \"details\": f\"HTTP {response.status_code} from {health_url}\",\n                            \"url\": health_url,\n                        }\n                    )\n        except httpx.TimeoutException:\n            health_path = getattr(mcp_server, \"health_check\", \"/health\")\n            health_url = f\"{mcp_server.url.rstrip('/')}{health_path}\"\n            return Response(\n                {\n                    \"success\": False,\n                    \"message\": f\"MCP Server '{mcp_server.name}' health check timed out\",\n                    \"details\": f\"No response after 5 seconds from {health_url}\",\n                    \"url\": health_url,\n                }\n            )\n        except Exception as e:\n            health_path = getattr(mcp_server, \"health_check\", \"/health\")\n            health_url = f\"{mcp_server.url.rstrip('/')}{health_path}\"\n\n            # Only expose exception details in LOCAL environment for security\n            env = get_environment()\n            if env == NautobotEnvironment.LOCAL:\n                error_details = str(e)\n            else:\n                error_details = \"Connection error. Please check server configuration.\"\n\n            return Response(\n                {\n                    \"success\": False,\n                    \"message\": f\"MCP Server '{mcp_server.name}' health check failed\",\n                    \"details\": error_details,\n                    \"url\": health_url,\n                }\n            )\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.views.MCPServerViewSet.health_check","title":"<code>health_check(request, pk=None)</code>","text":"<p>Perform health check on MCP server.</p> Source code in <code>ai_ops/api/views.py</code> <pre><code>@action(detail=True, methods=[\"post\"], url_path=\"health-check\")\ndef health_check(self, request, pk=None):\n    \"\"\"Perform health check on MCP server.\"\"\"\n    mcp_server = self.get_object()\n\n    try:\n        # Build health check URL using base URL + health_check path\n        # Note: health check is NOT at the MCP endpoint, it's at the base URL\n        health_path = getattr(mcp_server, \"health_check\", \"/health\")\n        health_url = f\"{mcp_server.url.rstrip('/')}{health_path}\"\n\n        # Only disable SSL verification for internal MCP servers\n        verify_ssl = mcp_server.mcp_type != \"internal\"\n\n        # Perform health check\n        with httpx.Client(verify=verify_ssl, timeout=5.0) as client:\n            response = client.get(health_url)\n\n            if response.status_code == 200:\n                return Response(\n                    {\n                        \"success\": True,\n                        \"message\": f\"MCP Server '{mcp_server.name}' is healthy\",\n                        \"details\": f\"Successfully connected to {health_url}\",\n                        \"url\": health_url,\n                    }\n                )\n            else:\n                return Response(\n                    {\n                        \"success\": False,\n                        \"message\": f\"MCP Server '{mcp_server.name}' health check failed\",\n                        \"details\": f\"HTTP {response.status_code} from {health_url}\",\n                        \"url\": health_url,\n                    }\n                )\n    except httpx.TimeoutException:\n        health_path = getattr(mcp_server, \"health_check\", \"/health\")\n        health_url = f\"{mcp_server.url.rstrip('/')}{health_path}\"\n        return Response(\n            {\n                \"success\": False,\n                \"message\": f\"MCP Server '{mcp_server.name}' health check timed out\",\n                \"details\": f\"No response after 5 seconds from {health_url}\",\n                \"url\": health_url,\n            }\n        )\n    except Exception as e:\n        health_path = getattr(mcp_server, \"health_check\", \"/health\")\n        health_url = f\"{mcp_server.url.rstrip('/')}{health_path}\"\n\n        # Only expose exception details in LOCAL environment for security\n        env = get_environment()\n        if env == NautobotEnvironment.LOCAL:\n            error_details = str(e)\n        else:\n            error_details = \"Connection error. Please check server configuration.\"\n\n        return Response(\n            {\n                \"success\": False,\n                \"message\": f\"MCP Server '{mcp_server.name}' health check failed\",\n                \"details\": error_details,\n                \"url\": health_url,\n            }\n        )\n</code></pre>"},{"location":"dev/code_reference/api.html#ai_ops.api.views.MiddlewareTypeViewSet","title":"<code>MiddlewareTypeViewSet</code>","text":"<p>               Bases: <code>NautobotModelViewSet</code></p> <p>MiddlewareType viewset.</p> Source code in <code>ai_ops/api/views.py</code> <pre><code>class MiddlewareTypeViewSet(NautobotModelViewSet):  # pylint: disable=too-many-ancestors\n    \"\"\"MiddlewareType viewset.\"\"\"\n\n    queryset = models.MiddlewareType.objects.all()\n    serializer_class = serializers.MiddlewareTypeSerializer\n    filterset_class = filters.MiddlewareTypeFilterSet\n</code></pre>"},{"location":"dev/code_reference/helpers.html","title":"Helper Modules","text":"<p>This page documents helper functions and utilities in the AI Ops App.</p>"},{"location":"dev/code_reference/helpers.html#azure-model-helper","title":"Azure Model Helper","text":"<p>The <code>get_azure_model</code> module provides functions to retrieve and configure Azure OpenAI models.</p>"},{"location":"dev/code_reference/helpers.html#get_azure_model","title":"get_azure_model()","text":"<pre><code>def get_azure_model(\n    model_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    azure_deployment: Optional[str] = None,\n    azure_endpoint: Optional[str] = None,\n    api_key: Optional[str] = None,\n    api_version: Optional[str] = None,\n    **kwargs\n) -&gt; AzureChatOpenAI:\n    \"\"\"Get Azure OpenAI model with environment-aware configuration.\n\n    Args:\n        model_name: Name of LLMModel to use (production only)\n        temperature: Override temperature setting\n        azure_deployment: Override deployment name\n        azure_endpoint: Override endpoint URL\n        api_key: Override API key\n        api_version: Override API version\n        **kwargs: Additional arguments for AzureChatOpenAI\n\n    Returns:\n        Configured AzureChatOpenAI instance\n    \"\"\"\n</code></pre>"},{"location":"dev/code_reference/helpers.html#environment-detection","title":"Environment Detection","text":"<p>The function automatically detects the environment based on hostname:</p> <pre><code>def get_environment() -&gt; str:\n    \"\"\"Detect the current environment.\n\n    Returns:\n        \"LAB\", \"NONPROD\", or \"PROD\"\n    \"\"\"\n    hostname = socket.gethostname().lower()\n\n    if \"nonprod\" in hostname:\n        return \"NONPROD\"\n    elif \"prod\" in hostname or \"prd\" in hostname:\n        return \"PROD\"\n    else:\n        return \"LAB\"\n</code></pre>"},{"location":"dev/code_reference/helpers.html#lab-environment","title":"LAB Environment","text":"<p>In LAB (local development), uses environment variables:</p> <pre><code># Uses these environment variables:\n# - AZURE_OPENAI_API_KEY\n# - AZURE_OPENAI_ENDPOINT\n# - AZURE_OPENAI_DEPLOYMENT_NAME\n# - AZURE_OPENAI_API_VERSION (optional)\n\nmodel = get_azure_model()  # Uses .env file\n</code></pre>"},{"location":"dev/code_reference/helpers.html#production-environment","title":"Production Environment","text":"<p>In NONPROD/PROD, uses database configuration:</p> <pre><code>from ai_ops.helpers.get_azure_model import get_azure_model\n\n# Use default model\nmodel = get_azure_model()\n\n# Use specific model by name\nmodel = get_azure_model(model_name=\"gpt-4-turbo\")\n\n# Override temperature\nmodel = get_azure_model(temperature=0.7)\n\n# Pass additional kwargs\nmodel = get_azure_model(max_tokens=2000, request_timeout=30)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#async-version","title":"Async Version","text":"<p>For async operations, use <code>get_azure_model_async()</code>:</p> <pre><code>async def my_async_function():\n    model = await get_azure_model_async()\n    result = await model.ainvoke(\"Hello\")\n</code></pre>"},{"location":"dev/code_reference/helpers.html#usage-examples","title":"Usage Examples","text":""},{"location":"dev/code_reference/helpers.html#basic-usage","title":"Basic Usage","text":"<pre><code>from ai_ops.helpers.get_azure_model import get_azure_model\n\n# Get default model with default settings\nmodel = get_azure_model()\n\n# Use the model\nresponse = model.invoke(\"What is Nautobot?\")\nprint(response.content)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#custom-temperature","title":"Custom Temperature","text":"<pre><code># Creative responses\ncreative_model = get_azure_model(temperature=0.9)\n\n# Deterministic responses  \ndeterministic_model = get_azure_model(temperature=0.0)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#specific-model-selection","title":"Specific Model Selection","text":"<pre><code># Use a specific model from database\nfast_model = get_azure_model(model_name=\"gpt-4-turbo\")\ndetailed_model = get_azure_model(model_name=\"gpt-4o\")\n</code></pre>"},{"location":"dev/code_reference/helpers.html#complete-override","title":"Complete Override","text":"<pre><code># Completely manual configuration (bypasses database)\ncustom_model = get_azure_model(\n    azure_deployment=\"my-deployment\",\n    azure_endpoint=\"https://my-resource.openai.azure.com/\",\n    api_key=\"my-api-key\",\n    api_version=\"2024-02-15-preview\",\n    temperature=0.5\n)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#with-additional-parameters","title":"With Additional Parameters","text":"<pre><code># Pass AzureChatOpenAI parameters\nmodel = get_azure_model(\n    max_tokens=1000,\n    request_timeout=60,\n    max_retries=3,\n    streaming=True\n)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#information-helpers","title":"Information Helpers","text":"<p>Provides utility functions for retrieving information from Nautobot.</p>"},{"location":"dev/code_reference/helpers.html#get_default_status","title":"get_default_status()","text":"<pre><code>def get_default_status() -&gt; Status:\n    \"\"\"Get or create the default 'Active' status.\n\n    Returns:\n        Status object for 'Active' status\n    \"\"\"\n</code></pre> <p>Used as default value for StatusField in models.</p>"},{"location":"dev/code_reference/helpers.html#usage-example","title":"Usage Example","text":"<pre><code>from ai_ops.helpers.get_info import get_default_status\nfrom ai_ops.models import MCPServer\n\n# Create MCP server with default status\nserver = MCPServer.objects.create(\n    name=\"my-server\",\n    status=get_default_status(),  # Sets to 'Active' by default\n    url=\"https://example.com\"\n)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#langgraph-serializers","title":"LangGraph Serializers","text":"<p>Custom serializers for LangGraph-specific data types.</p>"},{"location":"dev/code_reference/helpers.html#checkpointtupleserializer","title":"CheckpointTupleSerializer","text":"<p>Serializes LangGraph checkpoint tuples for Django REST Framework:</p> <pre><code>class CheckpointTupleSerializer(serializers.Serializer):\n    \"\"\"Serializer for LangGraph CheckpointTuple objects.\"\"\"\n\n    checkpoint = serializers.JSONField()\n    metadata = serializers.JSONField()\n    parent_config = serializers.JSONField(required=False)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#usage-in-api-views","title":"Usage in API Views","text":"<pre><code>from ai_ops.helpers.langgraph_serializers import CheckpointTupleSerializer\n\n# Serialize checkpoint data\ncheckpoint_data = {\n    \"checkpoint\": {...},\n    \"metadata\": {...},\n    \"parent_config\": {...}\n}\n\nserializer = CheckpointTupleSerializer(data=checkpoint_data)\nif serializer.is_valid():\n    return Response(serializer.data)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#common-utilities","title":"Common Utilities","text":"<p>The <code>ai_ops/helpers/common/</code> package provides shared utilities.</p>"},{"location":"dev/code_reference/helpers.html#api-handler","title":"API Handler","text":"<p>Provides utilities for API interactions and HTTP requests.</p>"},{"location":"dev/code_reference/helpers.html#constants","title":"Constants","text":"<p>Application-wide constants and configuration values.</p>"},{"location":"dev/code_reference/helpers.html#encoders","title":"Encoders","text":"<p>Custom JSON encoders for complex data types.</p>"},{"location":"dev/code_reference/helpers.html#enums","title":"Enums","text":"<p>Enumeration classes for the application.</p>"},{"location":"dev/code_reference/helpers.html#exceptions","title":"Exceptions","text":"<p>Custom exception classes.</p>"},{"location":"dev/code_reference/helpers.html#helper-functions","title":"Helper Functions","text":"<p>General-purpose helper functions.</p>"},{"location":"dev/code_reference/helpers.html#usage-examples_1","title":"Usage Examples","text":""},{"location":"dev/code_reference/helpers.html#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>from ai_ops.helpers.get_azure_model import get_azure_model\nfrom ai_ops.models import LLMModel, MCPServer\nfrom ai_ops.helpers.get_info import get_default_status\n\n# Create LLM model\nmodel_config = LLMModel.objects.create(\n    name=\"gpt-4o\",\n    description=\"Production GPT-4o model\",\n    model_secret_key=\"azure_api_key\",\n    azure_endpoint=\"https://my-resource.openai.azure.com/\",\n    api_version=\"2024-02-15-preview\",\n    is_default=True,\n    temperature=0.3\n)\n\n# Create MCP server\nmcp_server = MCPServer.objects.create(\n    name=\"my-mcp\",\n    status=get_default_status(),\n    protocol=\"http\",\n    url=\"https://mcp.internal.com\",\n    mcp_type=\"internal\"\n)\n\n# Get Azure model\nazure_model = get_azure_model()  # Uses default model from DB\n\n# Use the model\nresponse = azure_model.invoke(\"Explain Nautobot plugins\")\nprint(response.content)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#error-handling-example","title":"Error Handling Example","text":"<pre><code>from ai_ops.helpers.get_azure_model import get_azure_model\nfrom ai_ops.models import LLMModel\nfrom django.core.exceptions import ValidationError\n\ntry:\n    # Try to get model\n    model = get_azure_model(model_name=\"nonexistent-model\")\nexcept LLMModel.DoesNotExist:\n    print(\"Model not found, using default\")\n    model = get_azure_model()\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"dev/code_reference/helpers.html#async-helper-example","title":"Async Helper Example","text":"<pre><code>from ai_ops.helpers.get_azure_model import get_azure_model_async\nfrom ai_ops.agents.multi_mcp_agent import process_message\nfrom ai_ops.checkpointer import get_checkpointer\n\nasync def chat_workflow(user_message: str, session_id: str):\n    \"\"\"Complete async chat workflow.\"\"\"\n\n    # Get model asynchronously\n    model = await get_azure_model_async()\n\n    # Process message with conversation history\n    async with get_checkpointer() as checkpointer:\n        response = await process_message(\n            user_message=user_message,\n            thread_id=session_id,\n            checkpointer=checkpointer\n        )\n\n    return response\n\n# Usage\nimport asyncio\nresponse = asyncio.run(chat_workflow(\n    \"What devices are in my network?\",\n    \"user-123\"\n))\n</code></pre>"},{"location":"dev/code_reference/helpers.html#environment-variables","title":"Environment Variables","text":""},{"location":"dev/code_reference/helpers.html#required-variables-lab","title":"Required Variables (LAB)","text":"<pre><code># .env file for local development\nAZURE_OPENAI_API_KEY=your-api-key-here\nAZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\nAZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o\nAZURE_OPENAI_API_VERSION=2024-02-15-preview\n</code></pre>"},{"location":"dev/code_reference/helpers.html#required-variables-production","title":"Required Variables (Production)","text":"<pre><code># Redis configuration\nNAUTOBOT_REDIS_HOST=redis.internal.com\nNAUTOBOT_REDIS_PORT=6379\nNAUTOBOT_REDIS_PASSWORD=secure-password\nLANGGRAPH_REDIS_DB=2\n\n# Environment detection\n# (Hostname-based, no variable needed)\n</code></pre>"},{"location":"dev/code_reference/helpers.html#best-practices","title":"Best Practices","text":""},{"location":"dev/code_reference/helpers.html#model-configuration","title":"Model Configuration","text":"<ol> <li>Use Database in Production: Store configurations in LLMModel</li> <li>Use Environment Variables in LAB: Keep development flexible</li> <li>Secure API Keys: Always use Secrets in production</li> <li>Test Both Paths: Verify LAB and production configurations</li> </ol>"},{"location":"dev/code_reference/helpers.html#error-handling","title":"Error Handling","text":"<ol> <li>Wrap Helper Calls: Use try/except for error handling</li> <li>Provide Fallbacks: Default to safe configurations</li> <li>Log Errors: Help debugging with clear error messages</li> <li>Validate Inputs: Check parameters before using</li> </ol>"},{"location":"dev/code_reference/helpers.html#performance","title":"Performance","text":"<ol> <li>Cache Models: Reuse model instances when possible</li> <li>Async When Possible: Use async versions for async contexts</li> <li>Monitor API Calls: Track Azure OpenAI usage</li> <li>Optimize Temperature: Lower values can be faster</li> </ol>"},{"location":"dev/code_reference/helpers.html#testing","title":"Testing","text":"<ol> <li>Mock External Calls: Mock Azure API in tests</li> <li>Test Both Environments: LAB and production code paths</li> <li>Validate Configuration: Test model configuration retrieval</li> <li>Handle Failures: Test error scenarios</li> </ol>"},{"location":"dev/code_reference/helpers.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"dev/code_reference/helpers.html#common-issues","title":"Common Issues","text":"<p>\"No LLMModel instances exist in the database\" - Create at least one LLM model in the database - Mark one as default with <code>is_default=True</code></p> <p>\"model_secret_key is not configured\" - Set the <code>model_secret_key</code> field in LLMModel - Create the referenced Secret in Nautobot</p> <p>Environment variable not found (LAB) - Check <code>.env</code> file exists - Verify variable names are correct - Environment variables should be set in your shell or <code>.env</code> file (LAB environments may use python-dotenv for loading)</p> <p>Wrong environment detected - Check hostname matches environment patterns - Override by modifying <code>get_environment()</code> function - Verify environment-specific configuration</p>"},{"location":"dev/code_reference/helpers.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Models - Database models</li> <li>Agents - AI agent implementations</li> <li>Usage Examples - Practical examples (see <code>ai_ops/helpers/USAGE_EXAMPLES.md</code>)</li> <li>API - REST API documentation</li> </ul>"},{"location":"dev/code_reference/jobs.html","title":"Background Jobs","text":"<p>This page documents the background jobs provided by the AI Ops App.</p>"},{"location":"dev/code_reference/jobs.html#overview","title":"Overview","text":"<p>The AI Ops App includes Nautobot Jobs for automated maintenance tasks. Jobs can be run manually or scheduled for automatic execution.</p>"},{"location":"dev/code_reference/jobs.html#cleanup-checkpoints-job","title":"Cleanup Checkpoints Job","text":"<p>The Cleanup Checkpoints Job removes old conversation history from Redis to prevent unbounded growth.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.checkpoint_cleanup.CleanupCheckpointsJob","title":"<code>ai_ops.jobs.checkpoint_cleanup.CleanupCheckpointsJob</code>","text":"<p>               Bases: <code>Job</code></p> <p>Job to clean up old conversation checkpoints from Redis.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.checkpoint_cleanup.CleanupCheckpointsJob.Meta","title":"<code>Meta</code>","text":"<p>Meta class for CleanupCheckpointsJob.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.checkpoint_cleanup.CleanupCheckpointsJob.run","title":"<code>run()</code>","text":"<p>Entry point for the job.</p>"},{"location":"dev/code_reference/jobs.html#purpose","title":"Purpose","text":"<p>Conversation checkpoints are stored in Redis for maintaining chat history. Over time, these checkpoints accumulate and consume Redis memory. This job periodically cleans up old checkpoints based on a retention policy.</p>"},{"location":"dev/code_reference/jobs.html#job-details","title":"Job Details","text":"<ul> <li>Name: Cleanup Old Checkpoints</li> <li>Group: AI Agents</li> <li>Description: Clean up old LangGraph conversation checkpoints from Redis based on retention policy</li> <li>Scheduling: Can be scheduled for automatic execution</li> <li>Sensitive Variables: None</li> </ul>"},{"location":"dev/code_reference/jobs.html#how-it-works","title":"How It Works","text":"<pre><code>class CleanupCheckpointsJob(Job):\n    \"\"\"Job to clean up old conversation checkpoints from Redis.\"\"\"\n\n    def run(self):\n        \"\"\"Entry point for the job.\"\"\"\n        # Execute cleanup task\n        result = cleanup_old_checkpoints()\n\n        if result.get(\"success\"):\n            self.logger.info(\n                f\"\u2705 Checkpoint cleanup completed: \"\n                f\"processed {result['processed_count']} keys \"\n                f\"(retention: {result['retention_days']} days)\"\n            )\n        else:\n            self.logger.error(f\"\u274c Checkpoint cleanup failed: {result.get('error')}\")\n            raise Exception(f\"Cleanup failed: {result.get('error')}\")\n\n        return result\n</code></pre>"},{"location":"dev/code_reference/jobs.html#cleanup-task","title":"Cleanup Task","text":"<p>The underlying cleanup task is defined in <code>ai_ops/celery_tasks.py</code>:</p> <pre><code>def cleanup_old_checkpoints(retention_days: int = 30) -&gt; dict:\n    \"\"\"Clean up old LangGraph checkpoints from Redis.\n\n    Args:\n        retention_days: Number of days to retain checkpoints\n\n    Returns:\n        Dictionary with cleanup results:\n        {\n            \"success\": bool,\n            \"processed_count\": int,\n            \"deleted_count\": int,\n            \"retention_days\": int,\n            \"error\": str (if failed)\n        }\n    \"\"\"\n</code></pre>"},{"location":"dev/code_reference/jobs.html#retention-policy","title":"Retention Policy","text":"<p>Default Retention: 30 days</p> <p>Checkpoints older than the retention period are removed. The retention period is calculated from the checkpoint's timestamp.</p> <p>Configurable: The retention period can be adjusted by modifying the <code>cleanup_old_checkpoints()</code> function call.</p>"},{"location":"dev/code_reference/jobs.html#running-the-job","title":"Running the Job","text":""},{"location":"dev/code_reference/jobs.html#manual-execution","title":"Manual Execution","text":"<ol> <li>Navigate to Jobs &gt; Jobs in Nautobot</li> <li>Find AI Agents &gt; Cleanup Old Checkpoints</li> <li>Click Run Job Now</li> <li>Review the job log for results</li> </ol>"},{"location":"dev/code_reference/jobs.html#scheduled-execution","title":"Scheduled Execution","text":"<ol> <li>Navigate to Jobs &gt; Jobs</li> <li>Find AI Agents &gt; Cleanup Old Checkpoints</li> <li>Click Schedule Job</li> <li>Configure schedule:</li> <li>Name: Descriptive name for the schedule</li> <li>Interval: How often to run (e.g., daily, weekly)</li> <li>Start Time: When to start running</li> <li>Enabled: Check to activate the schedule</li> </ol> <p>Recommended Schedule: Daily or weekly, depending on usage volume.</p>"},{"location":"dev/code_reference/jobs.html#job-output","title":"Job Output","text":"<p>The job returns a dictionary with cleanup statistics:</p> <pre><code>{\n    \"success\": True,\n    \"processed_count\": 150,  # Total keys scanned\n    \"deleted_count\": 45,     # Keys deleted\n    \"retention_days\": 30,    # Retention period used\n    \"error\": None            # Error message if failed\n}\n</code></pre>"},{"location":"dev/code_reference/jobs.html#example-job-log","title":"Example Job Log","text":"<pre><code>2024-12-05 10:30:00 INFO Starting checkpoint cleanup task...\n2024-12-05 10:30:05 INFO \u2705 Checkpoint cleanup completed: processed 150 keys (retention: 30 days)\n2024-12-05 10:30:05 INFO Deleted 45 old checkpoint keys\n2024-12-05 10:30:05 SUCCESS Job completed successfully\n</code></pre>"},{"location":"dev/code_reference/jobs.html#mcp-server-health-check-job","title":"MCP Server Health Check Job","text":"<p>The MCP Server Health Check Job performs automated health monitoring of HTTP-based MCP servers to ensure they're operational.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.mcp_health_check.MCPServerHealthCheckJob","title":"<code>ai_ops.jobs.mcp_health_check.MCPServerHealthCheckJob</code>","text":"<p>               Bases: <code>Job</code></p> <p>Job to perform automated health checks on MCP servers.</p> <p>This job checks all HTTP MCP servers (excluding those with Vulnerable status) and updates their health status based on HTTP health check results.</p> <p>Features: - Parallel execution using ThreadPoolExecutor (1 worker per server, max 4 workers) - Retry logic: 2 verification checks (5s apart) before status change - Cache invalidation: Clears MCP client cache if any status changes - Skips servers with \"Vulnerable\" status - Skips servers with \"stdio\" protocol (only checks HTTP servers)</p> <p>Status change logic: - Healthy server + successful check = no change - Unhealthy server + failed check = no change - Status differs = perform 2 verification checks, then flip if confirmed</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.mcp_health_check.MCPServerHealthCheckJob.Meta","title":"<code>Meta</code>","text":"<p>Meta class for MCPServerHealthCheckJob.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.mcp_health_check.MCPServerHealthCheckJob.run","title":"<code>run()</code>","text":"<p>Entry point for the job.</p>"},{"location":"dev/code_reference/jobs.html#purpose_1","title":"Purpose","text":"<p>MCP servers are critical for providing tools and capabilities to AI agents. This job automatically:</p> <ul> <li>Checks all HTTP MCP servers for availability</li> <li>Updates server status based on health check results</li> <li>Implements retry logic to avoid false positives</li> <li>Invalidates agent cache when server status changes</li> <li>Runs in parallel for efficient checking</li> </ul>"},{"location":"dev/code_reference/jobs.html#job-details_1","title":"Job Details","text":"<ul> <li>Name: MCP Server Health Check</li> <li>Group: AI Agents</li> <li>Description: Perform automated health checks on HTTP MCP servers with retry logic and parallel execution</li> <li>Scheduling: Can be scheduled for automatic execution</li> <li>Hidden: Yes (typically triggered by scheduler, not manually run)</li> <li>Sensitive Variables: None</li> </ul>"},{"location":"dev/code_reference/jobs.html#key-features","title":"Key Features","text":"<ul> <li>Parallel Execution: Uses ThreadPoolExecutor (1 worker per server, max 4 workers)</li> <li>Retry Logic: 2 verification checks (5 seconds apart) before status change</li> <li>Cache Invalidation: Clears MCP client cache if any status changes</li> <li>Protocol Filtering: Only checks HTTP servers, skips STDIO protocol</li> <li>Status Filtering: Skips servers with \"Vulnerable\" status</li> </ul>"},{"location":"dev/code_reference/jobs.html#health-check-process","title":"Health Check Process","text":"<pre><code># For each HTTP MCP server:\n1. Send GET request to {server.url}{server.health_check}\n2. If response differs from current status:\n   a. Wait 5 seconds\n   b. Perform verification check\n   c. Wait 5 seconds  \n   d. Perform second verification check\n   e. If both verifications confirm: update status\n3. If any status changed: clear agent MCP cache\n</code></pre>"},{"location":"dev/code_reference/jobs.html#status-change-logic","title":"Status Change Logic","text":"<ul> <li>Healthy server + successful check = No change</li> <li>Unhealthy server + failed check = No change</li> <li>Status differs = Perform 2 verification checks, then flip if confirmed</li> </ul>"},{"location":"dev/code_reference/jobs.html#usage-example","title":"Usage Example","text":""},{"location":"dev/code_reference/jobs.html#manual-execution_1","title":"Manual Execution","text":"<ol> <li>Navigate to Jobs &gt; Jobs in Nautobot</li> <li>Find AI Agents &gt; MCP Server Health Check</li> <li>Click Run Job Now</li> <li>Review job log for health check results</li> </ol>"},{"location":"dev/code_reference/jobs.html#scheduled-execution_1","title":"Scheduled Execution","text":"<p>Recommended schedule: Every 5-15 minutes</p> <pre><code># Configure via Nautobot UI or programmatically\nfrom nautobot.extras.models import ScheduledJob\n\nScheduledJob.objects.create(\n    name=\"MCP Health Monitoring\",\n    job_model=\"ai_ops.jobs.mcp_health_check.MCPServerHealthCheckJob\",\n    interval=\"crontab\",\n    crontab=\"*/10 * * * *\",  # Every 10 minutes\n    enabled=True\n)\n</code></pre>"},{"location":"dev/code_reference/jobs.html#job-output_1","title":"Job Output","text":"<pre><code>{\n    \"success\": True,\n    \"checked_count\": 5,        # Number of servers checked\n    \"changed_count\": 1,        # Number of status changes\n    \"failed_count\": 1,         # Number of servers that failed\n    \"worker_count\": 4,         # Number of parallel workers used\n    \"cache_cleared\": True,     # Whether agent cache was invalidated\n    \"error\": None              # Error message if failed\n}\n</code></pre>"},{"location":"dev/code_reference/jobs.html#example-job-log_1","title":"Example Job Log","text":"<pre><code>2024-12-18 10:00:00 INFO Starting MCP server health checks...\n2024-12-18 10:00:02 INFO \u2705 MCP health check completed: 5 server(s) checked using 4 worker(s), 1 status change(s), 1 failure(s)\n2024-12-18 10:00:02 INFO \u2705 MCP client cache cleared due to status changes\n2024-12-18 10:00:02 WARNING \u26a0\ufe0f 1 server(s) changed status - check logs for details\n2024-12-18 10:00:02 SUCCESS Job completed successfully\n</code></pre>"},{"location":"dev/code_reference/jobs.html#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Parallel Workers: Max 4 workers to balance speed vs resource usage</li> <li>Timeout: Each health check times out after 10 seconds</li> <li>Verification Delay: 5 seconds between verification checks</li> <li>Total Time: Typically completes in 10-30 seconds for 5-10 servers</li> </ul>"},{"location":"dev/code_reference/jobs.html#middleware-cache-jobs","title":"Middleware Cache Jobs","text":"<p>The AI Ops App includes two job hooks that automatically manage middleware caching when models change.</p>"},{"location":"dev/code_reference/jobs.html#middleware-cache-invalidation-job","title":"Middleware Cache Invalidation Job","text":"<p>Automatically invalidates middleware cache when <code>LLMMiddleware</code> objects are created, updated, or deleted.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.middleware_cache_jobs.MiddlewareCacheInvalidationJob","title":"<code>ai_ops.jobs.middleware_cache_jobs.MiddlewareCacheInvalidationJob</code>","text":"<p>               Bases: <code>JobHookReceiver</code></p> <p>Job hook to invalidate middleware cache when LLMMiddleware objects change.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.middleware_cache_jobs.MiddlewareCacheInvalidationJob.Meta","title":"<code>Meta</code>","text":"<p>Meta class for MiddlewareCacheInvalidationJob.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.middleware_cache_jobs.MiddlewareCacheInvalidationJob.receive_job_hook","title":"<code>receive_job_hook(change, action, changed_object)</code>","text":"<p>Handle LLMMiddleware object changes.</p> <p>Parameters:</p> Name Type Description Default <code>change</code> <code>dict</code> <p>The ObjectChange instance</p> required <code>action</code> <code>str</code> <p>The action performed (create, update, delete)</p> required <code>changed_object</code> <code>object</code> <p>The LLMMiddleware instance that changed</p> required"},{"location":"dev/code_reference/jobs.html#purpose_2","title":"Purpose","text":"<p>Middleware configurations are cached for performance. When middleware settings change, the cache must be invalidated to ensure the agent uses current configurations.</p>"},{"location":"dev/code_reference/jobs.html#job-details_2","title":"Job Details","text":"<ul> <li>Name: Middleware Cache Invalidation</li> <li>Group: AI Agents (JobHookReceiver)</li> <li>Trigger: Automatic on LLMMiddleware changes</li> <li>Hidden: Yes (runs automatically)</li> <li>Type: JobHookReceiver</li> </ul>"},{"location":"dev/code_reference/jobs.html#trigger-events","title":"Trigger Events","text":"<p>Responds to these <code>LLMMiddleware</code> events: - Create: New middleware added to a model - Update: Middleware configuration changed - Delete: Middleware removed from a model</p>"},{"location":"dev/code_reference/jobs.html#example-log","title":"Example Log","text":"<pre><code>2024-12-18 10:15:00 INFO Middleware cache invalidation triggered: update on CacheMiddleware for model gpt-4o\n2024-12-18 10:15:00 INFO Middleware cache cleared. Previous state: {'cached_models': 3, 'total_middleware': 8}\n</code></pre>"},{"location":"dev/code_reference/jobs.html#default-model-cache-warming-job","title":"Default Model Cache Warming Job","text":"<p>Automatically warms middleware cache when a model is marked as the default model.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.middleware_cache_jobs.DefaultModelCacheWarmingJob","title":"<code>ai_ops.jobs.middleware_cache_jobs.DefaultModelCacheWarmingJob</code>","text":"<p>               Bases: <code>JobHookReceiver</code></p> <p>Job hook to warm middleware cache when default model changes.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.middleware_cache_jobs.DefaultModelCacheWarmingJob.Meta","title":"<code>Meta</code>","text":"<p>Meta class for DefaultModelCacheWarmingJob.</p>"},{"location":"dev/code_reference/jobs.html#ai_ops.jobs.middleware_cache_jobs.DefaultModelCacheWarmingJob.receive_job_hook","title":"<code>receive_job_hook(change, action, changed_object)</code>","text":"<p>Handle LLMModel default status changes.</p> <p>Parameters:</p> Name Type Description Default <code>change</code> <code>dict</code> <p>The ObjectChange instance</p> required <code>action</code> <code>str</code> <p>The action performed (create, update, delete)</p> required <code>changed_object</code> <code>object</code> <p>The LLMModel instance that changed</p> required"},{"location":"dev/code_reference/jobs.html#purpose_3","title":"Purpose","text":"<p>When a new default model is set, pre-loading its middleware configuration improves first-request performance for users.</p>"},{"location":"dev/code_reference/jobs.html#job-details_3","title":"Job Details","text":"<ul> <li>Name: Default Model Cache Warming</li> <li>Group: AI Agents (JobHookReceiver)</li> <li>Trigger: Automatic when <code>is_default=True</code> set on LLMModel</li> <li>Hidden: Yes (runs automatically)</li> <li>Type: JobHookReceiver</li> </ul>"},{"location":"dev/code_reference/jobs.html#trigger-events_1","title":"Trigger Events","text":"<p>Responds to: - LLMModel Update: When <code>is_default</code> field changes to <code>True</code></p>"},{"location":"dev/code_reference/jobs.html#example-log_1","title":"Example Log","text":"<pre><code>2024-12-18 10:20:00 INFO Default model cache warming triggered for model gpt-4o\n2024-12-18 10:20:01 INFO Middleware cache warmed. Cache state: {'cached_models': 1, 'total_middleware': 3}\n</code></pre>"},{"location":"dev/code_reference/jobs.html#cache-warming-process","title":"Cache Warming Process","text":"<pre><code># 1. Load default model's middleware configurations\nmodel = LLMModel.get_default_model()\nmiddlewares = model.middlewares.filter(is_active=True).order_by('priority')\n\n# 2. Initialize middleware instances\nfor middleware_config in middlewares:\n    middleware_instance = initialize_middleware(middleware_config)\n    cache[model.id][middleware_config.id] = middleware_instance\n\n# 3. Update cache metadata\ncache['last_warmed'] = datetime.now()\ncache['cached_models'].add(model.id)\n</code></pre>"},{"location":"dev/code_reference/jobs.html#checkpoint-storage","title":"Checkpoint Storage","text":""},{"location":"dev/code_reference/jobs.html#redis-key-structure","title":"Redis Key Structure","text":"<p>Checkpoints are stored in Redis with a specific key pattern:</p> <pre><code>checkpoint:{thread_id}:{checkpoint_id}\n</code></pre> <p>Example keys: <pre><code>checkpoint:user-session-abc123:2024-12-05T10:30:00\ncheckpoint:user-session-def456:2024-12-05T11:45:00\n</code></pre></p>"},{"location":"dev/code_reference/jobs.html#checkpoint-content","title":"Checkpoint Content","text":"<p>Each checkpoint stores: - Messages: Conversation history - Metadata: Timestamp, user info, etc. - Agent State: Current state of the agent</p>"},{"location":"dev/code_reference/jobs.html#redis-database","title":"Redis Database","text":"<p>Checkpoints use a separate Redis database: - Default Database: DB 2 - Configurable via: <code>LANGGRAPH_REDIS_DB</code> environment variable - Isolation: Separate from cache (DB 0) and Celery (DB 1)</p>"},{"location":"dev/code_reference/jobs.html#cleanup-process","title":"Cleanup Process","text":""},{"location":"dev/code_reference/jobs.html#step-by-step-process","title":"Step-by-Step Process","text":"<ol> <li> <p>Connect to Redis <pre><code>redis_client = get_redis_connection()\n</code></pre></p> </li> <li> <p>Scan for Checkpoint Keys <pre><code>for key in redis_client.scan_iter(match=\"checkpoint:*\"):\n    process_key(key)\n</code></pre></p> </li> <li> <p>Check Timestamp <pre><code>checkpoint_data = redis_client.get(key)\ntimestamp = extract_timestamp(checkpoint_data)\nage = now - timestamp\n</code></pre></p> </li> <li> <p>Delete Old Checkpoints <pre><code>if age &gt; retention_period:\n    redis_client.delete(key)\n    deleted_count += 1\n</code></pre></p> </li> <li> <p>Return Results <pre><code>return {\n    \"success\": True,\n    \"processed_count\": total_keys,\n    \"deleted_count\": deleted_keys,\n    \"retention_days\": retention_days\n}\n</code></pre></p> </li> </ol>"},{"location":"dev/code_reference/jobs.html#performance-considerations_1","title":"Performance Considerations","text":"<ul> <li>Scan vs Keys: Uses <code>SCAN</code> to avoid blocking Redis</li> <li>Batch Processing: Processes keys in batches</li> <li>Memory Efficient: Doesn't load all keys into memory</li> <li>Non-Blocking: Allows Redis to serve other requests</li> </ul>"},{"location":"dev/code_reference/jobs.html#monitoring","title":"Monitoring","text":""},{"location":"dev/code_reference/jobs.html#job-execution-status","title":"Job Execution Status","text":"<p>Monitor job execution through Nautobot:</p> <ol> <li>Navigate to Jobs &gt; Job Results</li> <li>Filter by job name: \"Cleanup Old Checkpoints\"</li> <li>Review execution history:</li> <li>Success/failure status</li> <li>Execution duration</li> <li>Number of keys processed</li> <li>Error messages if any</li> </ol>"},{"location":"dev/code_reference/jobs.html#redis-monitoring","title":"Redis Monitoring","text":"<p>Monitor Redis usage:</p> <pre><code># Connect to Redis\nredis-cli -h localhost -p 6379 -n 2\n\n# Count checkpoint keys\nSCAN 0 MATCH checkpoint:* COUNT 1000\n\n# Check memory usage\nINFO memory\n\n# Get database statistics\nINFO keyspace\n</code></pre>"},{"location":"dev/code_reference/jobs.html#metrics-to-track","title":"Metrics to Track","text":"<ul> <li>Checkpoint Count: Total number of checkpoints</li> <li>Redis Memory: Memory used by checkpoint database</li> <li>Cleanup Frequency: How often cleanup runs</li> <li>Deletion Rate: Number of checkpoints deleted per run</li> </ul>"},{"location":"dev/code_reference/jobs.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"dev/code_reference/jobs.html#job-fails-to-execute","title":"Job Fails to Execute","text":"<p>Check Redis Connectivity: <pre><code>from ai_ops.checkpointer import get_redis_connection\n\ntry:\n    redis_client = get_redis_connection()\n    redis_client.ping()\n    print(\"Redis connection OK\")\nexcept Exception as e:\n    print(f\"Redis connection failed: {e}\")\n</code></pre></p> <p>Verify Environment Variables: <pre><code>echo $NAUTOBOT_REDIS_HOST\necho $NAUTOBOT_REDIS_PORT\necho $LANGGRAPH_REDIS_DB\n</code></pre></p> <p>Check Redis Permissions: - Ensure Redis password is correct - Verify network connectivity - Check firewall rules</p>"},{"location":"dev/code_reference/jobs.html#no-checkpoints-deleted","title":"No Checkpoints Deleted","text":"<p>Possible Causes: - All checkpoints are within retention period - Checkpoint keys have different pattern - Wrong Redis database selected</p> <p>Verify Checkpoint Keys: <pre><code>redis-cli -h localhost -p 6379 -n 2 KEYS \"checkpoint:*\"\n</code></pre></p>"},{"location":"dev/code_reference/jobs.html#job-takes-too-long","title":"Job Takes Too Long","text":"<p>For Large Datasets: - Increase job timeout - Run during off-peak hours - Consider reducing retention period - Optimize Redis performance</p>"},{"location":"dev/code_reference/jobs.html#memory-not-freed","title":"Memory Not Freed","text":"<p>After cleanup, Redis memory may not immediately decrease:</p> <ol> <li> <p>Check Deleted Keys:    <pre><code>INFO stats\n</code></pre>    Look for <code>evicted_keys</code> or deleted count</p> </li> <li> <p>Redis Memory Reclaim:    Redis may not immediately release memory to OS</p> </li> <li>Memory reused for new keys</li> <li> <p>Run <code>MEMORY PURGE</code> (Redis 4.0+)</p> </li> <li> <p>Verify Cleanup Results:    Check job log for deleted count</p> </li> </ol>"},{"location":"dev/code_reference/jobs.html#best-practices","title":"Best Practices","text":""},{"location":"dev/code_reference/jobs.html#scheduling","title":"Scheduling","text":"<ol> <li>Regular Execution: Schedule to run at least weekly</li> <li>Off-Peak Hours: Run during low-traffic periods</li> <li>Monitor First Runs: Check initial executions carefully</li> <li>Adjust Frequency: Based on checkpoint creation rate</li> </ol>"},{"location":"dev/code_reference/jobs.html#retention-policy_1","title":"Retention Policy","text":"<ol> <li>Balance History vs Space: Longer retention = more history, more space</li> <li>Consider Use Patterns: How long do users need history?</li> <li>Compliance Requirements: Legal/regulatory retention needs</li> <li>Storage Capacity: Redis memory limitations</li> </ol>"},{"location":"dev/code_reference/jobs.html#monitoring_1","title":"Monitoring","text":"<ol> <li>Set Up Alerts: Alert on job failures</li> <li>Track Metrics: Monitor key count and memory</li> <li>Regular Reviews: Periodically review cleanup effectiveness</li> <li>Log Analysis: Review logs for patterns</li> </ol>"},{"location":"dev/code_reference/jobs.html#disaster-recovery","title":"Disaster Recovery","text":"<ol> <li>Redis Backup: Regular Redis backups include checkpoints</li> <li>Retention Coordination: Align with backup schedule</li> <li>Test Restoration: Verify checkpoint data in backups</li> <li>Document Procedure: Clear recovery process</li> </ol>"},{"location":"dev/code_reference/jobs.html#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"dev/code_reference/jobs.html#custom-retention-period","title":"Custom Retention Period","text":"<p>Modify the retention period by editing the job:</p> <pre><code># ai_ops/jobs/checkpoint_cleanup.py\n\ndef run(self):\n    # Custom retention: 60 days instead of 30\n    result = cleanup_old_checkpoints(retention_days=60)\n    # ... rest of the code\n</code></pre>"},{"location":"dev/code_reference/jobs.html#conditional-cleanup","title":"Conditional Cleanup","text":"<p>Implement conditional cleanup based on memory usage:</p> <pre><code>def run(self):\n    redis_client = get_redis_connection()\n    memory_info = redis_client.info('memory')\n    used_memory_mb = memory_info['used_memory'] / (1024 * 1024)\n\n    if used_memory_mb &gt; 1000:  # Over 1GB\n        # Aggressive cleanup\n        result = cleanup_old_checkpoints(retention_days=7)\n    else:\n        # Normal cleanup\n        result = cleanup_old_checkpoints(retention_days=30)\n\n    return result\n</code></pre>"},{"location":"dev/code_reference/jobs.html#selective-cleanup","title":"Selective Cleanup","text":"<p>Clean up specific thread patterns:</p> <pre><code>def cleanup_user_checkpoints(user_id: str):\n    \"\"\"Clean up checkpoints for a specific user.\"\"\"\n    redis_client = get_redis_connection()\n    pattern = f\"checkpoint:user-{user_id}-*\"\n\n    deleted = 0\n    for key in redis_client.scan_iter(match=pattern):\n        redis_client.delete(key)\n        deleted += 1\n\n    return deleted\n</code></pre>"},{"location":"dev/code_reference/jobs.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Checkpointer Configuration - Redis checkpoint setup (see <code>ai_ops/checkpointer.py</code>)</li> <li>Celery Tasks - Background task definitions (see <code>ai_ops/celery_tasks.py</code>)</li> <li>Agents - How agents use checkpoints</li> <li>External Interactions - Redis configuration</li> </ul>"},{"location":"dev/code_reference/models.html","title":"Models","text":"<p>This page documents the database models provided by the AI Ops App.</p>"},{"location":"dev/code_reference/models.html#overview","title":"Overview","text":"<p>The AI Ops App uses a flexible multi-provider architecture to support various LLM providers and middleware configurations:</p> <ul> <li>LLMProvider: Defines available LLM providers (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace)</li> <li>LLMModel: Stores specific model configurations for a provider</li> <li>MiddlewareType: Defines middleware types that can be applied to models</li> <li>LLMMiddleware: Configures middleware instances for specific models</li> <li>MCPServer: Manages Model Context Protocol server connections</li> </ul>"},{"location":"dev/code_reference/models.html#llmprovider","title":"LLMProvider","text":"<p>The <code>LLMProvider</code> class defines available LLM providers and their configurations. Each provider can have multiple LLM models configured with provider-specific settings.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMProvider","title":"<code>ai_ops.models.LLMProvider</code>","text":"<p>               Bases: <code>PrimaryModel</code></p> <p>Model for storing LLM provider configurations.</p> <p>This model defines available LLM providers (e.g., Ollama, OpenAI, Azure AI, Anthropic, HuggingFace). Each provider can have multiple LLM models configured with provider-specific settings.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMProvider.get_handler","title":"<code>get_handler()</code>","text":"<p>Get the handler for this provider.</p> <p>Returns:</p> Name Type Description <code>BaseLLMProviderHandler</code> <code>BaseLLMProviderHandler</code> <p>The handler instance for this provider type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provider type is not registered.</p>"},{"location":"dev/code_reference/models.html#key-features","title":"Key Features","text":"<ul> <li>Multiple Provider Support: Ollama (default), OpenAI, Azure AI, Anthropic, HuggingFace, and Custom</li> <li>Dynamic Configuration: JSON schema for provider-specific settings</li> <li>Provider Handlers: Each provider has a dedicated handler class for initialization</li> <li>Enable/Disable Support: Toggle provider availability without deletion</li> </ul>"},{"location":"dev/code_reference/models.html#fields","title":"Fields","text":"Field Type Description <code>name</code> CharField (Choice) Provider name (ollama, openai, azure_ai, anthropic, huggingface, custom) <code>description</code> CharField Description of the provider and its capabilities <code>documentation_url</code> URLField URL to provider's documentation <code>config_schema</code> JSONField Provider-specific configuration as JSON <code>is_enabled</code> BooleanField Whether this provider is available for use"},{"location":"dev/code_reference/models.html#provider-choices","title":"Provider Choices","text":"<p>The available provider options are:</p> <ul> <li>ollama: Local open-source LLM runtime (default)</li> <li>openai: ChatGPT, GPT-4, and other OpenAI models</li> <li>azure_ai: Azure OpenAI Service deployments</li> <li>anthropic: Claude models from Anthropic</li> <li>huggingface: Models hosted on HuggingFace Hub</li> <li>custom: Custom provider implementations</li> </ul>"},{"location":"dev/code_reference/models.html#usage-example","title":"Usage Example","text":"<pre><code>from ai_ops.models import LLMProvider, LLMProviderChoice\n\n# Create an Azure AI provider\nazure_provider = LLMProvider.objects.create(\n    name=LLMProviderChoice.AZURE_AI,\n    description=\"Azure OpenAI Service for enterprise deployments\",\n    documentation_url=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/\",\n    config_schema={\n        \"api_version\": \"2024-02-15-preview\",\n        \"base_url\": \"https://your-resource.openai.azure.com/\"\n    },\n    is_enabled=True\n)\n\n# Get the handler for this provider\nhandler = azure_provider.get_handler()\n\n# Create an Ollama provider (local development)\nollama_provider = LLMProvider.objects.create(\n    name=LLMProviderChoice.OLLAMA,\n    description=\"Local Ollama installation for development\",\n    config_schema={\"base_url\": \"http://localhost:11434\"},\n    is_enabled=True\n)\n</code></pre>"},{"location":"dev/code_reference/models.html#configuration-schema-examples","title":"Configuration Schema Examples","text":"<p>Azure AI Provider: <pre><code>{\n    \"api_version\": \"2024-02-15-preview\",\n    \"base_url\": \"https://your-resource.openai.azure.com/\",\n    \"deployment_suffix\": \"\"\n}\n</code></pre></p> <p>OpenAI Provider: <pre><code>{\n    \"organization\": \"org-xxxxx\",\n    \"base_url\": \"https://api.openai.com/v1\"\n}\n</code></pre></p> <p>Ollama Provider: <pre><code>{\n    \"base_url\": \"http://localhost:11434\",\n    \"timeout\": 300\n}\n</code></pre></p>"},{"location":"dev/code_reference/models.html#llmmodel","title":"LLMModel","text":"<p>The <code>LLMModel</code> class stores configurations for Large Language Models from any supported provider. It supports both LAB (local development) and production environments, with flexible configuration options.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMModel","title":"<code>ai_ops.models.LLMModel</code>","text":"<p>               Bases: <code>PrimaryModel</code></p> <p>Model for storing LLM (Large Language Model) configurations.</p> <p>This model supports both LAB (local development) and production environments. In LAB, environment variables are used. In production, values are retrieved from the database.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMModel.get_default_model","title":"<code>get_default_model()</code>  <code>classmethod</code>","text":"<p>Get the default LLM model.</p> <p>Returns the model marked as default, or the first available model if none are marked as default.</p> <p>Returns:</p> Name Type Description <code>LLMModel</code> <code>LLMModel</code> <p>The default model instance.</p> <p>Raises:</p> Type Description <code>DoesNotExist</code> <p>If no models exist in the database.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMModel.get_all_models_summary","title":"<code>get_all_models_summary()</code>  <code>classmethod</code>","text":"<p>Get a summary of all available models.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of dictionaries containing model information.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMModel.get_api_key","title":"<code>get_api_key()</code>","text":"<p>Retrieve the API key from the Secret object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The API key value.</p> <p>Raises:</p> Type Description <code>DoesNotExist</code> <p>If the secret doesn't exist.</p> <code>ValidationError</code> <p>If model_secret_key is not configured.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMModel.get_llm_provider_handler","title":"<code>get_llm_provider_handler()</code>","text":"<p>Get the LLM provider handler for this model.</p> <p>Returns:</p> Name Type Description <code>BaseLLMProviderHandler</code> <code>BaseLLMProviderHandler</code> <p>The handler instance for this model's LLM provider.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMModel.clean","title":"<code>clean()</code>","text":"<p>Validate LLMModel instance.</p>"},{"location":"dev/code_reference/models.html#key-features_1","title":"Key Features","text":"<ul> <li>Multi-Provider Support: Works with any configured LLM provider</li> <li>Multi-Environment Support: Works in LAB, NONPROD, and PROD environments</li> <li>Secret Management: Integrates with Nautobot Secrets for API key storage</li> <li>Default Model Selection: Supports marking one model as the default</li> <li>Temperature Control: Configurable temperature for response variability</li> <li>Middleware Support: Can have multiple middleware configurations applied</li> <li>Cache Control: Configurable cache TTL for MCP client connections</li> <li>Validation: Automatic validation ensures only one default model exists</li> </ul>"},{"location":"dev/code_reference/models.html#fields_1","title":"Fields","text":"Field Type Description <code>llm_provider</code> ForeignKey Reference to the LLM provider (Ollama, OpenAI, Azure AI, etc.) <code>name</code> CharField Model name (e.g., gpt-4o, llama2, claude-3-opus) <code>description</code> CharField Description of the LLM and its capabilities <code>model_secret_key</code> CharField Name of the Secret object containing the API key <code>endpoint</code> URLField LLM endpoint URL <code>api_version</code> CharField API version (e.g., Azure OpenAI API version) <code>is_default</code> BooleanField Whether this is the default model <code>temperature</code> FloatField Temperature setting (0.0 to 2.0) <code>cache_ttl</code> IntegerField Cache TTL for MCP connections (minimum 60 seconds)"},{"location":"dev/code_reference/models.html#usage-example_1","title":"Usage Example","text":"<pre><code>from ai_ops.models import LLMModel, LLMProvider\n\n# Get the default model\ndefault_model = LLMModel.get_default_model()\nprint(f\"Using model: {default_model.name}\")\nprint(f\"Provider: {default_model.llm_provider.name}\")\n\n# Get model configuration\nconfig = default_model.config_dict\napi_key = default_model.get_api_key()\n\n# Get provider handler\nhandler = default_model.get_llm_provider_handler()\n\n# Create a new Azure AI model\nazure_provider = LLMProvider.objects.get(name=\"azure_ai\")\nazure_model = LLMModel.objects.create(\n    name=\"gpt-4o\",\n    llm_provider=azure_provider,\n    description=\"GPT-4 Optimized model for production\",\n    model_secret_key=\"azure_api_key\",\n    endpoint=\"https://your-resource.openai.azure.com/\",\n    api_version=\"2024-02-15-preview\",\n    is_default=True,\n    temperature=0.3,\n    cache_ttl=300\n)\n\n# Create an Ollama model for local development\nollama_provider = LLMProvider.objects.get(name=\"ollama\")\nollama_model = LLMModel.objects.create(\n    name=\"llama2\",\n    llm_provider=ollama_provider,\n    description=\"Llama 2 model for local testing\",\n    endpoint=\"http://localhost:11434\",\n    temperature=0.7,\n    cache_ttl=300\n)\n</code></pre>"},{"location":"dev/code_reference/models.html#middlewaretype","title":"MiddlewareType","text":"<p>The <code>MiddlewareType</code> class defines middleware types that can be applied to LLM models. Middleware can modify, enhance, or monitor interactions with LLM models.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.MiddlewareType","title":"<code>ai_ops.models.MiddlewareType</code>","text":"<p>               Bases: <code>PrimaryModel</code></p> <p>Model for storing middleware type definitions.</p> <p>This model defines available middleware types that can be used with LLM models. Middleware can be either built-in LangChain middleware or custom middleware classes.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.MiddlewareType.clean","title":"<code>clean()</code>","text":"<p>Validate MiddlewareType instance.</p>"},{"location":"dev/code_reference/models.html#key-features_2","title":"Key Features","text":"<ul> <li>Built-in &amp; Custom Support: Supports both LangChain middleware and custom implementations</li> <li>Type Classification: Distinguishes between built-in and custom middleware</li> <li>Name Validation: Automatically validates and formats middleware names</li> <li>Reusable Definitions: One type can be applied to multiple models</li> </ul>"},{"location":"dev/code_reference/models.html#fields_2","title":"Fields","text":"Field Type Description <code>name</code> CharField Middleware class name (must end with 'Middleware', PascalCase) <code>is_custom</code> BooleanField Whether this is custom (True) or built-in LangChain (False) <code>description</code> CharField Description of middleware functionality"},{"location":"dev/code_reference/models.html#name-validation","title":"Name Validation","text":"<p>The middleware name is automatically validated and formatted:</p> <ul> <li>Must be a valid Python class name</li> <li>Automatically appends \"Middleware\" suffix if missing</li> <li>Converts to PascalCase if needed</li> <li>Must contain only alphanumeric characters</li> </ul>"},{"location":"dev/code_reference/models.html#usage-example_2","title":"Usage Example","text":"<pre><code>from ai_ops.models import MiddlewareType\n\n# Create a built-in LangChain middleware type\ncache_middleware = MiddlewareType.objects.create(\n    name=\"CacheMiddleware\",\n    is_custom=False,\n    description=\"Caches LLM responses to reduce API calls and costs\"\n)\n\n# Create a custom middleware type\nlogging_middleware = MiddlewareType.objects.create(\n    name=\"CustomLogging\",  # Will become \"CustomLoggingMiddleware\"\n    is_custom=True,\n    description=\"Custom logging middleware for detailed request/response tracking\"\n)\n\n# The name is automatically formatted\nprint(logging_middleware.name)  # Output: \"CustomLoggingMiddleware\"\n</code></pre>"},{"location":"dev/code_reference/models.html#built-in-middleware-types","title":"Built-in Middleware Types","text":"<p>Common LangChain middleware types include:</p> <ul> <li>CacheMiddleware: Response caching</li> <li>RateLimitMiddleware: Request rate limiting</li> <li>RetryMiddleware: Automatic retry logic</li> <li>LoggingMiddleware: Request/response logging</li> <li>ValidationMiddleware: Input/output validation</li> </ul>"},{"location":"dev/code_reference/models.html#custom-middleware-types","title":"Custom Middleware Types","text":"<p>Custom middleware can implement:</p> <ul> <li>Domain-specific transformations</li> <li>Custom monitoring and metrics</li> <li>Security scanning</li> <li>Content filtering</li> <li>Custom retry logic with backoff</li> </ul>"},{"location":"dev/code_reference/models.html#llmmiddleware","title":"LLMMiddleware","text":"<p>The <code>LLMMiddleware</code> class configures middleware instances for specific LLM models. Middleware executes in priority order to process requests and responses.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMMiddleware","title":"<code>ai_ops.models.LLMMiddleware</code>","text":"<p>               Bases: <code>PrimaryModel</code></p> <p>Model for storing LLM middleware configurations.</p> <p>Middleware execute in priority order (lowest to highest) for each LLM model. Each middleware type can only be configured once per model.</p> <p>Supports both built-in LangChain middleware and custom middleware classes.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.LLMMiddleware.display","title":"<code>display: str</code>  <code>property</code>","text":"<p>User-friendly display name for the middleware instance.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Display name combining model and middleware.</p>"},{"location":"dev/code_reference/models.html#key-features_3","title":"Key Features","text":"<ul> <li>Priority-Based Execution: Lower priority values execute first (1-100)</li> <li>Model-Specific Configuration: Each model can have unique middleware setups</li> <li>Active/Inactive Toggle: Enable or disable middleware without deletion</li> <li>Critical Flag: Mark middleware as critical for initialization</li> <li>Unique Constraint: Each middleware type can only be configured once per model</li> <li>JSON Configuration: Flexible configuration storage</li> <li>TTL Management: Time-to-live for middleware data</li> </ul>"},{"location":"dev/code_reference/models.html#fields_3","title":"Fields","text":"Field Type Description <code>llm_model</code> ForeignKey The LLM model this middleware applies to <code>middleware</code> ForeignKey The middleware type to apply <code>config</code> JSONField JSON configuration for the middleware <code>config_version</code> CharField LangChain version this config is compatible with <code>is_active</code> BooleanField Whether this middleware is currently active <code>is_critical</code> BooleanField If True, agent fails if middleware can't load <code>priority</code> IntegerField Execution priority (1-100, lower executes first) <code>ttl</code> IntegerField Time-to-live for middleware data in seconds (min 60)"},{"location":"dev/code_reference/models.html#execution-order","title":"Execution Order","text":"<p>Middleware executes in priority order:</p> <ol> <li>Priority 1 middleware execute first</li> <li>Priority 10 middleware execute next</li> <li>Priority 100 middleware execute last</li> <li>Ties are broken alphabetically by middleware name</li> </ol>"},{"location":"dev/code_reference/models.html#usage-example_3","title":"Usage Example","text":"<pre><code>from ai_ops.models import LLMModel, MiddlewareType, LLMMiddleware\n\n# Get model and middleware type\nmodel = LLMModel.objects.get(name=\"gpt-4o\")\ncache_type = MiddlewareType.objects.get(name=\"CacheMiddleware\")\nlogging_type = MiddlewareType.objects.get(name=\"CustomLoggingMiddleware\")\n\n# Configure cache middleware (executes first)\ncache_config = LLMMiddleware.objects.create(\n    llm_model=model,\n    middleware=cache_type,\n    config={\n        \"cache_backend\": \"redis\",\n        \"max_entries\": 10000,\n        \"ttl_seconds\": 3600\n    },\n    config_version=\"1.1.0\",\n    is_active=True,\n    is_critical=False,\n    priority=10,\n    ttl=300\n)\n\n# Configure logging middleware (executes second)\nlogging_config = LLMMiddleware.objects.create(\n    llm_model=model,\n    middleware=logging_type,\n    config={\n        \"log_level\": \"INFO\",\n        \"include_tokens\": True,\n        \"log_to_file\": True,\n        \"file_path\": \"/var/log/ai_ops/llm_requests.log\"\n    },\n    is_active=True,\n    is_critical=True,  # Critical - agent won't start without it\n    priority=20,\n    ttl=300\n)\n\n# Query active middleware for a model\nactive_middleware = LLMMiddleware.objects.filter(\n    llm_model=model,\n    is_active=True\n).order_by('priority', 'middleware__name')\n\nfor mw in active_middleware:\n    print(f\"Priority {mw.priority}: {mw.middleware.name}\")\n</code></pre>"},{"location":"dev/code_reference/models.html#configuration-examples","title":"Configuration Examples","text":"<p>Cache Middleware: <pre><code>{\n    \"cache_backend\": \"redis\",\n    \"max_entries\": 10000,\n    \"ttl_seconds\": 3600,\n    \"key_prefix\": \"llm_cache\"\n}\n</code></pre></p> <p>Retry Middleware: <pre><code>{\n    \"max_retries\": 3,\n    \"initial_delay\": 1.0,\n    \"max_delay\": 60.0,\n    \"exponential_base\": 2,\n    \"retry_on\": [\"rate_limit\", \"timeout\", \"connection_error\"]\n}\n</code></pre></p> <p>Rate Limit Middleware: <pre><code>{\n    \"requests_per_minute\": 60,\n    \"requests_per_hour\": 1000,\n    \"burst_size\": 10\n}\n</code></pre></p> <p>Custom Logging Middleware: <pre><code>{\n    \"log_level\": \"INFO\",\n    \"include_tokens\": true,\n    \"include_latency\": true,\n    \"log_to_file\": true,\n    \"file_path\": \"/var/log/ai_ops/requests.log\"\n}\n</code></pre></p>"},{"location":"dev/code_reference/models.html#mcpserver","title":"MCPServer","text":"<p>The <code>MCPServer</code> class stores configurations for Model Context Protocol servers that extend the AI agent's capabilities.</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.MCPServer","title":"<code>ai_ops.models.MCPServer</code>","text":"<p>               Bases: <code>PrimaryModel</code></p> <p>Model for MCP Server configurations.</p> <p>Status can be automatically updated based on health check results: - Active: Server is healthy and responding - Failed: Health check failed - Maintenance: Manually disabled for maintenance</p>"},{"location":"dev/code_reference/models.html#ai_ops.models.MCPServer.clean","title":"<code>clean()</code>","text":"<p>Validate MCPServer instance.</p>"},{"location":"dev/code_reference/models.html#key-features_4","title":"Key Features","text":"<ul> <li>Status Tracking: Uses Nautobot Status field for health monitoring</li> <li>Protocol Support: Supports HTTP and STDIO protocols</li> <li>Health Checks: Automatic health check endpoint monitoring</li> <li>Type Classification: Distinguishes between internal and external servers</li> </ul>"},{"location":"dev/code_reference/models.html#fields_4","title":"Fields","text":"Field Type Description <code>name</code> CharField Unique name for the MCP server <code>status</code> StatusField Server status (Healthy, Failed, Maintenance) <code>protocol</code> CharField Connection type (STDIO or HTTP) <code>url</code> URLField MCP server endpoint URL <code>health_check</code> CharField Health check endpoint path <code>description</code> CharField Optional description <code>mcp_type</code> CharField Server type (internal or external)"},{"location":"dev/code_reference/models.html#status-meanings","title":"Status Meanings","text":"<ul> <li>Healthy: Server is operational and responding to health checks</li> <li>Failed: Health check failed; server excluded from agent operations</li> <li>Maintenance: Manually disabled for maintenance</li> </ul>"},{"location":"dev/code_reference/models.html#usage-example_4","title":"Usage Example","text":"<pre><code>from ai_ops.models import MCPServer\nfrom nautobot.extras.models import Status\n\n# Get healthy status\nhealthy_status = Status.objects.get(name=\"Healthy\")\n\n# Create a new MCP server\nmcp_server = MCPServer.objects.create(\n    name=\"monitoring-mcp\",\n    status=healthy_status,\n    protocol=\"http\",\n    url=\"https://monitoring.internal.com\",\n    health_check=\"/health\",\n    description=\"Monitoring and alerting MCP\",\n    mcp_type=\"internal\"\n)\n\n# Query healthy servers\nhealthy_servers = MCPServer.objects.filter(status__name=\"Healthy\")\nfor server in healthy_servers:\n    print(f\"Server: {server.name} - {server.url}\")\n</code></pre>"},{"location":"dev/code_reference/models.html#model-relationships","title":"Model Relationships","text":"<p>The AI Ops models form a hierarchical structure:</p> <pre><code>LLMProvider (1) \u2500\u2500\u2500\u2192 (N) LLMModel (1) \u2500\u2500\u2500\u2192 (N) LLMMiddleware\n                                \u2193\n                          MiddlewareType (N) \u2190\u2500\u2518\n\nMCPServer (independent)\n\nStatus \u2190\u2500\u2500\u2500\u2500 MCPServer\nSecret \u2190\u2500\u2500\u2500\u2500 LLMModel\n</code></pre>"},{"location":"dev/code_reference/models.html#llmprovider-llmmodel-relationship","title":"LLMProvider \u2192 LLMModel Relationship","text":"<p>Each LLM model belongs to one provider:</p> <pre><code>from ai_ops.models import LLMProvider, LLMModel\n\n# Get all models for a provider\nazure_provider = LLMProvider.objects.get(name=\"azure_ai\")\nazure_models = azure_provider.llm_models.all()\n\nfor model in azure_models:\n    print(f\"Model: {model.name}\")\n\n# Get provider from model\nmodel = LLMModel.objects.get(name=\"gpt-4o\")\nprovider = model.llm_provider\nprint(f\"Provider: {provider.get_name_display()}\")\n</code></pre>"},{"location":"dev/code_reference/models.html#llmmodel-llmmiddleware-relationship","title":"LLMModel \u2192 LLMMiddleware Relationship","text":"<p>Each model can have multiple middleware configurations:</p> <pre><code>from ai_ops.models import LLMModel, LLMMiddleware\n\n# Get all middleware for a model\nmodel = LLMModel.objects.get(name=\"gpt-4o\")\nmiddlewares = model.middlewares.filter(is_active=True).order_by('priority')\n\nfor mw in middlewares:\n    print(f\"Priority {mw.priority}: {mw.middleware.name}\")\n\n# Query models using specific middleware\ncache_models = LLMModel.objects.filter(\n    middlewares__middleware__name=\"CacheMiddleware\",\n    middlewares__is_active=True\n).distinct()\n</code></pre>"},{"location":"dev/code_reference/models.html#middlewaretype-llmmiddleware-relationship","title":"MiddlewareType \u2192 LLMMiddleware Relationship","text":"<p>Each middleware type can be configured for multiple models:</p> <pre><code>from ai_ops.models import MiddlewareType\n\n# Get all models using a middleware type\ncache_type = MiddlewareType.objects.get(name=\"CacheMiddleware\")\nconfigurations = cache_type.middleware_instances.filter(is_active=True)\n\nfor config in configurations:\n    print(f\"Model: {config.llm_model.name}, Config: {config.config}\")\n</code></pre>"},{"location":"dev/code_reference/models.html#llmmodel-and-secrets","title":"LLMModel and Secrets","text":"<p><code>LLMModel</code> integrates with Nautobot's Secrets management:</p> <pre><code>from ai_ops.models import LLMModel, LLMProvider\nfrom nautobot.extras.models import Secret\n\n# Create a secret\nsecret = Secret.objects.create(\n    name=\"azure_gpt4_api_key\",\n    provider=\"environment-variable\",\n)\n\n# Reference the secret in LLMModel\nazure_provider = LLMProvider.objects.get(name=\"azure_ai\")\nmodel = LLMModel.objects.create(\n    name=\"gpt-4o\",\n    llm_provider=azure_provider,\n    model_secret_key=\"azure_gpt4_api_key\",\n    endpoint=\"https://your-resource.openai.azure.com/\",\n    api_version=\"2024-02-15-preview\",\n    is_default=True,\n    temperature=0.3\n)\n\n# Retrieve API key from secret\napi_key = model.get_api_key()\n</code></pre>"},{"location":"dev/code_reference/models.html#mcpserver-and-status","title":"MCPServer and Status","text":"<p><code>MCPServer</code> uses Nautobot's Status model for health tracking:</p> <pre><code>from ai_ops.models import MCPServer\nfrom nautobot.extras.models import Status\n\n# Update server status\nserver = MCPServer.objects.get(name=\"my-mcp\")\nfailed_status = Status.objects.get(name=\"Failed\")\nserver.status = failed_status\nserver.save()\n\n# Query servers by status\nhealthy_servers = MCPServer.objects.filter(status__name=\"Active\")\nfailed_servers = MCPServer.objects.filter(status__name=\"Failed\")\n</code></pre>"},{"location":"dev/code_reference/models.html#extras-features","title":"Extras Features","text":"<p>All models support Nautobot's extras features:</p> <ul> <li>Custom Links: Create custom links for models</li> <li>Custom Validators: Add validation rules</li> <li>Export Templates: Define export templates</li> <li>GraphQL: Full GraphQL API support</li> <li>Webhooks: Receive notifications on changes</li> </ul>"},{"location":"dev/code_reference/models.html#llmprovider-extras","title":"LLMProvider Extras","text":"<pre><code>@extras_features(\n    \"custom_links\",\n    \"custom_validators\",\n    \"export_templates\",\n    \"graphql\",\n    \"webhooks\"\n)\nclass LLMProvider(PrimaryModel):\n    ...\n</code></pre>"},{"location":"dev/code_reference/models.html#llmmodel-extras","title":"LLMModel Extras","text":"<pre><code>@extras_features(\n    \"custom_links\",\n    \"custom_validators\", \n    \"export_templates\",\n    \"graphql\",\n    \"webhooks\"\n)\nclass LLMModel(PrimaryModel):\n    ...\n</code></pre>"},{"location":"dev/code_reference/models.html#middlewaretype-extras","title":"MiddlewareType Extras","text":"<pre><code>@extras_features(\n    \"custom_links\",\n    \"custom_validators\",\n    \"export_templates\",\n    \"graphql\",\n    \"webhooks\"\n)\nclass MiddlewareType(PrimaryModel):\n    ...\n</code></pre>"},{"location":"dev/code_reference/models.html#llmmiddleware-extras","title":"LLMMiddleware Extras","text":"<pre><code>@extras_features(\n    \"custom_links\",\n    \"custom_validators\",\n    \"export_templates\",\n    \"graphql\",\n    \"webhooks\"\n)\nclass LLMMiddleware(PrimaryModel):\n    ...\n</code></pre>"},{"location":"dev/code_reference/models.html#mcpserver-extras","title":"MCPServer Extras","text":"<pre><code>@extras_features(\n    \"custom_links\",\n    \"custom_validators\",\n    \"export_templates\", \n    \"graphql\",\n    \"statuses\",\n    \"webhooks\"\n)\nclass MCPServer(PrimaryModel):\n    ...\n</code></pre>"},{"location":"dev/code_reference/models.html#database-migrations","title":"Database Migrations","text":"<p>The models are defined in migration <code>0001_initial.py</code>. To create the database tables:</p> <pre><code>nautobot-server migrate ai_ops\n</code></pre>"},{"location":"dev/code_reference/models.html#model-validation","title":"Model Validation","text":"<p>All models include validation logic to ensure data integrity:</p>"},{"location":"dev/code_reference/models.html#llmprovider-validation","title":"LLMProvider Validation","text":"<ul> <li>Provider name must be one of the supported choices</li> <li>Config schema must be valid JSON</li> <li>Enabled/disabled state controls provider availability</li> </ul>"},{"location":"dev/code_reference/models.html#llmmodel-validation","title":"LLMModel Validation","text":"<ul> <li>Only one model can be marked as <code>is_default=True</code></li> <li>Prevents multiple default models</li> <li>Cache TTL must be at least 60 seconds</li> <li>Must reference a valid LLM provider</li> </ul>"},{"location":"dev/code_reference/models.html#middlewaretype-validation","title":"MiddlewareType Validation","text":"<ul> <li>Middleware name must be valid Python identifier</li> <li>Automatically formats name to PascalCase</li> <li>Automatically appends \"Middleware\" suffix if missing</li> <li>Name must contain only alphanumeric characters</li> </ul>"},{"location":"dev/code_reference/models.html#llmmiddleware-validation","title":"LLMMiddleware Validation","text":"<ul> <li>Each middleware type can only be configured once per model (unique_together constraint)</li> <li>Priority must be between 1 and 100</li> <li>TTL must be at least 60 seconds</li> <li>Critical flag determines initialization behavior</li> </ul>"},{"location":"dev/code_reference/models.html#mcpserver-validation","title":"MCPServer Validation","text":"<ul> <li>URL field is required</li> <li>Endpoint paths automatically prefixed with <code>/</code> if needed</li> <li>Ensures proper server configuration</li> </ul>"},{"location":"dev/code_reference/models.html#best-practices","title":"Best Practices","text":""},{"location":"dev/code_reference/models.html#llmprovider-best-practices","title":"LLMProvider Best Practices","text":"<ol> <li>Use descriptive names: Make provider purpose clear</li> <li>Document configuration: Provide detailed config_schema documentation</li> <li>Disable rather than delete: Use is_enabled flag to temporarily disable providers</li> <li>Keep documentation_url current: Link to provider's official docs</li> </ol>"},{"location":"dev/code_reference/models.html#llmmodel-best-practices","title":"LLMModel Best Practices","text":"<ol> <li>Always have a default model: Mark one model as default for fallback</li> <li>Use Secrets for API keys: Never store keys directly in the database</li> <li>Temperature settings: Use 0.0-0.3 for deterministic, 0.7-1.0 for creative</li> <li>Document model purposes: Use descriptive names and descriptions</li> <li>Set appropriate cache TTL: Balance performance vs freshness</li> <li>Group by provider: Use consistent naming within provider groups</li> </ol>"},{"location":"dev/code_reference/models.html#middlewaretype-best-practices","title":"MiddlewareType Best Practices","text":"<ol> <li>Clear naming: Use descriptive names that explain functionality</li> <li>Distinguish custom vs built-in: Set is_custom flag correctly</li> <li>Document thoroughly: Explain what the middleware does</li> <li>Follow naming convention: Always end with \"Middleware\"</li> </ol>"},{"location":"dev/code_reference/models.html#llmmiddleware-best-practices","title":"LLMMiddleware Best Practices","text":"<ol> <li>Set appropriate priorities: Lower numbers for foundational middleware (caching, auth)</li> <li>Use critical flag wisely: Only for essential middleware</li> <li>Document configuration: Comment complex config JSON</li> <li>Test middleware chains: Verify middleware execute in correct order</li> <li>Monitor performance: Track middleware overhead</li> <li>Version compatibility: Keep config_version current with LangChain</li> <li>Set reasonable TTLs: Balance cache performance with data freshness</li> </ol>"},{"location":"dev/code_reference/models.html#mcpserver-best-practices","title":"MCPServer Best Practices","text":"<ol> <li>Monitor health status: Regularly check server health</li> <li>Use descriptive names: Clear naming helps management</li> <li>Set appropriate status: Use Maintenance status during upgrades</li> <li>Test health checks: Verify health check endpoints work correctly</li> <li>Document server purpose: Clear descriptions aid troubleshooting</li> <li>Use internal for trusted: Mark organization-owned servers as internal</li> </ol>"},{"location":"dev/code_reference/models.html#common-workflows","title":"Common Workflows","text":""},{"location":"dev/code_reference/models.html#setting-up-a-new-provider","title":"Setting Up a New Provider","text":"<pre><code>from ai_ops.models import LLMProvider, LLMModel, LLMProviderChoice\nfrom nautobot.extras.models import Secret\n\n# 1. Create provider\nanthropic_provider = LLMProvider.objects.create(\n    name=LLMProviderChoice.ANTHROPIC,\n    description=\"Anthropic Claude models\",\n    documentation_url=\"https://docs.anthropic.com/\",\n    config_schema={\"api_base\": \"https://api.anthropic.com\"},\n    is_enabled=True\n)\n\n# 2. Create secret for API key\napi_secret = Secret.objects.create(\n    name=\"anthropic_api_key\",\n    provider=\"environment-variable\",\n)\n\n# 3. Create model\nclaude_model = LLMModel.objects.create(\n    name=\"claude-3-opus\",\n    llm_provider=anthropic_provider,\n    description=\"Claude 3 Opus for complex reasoning\",\n    model_secret_key=\"anthropic_api_key\",\n    temperature=0.7,\n    cache_ttl=300\n)\n</code></pre>"},{"location":"dev/code_reference/models.html#configuring-middleware-chain","title":"Configuring Middleware Chain","text":"<pre><code>from ai_ops.models import LLMModel, MiddlewareType, LLMMiddleware\n\nmodel = LLMModel.objects.get(name=\"gpt-4o\")\n\n# Create middleware types\ncache_type = MiddlewareType.objects.get_or_create(\n    name=\"CacheMiddleware\",\n    defaults={\"is_custom\": False, \"description\": \"Response caching\"}\n)[0]\n\nretry_type = MiddlewareType.objects.get_or_create(\n    name=\"RetryMiddleware\",\n    defaults={\"is_custom\": False, \"description\": \"Automatic retry with backoff\"}\n)[0]\n\nlogging_type = MiddlewareType.objects.get_or_create(\n    name=\"LoggingMiddleware\",\n    defaults={\"is_custom\": False, \"description\": \"Request/response logging\"}\n)[0]\n\n# Configure middleware chain (execution order: logging -&gt; cache -&gt; retry)\nLLMMiddleware.objects.create(\n    llm_model=model,\n    middleware=logging_type,\n    priority=10,\n    config={\"log_level\": \"INFO\"},\n    is_active=True,\n    is_critical=False\n)\n\nLLMMiddleware.objects.create(\n    llm_model=model,\n    middleware=cache_type,\n    priority=20,\n    config={\"ttl_seconds\": 3600, \"max_entries\": 10000},\n    is_active=True,\n    is_critical=False\n)\n\nLLMMiddleware.objects.create(\n    llm_model=model,\n    middleware=retry_type,\n    priority=30,\n    config={\"max_retries\": 3, \"initial_delay\": 1.0},\n    is_active=True,\n    is_critical=True\n)\n</code></pre>"},{"location":"dev/code_reference/models.html#managing-mcp-servers","title":"Managing MCP Servers","text":"<pre><code>from ai_ops.models import MCPServer\nfrom ai_ops.helpers.get_info import get_default_status\n\n# Create internal MCP server\ninternal_server = MCPServer.objects.create(\n    name=\"nautobot-tools\",\n    status=get_default_status(),\n    protocol=\"http\",\n    url=\"http://mcp-internal:8000\",\n    mcp_endpoint=\"/mcp\",\n    health_check=\"/health\",\n    description=\"Internal Nautobot integration tools\",\n    mcp_type=\"internal\"\n)\n\n# Create external MCP server\nexternal_server = MCPServer.objects.create(\n    name=\"network-monitoring\",\n    status=get_default_status(),\n    protocol=\"http\",\n    url=\"https://monitoring.example.com\",\n    mcp_endpoint=\"/mcp/v1\",\n    health_check=\"/api/health\",\n    description=\"External network monitoring integration\",\n    mcp_type=\"external\"\n)\n</code></pre>"},{"location":"dev/code_reference/models.html#related-documentation","title":"Related Documentation","text":"<ul> <li>LLM Providers - Custom provider development</li> <li>Middleware Development - Creating custom middleware</li> <li>Helpers - Helper functions for working with models</li> <li>API - REST API documentation</li> <li>Agents - AI agent implementations</li> <li>Usage Examples - Practical examples (see <code>ai_ops/helpers/USAGE_EXAMPLES.md</code>)</li> </ul>"},{"location":"dev/code_reference/package.html","title":"Package","text":""},{"location":"dev/code_reference/package.html#ai_ops","title":"<code>ai_ops</code>","text":"<p>App declaration for ai_ops.</p>"},{"location":"dev/code_reference/package.html#ai_ops.AiOpsConfig","title":"<code>AiOpsConfig</code>","text":"<p>               Bases: <code>NautobotAppConfig</code></p> <p>App configuration for the ai_ops app.</p> Source code in <code>ai_ops/__init__.py</code> <pre><code>class AiOpsConfig(NautobotAppConfig):\n    \"\"\"App configuration for the ai_ops app.\"\"\"\n\n    name = \"ai_ops\"\n    verbose_name = \"AI Ops\"\n    version = __version__\n    author = \"Kevin Campos\"\n    description = \"AI Ops integration for Nautobot.\"\n    base_url = \"ai-ops\"\n    required_settings = []\n    default_settings = {}\n    docs_view_name = \"plugins:ai_ops:docs\"\n    searchable_models = [\"llmmodel\", \"mcpserver\"]\n\n    def ready(self):\n        \"\"\"Connect signal handlers when the app is ready.\"\"\"\n        import logging\n\n        from .signals import (\n            assign_mcp_server_statuses,\n            create_default_llm_providers,\n            create_default_middleware_types,\n            setup_checkpoint_cleanup_schedule,\n            setup_mcp_health_check_schedule,\n            setup_middleware_cache_jobs,\n        )\n\n        logger = logging.getLogger(__name__)\n\n        nautobot_database_ready.connect(assign_mcp_server_statuses, sender=self)\n        nautobot_database_ready.connect(create_default_llm_providers, sender=self)\n        nautobot_database_ready.connect(create_default_middleware_types, sender=self)\n        nautobot_database_ready.connect(setup_checkpoint_cleanup_schedule, sender=self)\n        nautobot_database_ready.connect(setup_middleware_cache_jobs, sender=self)\n        nautobot_database_ready.connect(setup_mcp_health_check_schedule, sender=self)\n\n        # Note: Periodic tasks are handled via Nautobot Jobs (ai_agents.jobs).\n        # These jobs can be scheduled through the Nautobot UI for automatic execution.\n\n        # Warm caches on startup (if event loop is available)\n        # During unit tests, there may not be a running event loop\n        try:\n            import asyncio\n\n            from ai_ops.agents.multi_mcp_agent import warm_mcp_cache\n            from ai_ops.helpers.get_middleware import warm_middleware_cache\n\n            # Try to get the running event loop\n            try:\n                loop = asyncio.get_running_loop()\n                # If we have a running loop, schedule the tasks\n                loop.create_task(warm_mcp_cache())\n                loop.create_task(warm_middleware_cache())\n                logger.info(\"Scheduled MCP and middleware cache warming\")\n            except RuntimeError:\n                # No running event loop (e.g., during tests or startup)\n                # This is expected and not an error\n                logger.debug(\"No running event loop available for cache warming\")\n        except Exception as e:\n            logger.warning(f\"Failed to warm caches on startup: {e}\")\n\n        super().ready()\n</code></pre>"},{"location":"dev/code_reference/package.html#ai_ops.AiOpsConfig.ready","title":"<code>ready()</code>","text":"<p>Connect signal handlers when the app is ready.</p> Source code in <code>ai_ops/__init__.py</code> <pre><code>def ready(self):\n    \"\"\"Connect signal handlers when the app is ready.\"\"\"\n    import logging\n\n    from .signals import (\n        assign_mcp_server_statuses,\n        create_default_llm_providers,\n        create_default_middleware_types,\n        setup_checkpoint_cleanup_schedule,\n        setup_mcp_health_check_schedule,\n        setup_middleware_cache_jobs,\n    )\n\n    logger = logging.getLogger(__name__)\n\n    nautobot_database_ready.connect(assign_mcp_server_statuses, sender=self)\n    nautobot_database_ready.connect(create_default_llm_providers, sender=self)\n    nautobot_database_ready.connect(create_default_middleware_types, sender=self)\n    nautobot_database_ready.connect(setup_checkpoint_cleanup_schedule, sender=self)\n    nautobot_database_ready.connect(setup_middleware_cache_jobs, sender=self)\n    nautobot_database_ready.connect(setup_mcp_health_check_schedule, sender=self)\n\n    # Note: Periodic tasks are handled via Nautobot Jobs (ai_agents.jobs).\n    # These jobs can be scheduled through the Nautobot UI for automatic execution.\n\n    # Warm caches on startup (if event loop is available)\n    # During unit tests, there may not be a running event loop\n    try:\n        import asyncio\n\n        from ai_ops.agents.multi_mcp_agent import warm_mcp_cache\n        from ai_ops.helpers.get_middleware import warm_middleware_cache\n\n        # Try to get the running event loop\n        try:\n            loop = asyncio.get_running_loop()\n            # If we have a running loop, schedule the tasks\n            loop.create_task(warm_mcp_cache())\n            loop.create_task(warm_middleware_cache())\n            logger.info(\"Scheduled MCP and middleware cache warming\")\n        except RuntimeError:\n            # No running event loop (e.g., during tests or startup)\n            # This is expected and not an error\n            logger.debug(\"No running event loop available for cache warming\")\n    except Exception as e:\n        logger.warning(f\"Failed to warm caches on startup: {e}\")\n\n    super().ready()\n</code></pre>"},{"location":"user/app_getting_started.html","title":"Getting Started with the App","text":"<p>This document provides a step-by-step tutorial on how to get the AI Ops App configured and how to use it.</p>"},{"location":"user/app_getting_started.html#install-the-app","title":"Install the App","text":"<p>To install the App, please follow the instructions detailed in the Installation Guide.</p>"},{"location":"user/app_getting_started.html#first-steps-with-the-app","title":"First Steps with the App","text":"<p>After installing the app, follow these steps to get started:</p>"},{"location":"user/app_getting_started.html#step-1-configure-an-llm-model","title":"Step 1: Configure an LLM Model","text":"<p>Before you can use the AI Chat Assistant, you need to configure at least one LLM model.</p> <ol> <li>Navigate to AI Platform &gt; Configuration &gt; LLM Models in the Nautobot menu</li> <li>Click the + Add button to create a new model</li> <li>Fill in the required fields:</li> <li>Name: Azure deployment name (e.g., <code>gpt-4o</code>, <code>gpt-4-turbo</code>)</li> <li>Description: A description of the model's purpose and capabilities</li> <li>Model Secret Key: Name of the Nautobot Secret containing your Azure OpenAI API key</li> <li>Azure Endpoint: Your Azure OpenAI endpoint URL (e.g., <code>https://your-resource.openai.azure.com/</code>)</li> <li>API Version: API version (default: <code>2024-02-15-preview</code>)</li> <li>Is Default: Check this box to make this the default model</li> <li> <p>Temperature: Set the model temperature (0.0 for deterministic, higher for creative)</p> </li> <li> <p>Click Create to save the model</p> </li> </ol> <p>Tip</p> <p>For your first model, mark it as the default model by checking the \"Is Default\" checkbox. This ensures the chat assistant knows which model to use.</p>"},{"location":"user/app_getting_started.html#step-2-create-secrets-for-api-keys-production","title":"Step 2: Create Secrets for API Keys (Production)","text":"<p>For production environments, you should store API keys securely using Nautobot Secrets:</p> <ol> <li>Navigate to Secrets &gt; Secrets in Nautobot</li> <li>Create a new Secret with your Azure OpenAI API key</li> <li>Name the secret (e.g., <code>azure_gpt4o_api_key</code>)</li> <li>Configure the secret provider and value</li> <li>Use this secret name in your LLM Model configuration</li> </ol> <p>Note</p> <p>In LAB/development environments, the app can use environment variables for API keys instead of Secrets.</p>"},{"location":"user/app_getting_started.html#step-3-configure-mcp-servers-optional","title":"Step 3: Configure MCP Servers (Optional)","text":"<p>MCP (Model Context Protocol) servers extend the capabilities of your AI agent by providing additional tools and context:</p> <ol> <li>Navigate to AI Platform &gt; Configuration &gt; MCP Servers</li> <li>Click + Add to create a new server</li> <li>Fill in the fields:</li> <li>Name: Unique identifier for the server</li> <li>Status: Select \"Healthy\" to enable the server</li> <li>Protocol: Choose \"HTTP\" or \"STDIO\"</li> <li>URL: Server endpoint URL</li> <li>Health Check: Health check endpoint (default: <code>/health</code>)</li> <li>Description: Purpose of this MCP server</li> <li> <p>MCP Type: \"Internal\" for internal servers, \"External\" for third-party</p> </li> <li> <p>Click Create to save the server</p> </li> </ol> <p>Info</p> <p>MCP servers are optional. The AI Chat Assistant will work without them, but MCP servers can provide additional capabilities like code execution, file access, or integration with external systems.</p>"},{"location":"user/app_getting_started.html#step-4-use-the-ai-chat-assistant","title":"Step 4: Use the AI Chat Assistant","text":"<p>Now you're ready to use the AI Chat Assistant:</p> <ol> <li>Navigate to AI Platform &gt; Chat &amp; Assistance &gt; AI Chat Assistant</li> <li>Type your question or request in the chat input box</li> <li>Press Enter or click the Send button</li> <li>The AI agent will process your message and respond</li> </ol> <p>The chat interface maintains conversation history, allowing for contextual multi-turn conversations.</p>"},{"location":"user/app_getting_started.html#step-5-monitor-and-maintain","title":"Step 5: Monitor and Maintain","text":""},{"location":"user/app_getting_started.html#check-mcp-server-health","title":"Check MCP Server Health","text":"<p>MCP server health is automatically monitored. You can view the status:</p> <ol> <li>Go to AI Platform &gt; Configuration &gt; MCP Servers</li> <li>Check the Status column for each server</li> <li>Servers with \"Healthy\" status are actively used by the agent</li> <li>Failed servers will be automatically excluded from agent operations</li> </ol>"},{"location":"user/app_getting_started.html#schedule-checkpoint-cleanup","title":"Schedule Checkpoint Cleanup","text":"<p>To prevent Redis from accumulating old conversation data:</p> <ol> <li>Navigate to Jobs &gt; Jobs</li> <li>Find the job AI Agents &gt; Cleanup Old Checkpoints</li> <li>Click Run Job Now to run it manually, or</li> <li>Click Schedule Job to set up automatic recurring execution</li> </ol> <p>Tip</p> <p>Schedule the cleanup job to run daily or weekly depending on your usage patterns and storage constraints.</p>"},{"location":"user/app_getting_started.html#what-are-the-next-steps","title":"What are the next steps?","text":"<p>After completing the initial setup, you can:</p> <ul> <li>Explore Use Cases: Check out the Use Cases section for examples of what you can do with the AI Chat Assistant</li> <li>Configure Multiple Models: Set up different models for different use cases (e.g., fast model for quick responses, larger model for complex analysis)</li> <li>Integrate MCP Servers: Add MCP servers to extend agent capabilities with custom tools and integrations</li> <li>Review API Documentation: Learn about the REST API in External Interactions</li> <li>Customize Prompts: Advanced users can modify system prompts in the code (see Developer Guide)</li> </ul>"},{"location":"user/app_getting_started.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"user/app_getting_started.html#common-issues","title":"Common Issues","text":"<p>Chat not responding?</p> <ul> <li>Verify that at least one LLM Model exists and is marked as default</li> <li>Check that the API key Secret is configured correctly</li> <li>Review Nautobot logs for error messages</li> </ul> <p>MCP Server showing as Failed?</p> <ul> <li>Verify the server URL is accessible from the Nautobot instance</li> <li>Check the health check endpoint returns a successful response</li> <li>Review the MCP server logs for connection issues</li> </ul> <p>Conversation history not persisting?</p> <ul> <li>Ensure Redis is properly configured in <code>nautobot_config.py</code></li> <li>Verify the Redis connection using the checkpointer configuration</li> <li>Check that LANGGRAPH_REDIS_DB environment variable is set</li> </ul> <p>For more help, check the FAQ or contact your administrator.</p>"},{"location":"user/app_overview.html","title":"App Overview","text":"<p>This document provides an overview of the AI Ops App including critical information and important considerations when applying it to your Nautobot environment.</p> <p>Note</p> <p>Throughout this documentation, the terms \"app\" and \"plugin\" will be used interchangeably.</p>"},{"location":"user/app_overview.html#description","title":"Description","text":"<p>The AI Ops App is a comprehensive AI-powered operations assistant for Nautobot that integrates Large Language Models (LLMs) from multiple providers with the Model Context Protocol (MCP) to provide intelligent automation and assistance capabilities. The app supports a flexible multi-provider architecture (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, and custom providers) and includes a powerful middleware system for request/response processing. It leverages LangChain and LangGraph frameworks to create conversational AI agents that can interact with various MCP servers and perform complex operational tasks.</p>"},{"location":"user/app_overview.html#key-features","title":"Key Features","text":"<ul> <li>AI Chat Assistant: Interactive chat interface powered by configurable LLM models for conversational interactions</li> <li>Multi-Provider Support: Use Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, or custom providers</li> <li>LLM Provider Management: Configure and manage multiple LLM providers with provider-specific settings</li> <li>LLM Model Management: Configure and manage multiple models from different providers with varying capabilities and settings</li> <li>Middleware System: Apply middleware chains to models for caching, logging, validation, retry logic, and more</li> <li>MCP Server Integration: Connect to multiple Model Context Protocol servers for extended functionality</li> <li>Conversation History: Persistent conversation tracking using Redis-based checkpointing</li> <li>Health Monitoring: Automatic health checks for MCP servers with status management</li> <li>Background Jobs: Automated maintenance tasks for checkpoint cleanup and MCP health monitoring</li> <li>Multi-Agent Architecture: Support for both single and multi-MCP agent configurations</li> <li>Flexible Configuration: Environment-based configuration supporting LAB, NONPROD, and PROD environments</li> </ul>"},{"location":"user/app_overview.html#audience-user-personas-who-should-use-this-app","title":"Audience (User Personas) - Who should use this App?","text":"<p>This app is designed for:</p> <ul> <li>Network Engineers: Who need AI assistance for network operations, troubleshooting, and automation</li> <li>DevOps Engineers: Looking to integrate AI capabilities into their infrastructure management workflows</li> <li>Site Reliability Engineers (SREs): Requiring intelligent assistance for monitoring and incident response</li> <li>Nautobot Administrators: Who want to extend Nautobot with AI-powered features</li> <li>Infrastructure Teams: Seeking to leverage AI for operational insights and automation</li> </ul>"},{"location":"user/app_overview.html#authors-and-maintainers","title":"Authors and Maintainers","text":"<ul> <li>Primary Author: Kevin Campos</li> <li>Organization: Infrastructure Automation Team</li> </ul>"},{"location":"user/app_overview.html#nautobot-features-used","title":"Nautobot Features Used","text":"<p>The AI Ops App leverages the following Nautobot features and capabilities:</p>"},{"location":"user/app_overview.html#models","title":"Models","text":"<p>The app introduces five primary models:</p> <ul> <li>LLMProvider: Manages LLM provider configurations (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, Custom)</li> <li>LLMModel: Manages LLM model configurations including API keys, endpoints, and parameters for any supported provider</li> <li>MiddlewareType: Defines middleware types (built-in LangChain or custom) that can be applied to models</li> <li>LLMMiddleware: Configures middleware instances for specific models with priority-based execution</li> <li>MCPServer: Manages Model Context Protocol server configurations for extended agent capabilities</li> </ul>"},{"location":"user/app_overview.html#extras","title":"Extras","text":"<ul> <li>Custom Links: Available on all model objects</li> <li>Custom Validators: Validation rules for all model configurations</li> <li>Export Templates: Data export capabilities for all models</li> <li>GraphQL: Full GraphQL API support for all models</li> <li>Statuses: Status tracking for MCP servers (Active, Failed, Maintenance)</li> <li>Webhooks: Event notifications for all model and server changes</li> <li>Secrets Management: Integration with Nautobot Secrets for API key storage</li> </ul>"},{"location":"user/app_overview.html#jobs","title":"Jobs","text":"<ul> <li>Cleanup Old Checkpoints: Scheduled job to maintain conversation history by removing old checkpoints from Redis</li> <li>MCP Server Health Check: Automated health monitoring for HTTP MCP servers with retry logic and cache invalidation</li> <li>Middleware Cache Invalidation: Automatic cache clearing when middleware configurations change</li> <li>Default Model Cache Warming: Pre-loads middleware for newly set default models</li> </ul>"},{"location":"user/app_overview.html#user-interface","title":"User Interface","text":"<ul> <li>Navigation Menu: \"AI Platform\" tab with sections for:<ul> <li>Chat &amp; Assistance: AI Chat Assistant interface</li> <li>Configuration: LLM Providers, LLM Models, Middleware Types, LLM Middleware, and MCP Servers management</li> </ul> </li> <li>List Views: Comprehensive list views with filtering and sorting for all models</li> <li>Detail Views: Rich detail pages using Nautobot's UI Component Framework</li> <li>Forms: Create, update, and bulk edit forms for all models</li> </ul>"},{"location":"user/app_overview.html#api-endpoints","title":"API Endpoints","text":"<ul> <li>REST API: Full REST API for all models via NautobotModelViewSet</li> <li><code>/api/plugins/ai-ops/llm-providers/</code></li> <li><code>/api/plugins/ai-ops/llm-models/</code></li> <li><code>/api/plugins/ai-ops/middleware-types/</code></li> <li><code>/api/plugins/ai-ops/llm-middleware/</code></li> <li><code>/api/plugins/ai-ops/mcp-servers/</code></li> <li>Chat API: POST endpoint for processing chat messages through the AI agent</li> <li>Filtering: Advanced filtering capabilities using Django Filter</li> <li>Serialization: Custom serializers for LangGraph-specific data types</li> </ul>"},{"location":"user/app_overview.html#integration-points","title":"Integration Points","text":"<ul> <li>Django ORM: Full database integration with PostgreSQL/MySQL</li> <li>Redis: Conversation checkpointing, middleware caching, and MCP client caching</li> <li>Celery: Asynchronous task processing for background jobs</li> <li>Secrets: Secure API key storage using Nautobot Secrets</li> <li>Multiple LLM Providers: Extensible provider system with handler classes</li> </ul>"},{"location":"user/app_use_cases.html","title":"Using the App","text":"<p>This document describes common use-cases and scenarios for the AI Ops App.</p>"},{"location":"user/app_use_cases.html#general-usage","title":"General Usage","text":"<p>The AI Ops App provides an AI-powered chat interface that can assist with various operational tasks. The chat assistant uses configurable LLM models from multiple providers (Ollama, OpenAI, Azure AI, Anthropic, HuggingFace, or custom) and can be extended with MCP servers to provide additional capabilities.</p>"},{"location":"user/app_use_cases.html#basic-chat-interaction","title":"Basic Chat Interaction","text":"<ol> <li>Open the AI Chat Assistant from the navigation menu</li> <li>Type a natural language question or request</li> <li>The AI agent processes your message through configured middleware and LLM</li> <li>The agent provides a response, with conversation context maintained</li> <li>Continue the conversation - the agent maintains context from previous messages</li> <li>Start a new conversation by refreshing the page or clearing your session</li> </ol>"},{"location":"user/app_use_cases.html#use-cases-and-common-workflows","title":"Use-cases and common workflows","text":""},{"location":"user/app_use_cases.html#use-case-1-information-retrieval","title":"Use Case 1: Information Retrieval","text":"<p>Scenario: You need to understand how a particular feature works in Nautobot or your infrastructure.</p> <p>Workflow: 1. Open the AI Chat Assistant 2. Ask questions like:    - \"How do I configure a new device in Nautobot?\"    - \"What are the best practices for IP address management?\"    - \"Explain the difference between sites and locations in Nautobot\" 3. The AI provides detailed explanations based on its training</p> <p>Example Interaction: <pre><code>User: What is the purpose of the LLMModel in this plugin?\nAI: The LLMModel is a database model that stores configurations for \n    Large Language Models from any supported provider (Ollama, OpenAI,\n    Azure AI, Anthropic, HuggingFace, or custom). It includes fields \n    for the model name, provider relationship, API endpoint, API keys \n    (via Secrets), temperature settings, cache TTL, and allows you to \n    designate a default model. The model can also have middleware \n    configurations applied for caching, logging, and other processing...\n</code></pre></p>"},{"location":"user/app_use_cases.html#use-case-2-troubleshooting-assistance","title":"Use Case 2: Troubleshooting Assistance","text":"<p>Scenario: You encounter an issue and need guidance on debugging or resolving it.</p> <p>Workflow: 1. Describe the problem to the AI Chat Assistant 2. Provide error messages or symptoms 3. Follow the AI's step-by-step troubleshooting suggestions 4. Ask follow-up questions for clarification</p>"},{"location":"user/app_use_cases.html#use-case-3-configuration-guidance","title":"Use Case 3: Configuration Guidance","text":"<p>Scenario: You need help configuring the AI Ops App or understanding configuration options.</p> <p>Workflow: 1. Ask about configuration parameters 2. Request examples of proper configuration 3. Get recommendations for your specific use case</p>"},{"location":"user/app_use_cases.html#use-case-4-mcp-server-integration","title":"Use Case 4: MCP Server Integration","text":"<p>Scenario: You want to extend the AI agent's capabilities with custom tools via MCP servers.</p> <p>Workflow: 1. Configure MCP servers in the app 2. The agent automatically discovers available tools from healthy MCP servers 3. Ask the agent to perform tasks that require those tools 4. The agent uses MCP tools transparently to accomplish the task</p>"},{"location":"user/app_use_cases.html#use-case-5-multi-turn-conversations","title":"Use Case 5: Multi-Turn Conversations","text":"<p>Scenario: You have a complex task that requires multiple steps and ongoing dialogue.</p> <p>Workflow: 1. Start a conversation with the initial request 2. The AI provides information or asks clarifying questions 3. Continue the conversation with follow-ups and refinements 4. The agent maintains context throughout the session</p>"},{"location":"user/app_use_cases.html#use-case-6-multi-provider-llm-management","title":"Use Case 6: Multi-Provider LLM Management","text":"<p>Scenario: Managing multiple LLM providers and models for different purposes.</p> <p>Workflow: 1. Configure LLM providers (e.g., Ollama for development, Azure AI for production) 2. Create multiple LLM models under different providers 3. Designate one model as the default for general use 4. Use provider-specific configurations for specialized tasks</p> <p>Use Cases for Multiple Providers: - Ollama (llama2): Local development and testing without API costs - OpenAI (gpt-4o): Fast, production-quality responses - Azure AI (gpt-4-turbo): Enterprise deployment with Azure compliance - Anthropic (claude-3-opus): Complex reasoning tasks requiring deep analysis</p>"},{"location":"user/app_use_cases.html#use-case-7-middleware-configuration","title":"Use Case 7: Middleware Configuration","text":"<p>Scenario: Applying middleware to models for caching, logging, validation, and retry logic.</p> <p>Workflow: 1. Create middleware types (built-in or custom) 2. Configure middleware instances for specific models 3. Set execution priorities (lower numbers execute first) 4. Enable/disable middleware as needed without deletion</p> <p>Common Middleware Chains:</p> <p>Production Model (gpt-4o): - Priority 10: LoggingMiddleware (request/response tracking) - Priority 20: CacheMiddleware (reduce API calls, 1-hour TTL) - Priority 30: RetryMiddleware (3 retries with exponential backoff) - Priority 40: ValidationMiddleware (input/output validation)</p> <p>Development Model (ollama:llama2): - Priority 10: LoggingMiddleware (verbose debugging) - Priority 20: ValidationMiddleware (input validation only)</p>"},{"location":"user/app_use_cases.html#use-case-8-provider-specific-deployments","title":"Use Case 8: Provider-Specific Deployments","text":"<p>Scenario: Different environments use different LLM providers.</p> <p>Deployment Examples:</p> <p>Local Development: <pre><code># Ollama provider for cost-free local testing\nProvider: Ollama\nModel: llama2\nEndpoint: http://localhost:11434\nMiddleware: LoggingMiddleware only\n</code></pre></p> <p>Non-Production: <pre><code># Azure AI with caching for cost control\nProvider: Azure AI\nModel: gpt-3.5-turbo\nEndpoint: https://nonprod.openai.azure.com/\nMiddleware: LoggingMiddleware, CacheMiddleware (4-hour TTL)\n</code></pre></p> <p>Production: <pre><code># Azure AI with full middleware stack\nProvider: Azure AI\nModel: gpt-4o\nEndpoint: https://prod.openai.azure.com/\nMiddleware: LoggingMiddleware, CacheMiddleware, RetryMiddleware, ValidationMiddleware\n</code></pre></p>"},{"location":"user/app_use_cases.html#tips-for-effective-use","title":"Tips for Effective Use","text":""},{"location":"user/app_use_cases.html#getting-better-responses","title":"Getting Better Responses","text":"<ol> <li>Be Specific: Provide clear, detailed questions</li> <li>Provide Context: Include relevant information about your environment</li> <li>Use Follow-ups: Don't hesitate to ask for clarification or more details</li> <li>Try Different Phrasings: If you don't get the answer you need, rephrase your question</li> </ol>"},{"location":"user/app_use_cases.html#leveraging-conversation-history","title":"Leveraging Conversation History","text":"<ul> <li>The agent remembers previous messages in the same session</li> <li>Reference earlier parts of the conversation naturally</li> <li>Build on previous responses to dig deeper into topics</li> <li>Start fresh by clearing your session when changing topics</li> </ul>"},{"location":"user/app_use_cases.html#provider-selection","title":"Provider Selection","text":"<ul> <li>Ollama: Best for local development, no API costs, privacy</li> <li>OpenAI: Fast responses, good for general tasks</li> <li>Azure AI: Enterprise features, compliance, SLAs</li> <li>Anthropic: Strong reasoning, context handling</li> <li>HuggingFace: Access to open-source models</li> </ul>"},{"location":"user/app_use_cases.html#middleware-best-practices","title":"Middleware Best Practices","text":"<ul> <li>Logging: Always enable for production debugging</li> <li>Caching: Use for models with API costs to reduce expenses</li> <li>Retry: Critical for production reliability</li> <li>Validation: Essential for security and data integrity</li> <li>Priority Order: Logging (10) \u2192 Cache (20) \u2192 Retry (30) \u2192 Validation (40)</li> </ul>"},{"location":"user/app_use_cases.html#limitations-and-considerations","title":"Limitations and Considerations","text":"<ul> <li>Model Knowledge Cutoff: LLM models have a training data cutoff date</li> <li>Context Window: Very long conversations may exceed context limits</li> <li>Rate Limits: LLM provider APIs have rate limits that may affect response times</li> <li>Provider Costs: OpenAI, Azure AI, and Anthropic have per-token costs</li> <li>Ollama Performance: Local models may be slower than cloud providers</li> <li>Accuracy: Always verify critical information from AI responses</li> <li>MCP Server Dependency: Some capabilities require healthy MCP servers</li> <li>Middleware Overhead: Each middleware adds processing time</li> </ul>"},{"location":"user/app_use_cases.html#provider-specific-notes","title":"Provider-Specific Notes","text":""},{"location":"user/app_use_cases.html#ollama","title":"Ollama","text":"<ul> <li>Free and private</li> <li>Requires local installation</li> <li>Performance depends on hardware</li> <li>Good for development and testing</li> </ul>"},{"location":"user/app_use_cases.html#openai","title":"OpenAI","text":"<ul> <li>Pay-per-use pricing</li> <li>Fast response times</li> <li>May have availability issues during high demand</li> <li>Regular model updates</li> </ul>"},{"location":"user/app_use_cases.html#azure-ai","title":"Azure AI","text":"<ul> <li>Enterprise SLAs</li> <li>Regional deployments</li> <li>Microsoft compliance standards</li> <li>More expensive than OpenAI direct</li> </ul>"},{"location":"user/app_use_cases.html#anthropic","title":"Anthropic","text":"<ul> <li>Strong analytical capabilities</li> <li>Longer context windows</li> <li>Good for complex reasoning</li> <li>Higher cost per token</li> </ul> <p>For additional examples and advanced usage, refer to the Developer Guide.</p>"},{"location":"user/external_interactions.html","title":"External Interactions","text":"<p>This document describes external dependencies, prerequisites, and integrations for the AI Ops App.</p>"},{"location":"user/external_interactions.html#external-system-integrations","title":"External System Integrations","text":""},{"location":"user/external_interactions.html#from-the-app-to-other-systems","title":"From the App to Other Systems","text":""},{"location":"user/external_interactions.html#azure-openai-service","title":"Azure OpenAI Service","text":"<p>The AI Ops App integrates with Azure OpenAI Service to provide LLM capabilities:</p> <ul> <li>Purpose: Large Language Model inference and chat completion</li> <li>Protocol: HTTPS REST API</li> <li>Authentication: API Key (stored in Nautobot Secrets)</li> <li>Endpoints: Configurable Azure OpenAI endpoint URLs</li> <li>Models Supported: GPT-4, GPT-4o, GPT-4-turbo, and other Azure OpenAI deployments</li> </ul> <p>Configuration Requirements: - Azure OpenAI resource provisioned - Model deployment created in Azure - API key with appropriate permissions - Network connectivity from Nautobot to Azure OpenAI endpoints</p> <p>Data Flow: 1. User sends message through chat interface 2. App retrieves LLM configuration from database 3. App constructs request with conversation history 4. Request sent to Azure OpenAI endpoint 5. Response returned and displayed to user</p>"},{"location":"user/external_interactions.html#mcp-model-context-protocol-servers","title":"MCP (Model Context Protocol) Servers","text":"<p>The app can connect to external or internal MCP servers to extend agent capabilities:</p> <ul> <li>Purpose: Provide additional tools and context to the AI agent</li> <li>Protocols: HTTP or STDIO</li> <li>Authentication: Configured per MCP server</li> <li>Health Monitoring: Automatic health checks via <code>/health</code> endpoint</li> </ul> <p>MCP Server Types: - Internal: Servers hosted within your infrastructure - External: Third-party MCP servers</p> <p>Data Flow: 1. App queries healthy MCP servers on startup 2. Agent discovers available tools from each server 3. During conversation, agent invokes tools as needed 4. Tool results incorporated into agent responses</p>"},{"location":"user/external_interactions.html#redis","title":"Redis","text":"<p>Redis is used for conversation checkpoint storage:</p> <ul> <li>Purpose: Store conversation history and agent state</li> <li>Protocol: Redis protocol</li> <li>Database: Separate database number (default: DB 2)</li> <li>Configuration: Shared with Nautobot's Redis infrastructure</li> </ul> <p>Data Stored: - Conversation messages per session - Agent state and intermediate results - Checkpoint metadata</p>"},{"location":"user/external_interactions.html#from-other-systems-to-the-app","title":"From Other Systems to the App","text":""},{"location":"user/external_interactions.html#rest-api-clients","title":"REST API Clients","text":"<p>External systems can interact with the AI Ops App via the Nautobot REST API:</p> <p>Available Endpoints:</p> <ul> <li><code>GET /api/plugins/ai-ops/llm-models/</code> - List LLM models</li> <li><code>POST /api/plugins/ai-ops/llm-models/</code> - Create LLM model</li> <li><code>GET /api/plugins/ai-ops/llm-models/{id}/</code> - Get model details</li> <li><code>PATCH /api/plugins/ai-ops/llm-models/{id}/</code> - Update model</li> <li><code>DELETE /api/plugins/ai-ops/llm-models/{id}/</code> - Delete model</li> <li><code>GET /api/plugins/ai-ops/mcp-servers/</code> - List MCP servers</li> <li><code>POST /api/plugins/ai-ops/mcp-servers/</code> - Create MCP server</li> <li><code>GET /api/plugins/ai-ops/mcp-servers/{id}/</code> - Get server details</li> <li><code>PATCH /api/plugins/ai-ops/mcp-servers/{id}/</code> - Update server</li> <li><code>DELETE /api/plugins/ai-ops/mcp-servers/{id}/</code> - Delete server</li> </ul> <p>Authentication: Token-based authentication (Nautobot API tokens)</p>"},{"location":"user/external_interactions.html#chat-api","title":"Chat API","text":"<p>Applications can send chat messages programmatically:</p> <ul> <li><code>POST /plugins/ai-ops/api/chat/</code> - Send message to AI agent</li> </ul> <p>Request Format: <pre><code>{\n  \"message\": \"Your question here\"\n}\n</code></pre></p> <p>Response Format: <pre><code>{\n  \"response\": \"AI agent response\",\n  \"thread_id\": \"session-identifier\"\n}\n</code></pre></p>"},{"location":"user/external_interactions.html#nautobot-rest-api-endpoints","title":"Nautobot REST API Endpoints","text":""},{"location":"user/external_interactions.html#authentication","title":"Authentication","text":"<p>All API requests require authentication using Nautobot API tokens:</p> <pre><code>curl -H \"Authorization: Token YOUR_API_TOKEN\" \\\n  https://nautobot.example.com/api/plugins/ai-ops/llm-models/\n</code></pre>"},{"location":"user/external_interactions.html#llm-model-endpoints","title":"LLM Model Endpoints","text":""},{"location":"user/external_interactions.html#list-all-llm-models","title":"List All LLM Models","text":"<p>Request: <pre><code>curl -X GET https://nautobot.example.com/api/plugins/ai-ops/llm-models/ \\\n  -H \"Authorization: Token YOUR_API_TOKEN\" \\\n  -H \"Accept: application/json\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"count\": 2,\n  \"results\": [\n    {\n      \"id\": \"uuid\",\n      \"name\": \"gpt-4o\",\n      \"description\": \"GPT-4 Optimized\",\n      \"azure_endpoint\": \"https://your-resource.openai.azure.com/\",\n      \"api_version\": \"2024-02-15-preview\",\n      \"is_default\": true,\n      \"temperature\": 0.0\n    }\n  ]\n}\n</code></pre></p>"},{"location":"user/external_interactions.html#create-llm-model","title":"Create LLM Model","text":"<p>Request: <pre><code>curl -X POST https://nautobot.example.com/api/plugins/ai-ops/llm-models/ \\\n  -H \"Authorization: Token YOUR_API_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"gpt-4-turbo\",\n    \"description\": \"Fast GPT-4 model\",\n    \"model_secret_key\": \"azure_gpt4_api_key\",\n    \"azure_endpoint\": \"https://your-resource.openai.azure.com/\",\n    \"api_version\": \"2024-02-15-preview\",\n    \"is_default\": false,\n    \"temperature\": 0.3\n  }'\n</code></pre></p>"},{"location":"user/external_interactions.html#update-llm-model","title":"Update LLM Model","text":"<p>Request: <pre><code>curl -X PATCH https://nautobot.example.com/api/plugins/ai-ops/llm-models/{id}/ \\\n  -H \"Authorization: Token YOUR_API_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"temperature\": 0.5,\n    \"description\": \"Updated description\"\n  }'\n</code></pre></p>"},{"location":"user/external_interactions.html#mcp-server-endpoints","title":"MCP Server Endpoints","text":""},{"location":"user/external_interactions.html#list-all-mcp-servers","title":"List All MCP Servers","text":"<p>Request: <pre><code>curl -X GET https://nautobot.example.com/api/plugins/ai-ops/mcp-servers/ \\\n  -H \"Authorization: Token YOUR_API_TOKEN\" \\\n  -H \"Accept: application/json\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"count\": 1,\n  \"results\": [\n    {\n      \"id\": \"uuid\",\n      \"name\": \"internal-mcp-1\",\n      \"status\": {\"name\": \"Healthy\"},\n      \"protocol\": \"http\",\n      \"url\": \"https://mcp-server.internal.com\",\n      \"health_check\": \"/health\",\n      \"mcp_type\": \"internal\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"user/external_interactions.html#create-mcp-server","title":"Create MCP Server","text":"<p>Request: <pre><code>curl -X POST https://nautobot.example.com/api/plugins/ai-ops/mcp-servers/ \\\n  -H \"Authorization: Token YOUR_API_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"new-mcp-server\",\n    \"protocol\": \"http\",\n    \"url\": \"https://mcp.example.com\",\n    \"health_check\": \"/health\",\n    \"description\": \"External MCP server\",\n    \"mcp_type\": \"external\",\n    \"status\": \"healthy-status-id\"\n  }'\n</code></pre></p>"},{"location":"user/external_interactions.html#python-examples","title":"Python Examples","text":""},{"location":"user/external_interactions.html#using-python-requests-library","title":"Using Python Requests Library","text":"<pre><code>import requests\n\n# Configuration\nBASE_URL = \"https://nautobot.example.com\"\nAPI_TOKEN = \"your-api-token-here\"\nheaders = {\n    \"Authorization\": f\"Token {API_TOKEN}\",\n    \"Content-Type\": \"application/json\",\n}\n\n# List all LLM models\nresponse = requests.get(\n    f\"{BASE_URL}/api/plugins/ai-ops/llm-models/\",\n    headers=headers\n)\nmodels = response.json()\nprint(f\"Found {models['count']} models\")\n\n# Get default model\ndefault_model = next(\n    (m for m in models['results'] if m['is_default']), \n    None\n)\nif default_model:\n    print(f\"Default model: {default_model['name']}\")\n\n# Create new MCP server\nnew_server = {\n    \"name\": \"monitoring-mcp\",\n    \"protocol\": \"http\",\n    \"url\": \"https://monitoring.internal.com\",\n    \"health_check\": \"/health\",\n    \"description\": \"Monitoring tools MCP\",\n    \"mcp_type\": \"internal\"\n}\nresponse = requests.post(\n    f\"{BASE_URL}/api/plugins/ai-ops/mcp-servers/\",\n    headers=headers,\n    json=new_server\n)\nif response.status_code == 201:\n    print(\"MCP server created successfully\")\n    server = response.json()\n    print(f\"Server ID: {server['id']}\")\n\n# Send chat message\nchat_response = requests.post(\n    f\"{BASE_URL}/plugins/ai-ops/api/chat/\",\n    headers=headers,\n    json={\"message\": \"What is the status of my infrastructure?\"}\n)\nif chat_response.status_code == 200:\n    result = chat_response.json()\n    print(f\"AI Response: {result['response']}\")\n</code></pre>"},{"location":"user/external_interactions.html#using-pynautobot","title":"Using pynautobot","text":"<pre><code>from pynautobot import api\n\n# Initialize API connection\nnautobot = api(\n    url=\"https://nautobot.example.com\",\n    token=\"your-api-token-here\"\n)\n\n# Access AI Ops plugin endpoints\nai_ops = nautobot.plugins.ai_ops\n\n# List LLM models\nmodels = ai_ops.llm_models.all()\nfor model in models:\n    print(f\"Model: {model.name} - Default: {model.is_default}\")\n\n# Create MCP server\nnew_server = ai_ops.mcp_servers.create(\n    name=\"analytics-mcp\",\n    protocol=\"http\",\n    url=\"https://analytics.internal.com\",\n    health_check=\"/health\",\n    mcp_type=\"internal\"\n)\nprint(f\"Created server: {new_server.name}\")\n\n# Update server\nnew_server.description = \"Analytics and reporting MCP\"\nnew_server.save()\n</code></pre>"},{"location":"user/external_interactions.html#network-requirements","title":"Network Requirements","text":""},{"location":"user/external_interactions.html#firewall-rules","title":"Firewall Rules","text":"<p>Outbound from Nautobot: - Azure OpenAI endpoints (HTTPS/443) - MCP server endpoints (protocol-specific ports)</p> <p>Inbound to Nautobot: - Standard Nautobot ports for API access - No additional ports required for AI Ops</p>"},{"location":"user/external_interactions.html#dns-requirements","title":"DNS Requirements","text":"<ul> <li>Azure OpenAI endpoint resolution</li> <li>MCP server endpoint resolution</li> <li>Standard Nautobot DNS requirements</li> </ul>"},{"location":"user/external_interactions.html#security-considerations","title":"Security Considerations","text":""},{"location":"user/external_interactions.html#api-key-management","title":"API Key Management","text":"<ul> <li>Store Azure OpenAI API keys in Nautobot Secrets</li> <li>Use Secret providers appropriate for your environment</li> <li>Rotate API keys according to security policy</li> <li>Never commit API keys to source control</li> </ul>"},{"location":"user/external_interactions.html#mcp-server-security","title":"MCP Server Security","text":"<ul> <li>Use HTTPS for HTTP-based MCP servers</li> <li>Implement authentication on MCP servers</li> <li>Limit MCP server access to trusted networks</li> <li>Monitor MCP server access logs</li> </ul>"},{"location":"user/external_interactions.html#api-access-control","title":"API Access Control","text":"<ul> <li>Use Nautobot's permission system for API access</li> <li>Create dedicated API tokens for external integrations</li> <li>Limit token permissions to required operations</li> <li>Regularly audit API token usage</li> </ul>"},{"location":"user/external_interactions.html#data-privacy","title":"Data Privacy","text":"<ul> <li>Conversation history stored in Redis</li> <li>Configure appropriate Redis retention policies</li> <li>Consider data residency requirements for Azure OpenAI</li> <li>Review and comply with data protection regulations</li> </ul>"},{"location":"user/external_interactions.html#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"user/external_interactions.html#health-checks","title":"Health Checks","text":"<p>Monitor these components:</p> <ol> <li>Azure OpenAI Connectivity: Test API endpoint accessibility</li> <li>MCP Server Health: Check server status in UI or via API</li> <li>Redis Connectivity: Verify checkpoint storage is working</li> <li>API Response Times: Monitor for performance issues</li> </ol>"},{"location":"user/external_interactions.html#common-issues","title":"Common Issues","text":"<p>API Key Errors: - Verify Secret configuration - Check API key permissions in Azure - Ensure API key is not expired</p> <p>MCP Server Connection Failures: - Check server URL accessibility - Verify health check endpoint - Review firewall rules - Check MCP server logs</p> <p>Slow Response Times: - Check Azure OpenAI rate limits - Monitor Redis performance - Review MCP server response times - Consider adjusting timeout settings</p> <p>For additional troubleshooting, see the FAQ.</p>"},{"location":"user/faq.html","title":"Frequently Asked Questions","text":""},{"location":"user/faq.html#general-questions","title":"General Questions","text":""},{"location":"user/faq.html#what-is-the-ai-ops-app","title":"What is the AI Ops App?","text":"<p>The AI Ops App is a Nautobot plugin that integrates Large Language Models (LLMs) from Azure OpenAI with Nautobot to provide AI-powered assistance for network operations tasks. It uses the LangChain and LangGraph frameworks to create conversational AI agents that can be extended with Model Context Protocol (MCP) servers.</p>"},{"location":"user/faq.html#what-azure-openai-models-are-supported","title":"What Azure OpenAI models are supported?","text":"<p>The app supports any Azure OpenAI deployment, including: - GPT-4 - GPT-4o (Optimized) - GPT-4-turbo - GPT-3.5-turbo - Any custom Azure OpenAI deployments</p> <p>You configure these through the LLM Models interface in Nautobot.</p>"},{"location":"user/faq.html#do-i-need-azure-openai-to-use-this-app","title":"Do I need Azure OpenAI to use this app?","text":"<p>Yes, the app currently requires an Azure OpenAI service subscription and at least one deployed model. The app does not support OpenAI's public API directly, only Azure OpenAI endpoints.</p>"},{"location":"user/faq.html#what-is-mcp-model-context-protocol","title":"What is MCP (Model Context Protocol)?","text":"<p>MCP (Model Context Protocol) is a protocol that allows AI agents to connect to external services and tools. MCP servers provide additional capabilities to the AI agent, such as access to databases, APIs, file systems, or custom business logic.</p> <p>MCP servers are optional but extend the agent's capabilities beyond basic conversation.</p>"},{"location":"user/faq.html#installation-and-configuration","title":"Installation and Configuration","text":""},{"location":"user/faq.html#how-do-i-get-started-with-the-app","title":"How do I get started with the app?","text":"<ol> <li>Install the app via pip: <code>pip install ai-ops</code></li> <li>Add <code>\"ai_ops\"</code> to the <code>PLUGINS</code> list in <code>nautobot_config.py</code></li> <li>Run <code>nautobot-server post_upgrade</code></li> <li>Restart Nautobot services</li> <li>Configure at least one LLM Model through the UI</li> <li>Access the AI Chat Assistant from the menu</li> </ol> <p>See the Installation Guide for detailed instructions.</p>"},{"location":"user/faq.html#where-should-i-store-azure-openai-api-keys","title":"Where should I store Azure OpenAI API keys?","text":"<p>In production environments, always store API keys in Nautobot Secrets:</p> <ol> <li>Navigate to Secrets &gt; Secrets in Nautobot</li> <li>Create a new Secret with your API key</li> <li>Reference the Secret name in your LLM Model configuration</li> </ol> <p>In LAB/development environments, you can use environment variables, but Secrets are recommended for all environments.</p>"},{"location":"user/faq.html#how-do-i-configure-multiple-llm-models","title":"How do I configure multiple LLM models?","text":"<ol> <li>Navigate to AI Platform &gt; Configuration &gt; LLM Models</li> <li>Create each model with its specific configuration</li> <li>Mark one model as \"default\" by checking the \"Is Default\" checkbox</li> <li>The default model is used when no specific model is requested</li> </ol> <p>Different models can be used for different purposes (e.g., fast model for quick queries, detailed model for analysis).</p>"},{"location":"user/faq.html#what-redis-configuration-is-required","title":"What Redis configuration is required?","text":"<p>The app uses Redis for conversation checkpointing. Configuration:</p> <ul> <li>Uses the same Redis instance as Nautobot's cache and Celery</li> <li>Requires a separate database number (default: DB 2)</li> <li>Configure via environment variables:</li> <li><code>NAUTOBOT_REDIS_HOST</code></li> <li><code>NAUTOBOT_REDIS_PORT</code></li> <li><code>NAUTOBOT_REDIS_PASSWORD</code></li> <li><code>LANGGRAPH_REDIS_DB</code> (defaults to \"2\")</li> </ul> <p>No additional Redis infrastructure needed beyond what Nautobot already uses.</p>"},{"location":"user/faq.html#usage-questions","title":"Usage Questions","text":""},{"location":"user/faq.html#how-do-i-use-the-ai-chat-assistant","title":"How do I use the AI Chat Assistant?","text":"<ol> <li>Navigate to AI Platform &gt; Chat &amp; Assistance &gt; AI Chat Assistant</li> <li>Type your question in the input box</li> <li>Press Enter or click Send</li> <li>The AI responds with assistance</li> <li>Continue the conversation - context is maintained</li> </ol> <p>See Getting Started for detailed usage instructions.</p>"},{"location":"user/faq.html#does-the-ai-remember-previous-messages","title":"Does the AI remember previous messages?","text":"<p>Yes, the app maintains conversation history within a session:</p> <ul> <li>Each browser session has a unique thread ID</li> <li>Messages within that session are remembered</li> <li>Context is maintained across multiple turns</li> <li>Starting a new session (new browser tab, clearing cookies) starts fresh</li> </ul>"},{"location":"user/faq.html#how-long-is-conversation-history-kept","title":"How long is conversation history kept?","text":"<p>Conversation history is stored in Redis with configurable retention:</p> <ul> <li>By default, checkpoints older than 30 days are eligible for cleanup</li> <li>Run the \"Cleanup Old Checkpoints\" job to remove old conversations</li> <li>Schedule the job to run automatically (recommended: daily or weekly)</li> <li>Active conversations are never removed</li> </ul>"},{"location":"user/faq.html#can-i-use-the-ai-agent-via-api","title":"Can I use the AI agent via API?","text":"<p>Yes, the app provides a REST API endpoint for programmatic access:</p> <pre><code>POST /plugins/ai-ops/api/chat/\nContent-Type: application/json\n\n{\n  \"message\": \"Your question here\"\n}\n</code></pre> <p>See External Interactions for API documentation and examples.</p>"},{"location":"user/faq.html#what-permissions-are-required-to-use-the-app","title":"What permissions are required to use the app?","text":"<ul> <li><code>ai_ops.view_llmmodel</code> - View LLM models</li> <li><code>ai_ops.add_llmmodel</code> - Create LLM models</li> <li><code>ai_ops.change_llmmodel</code> - Edit LLM models</li> <li><code>ai_ops.delete_llmmodel</code> - Delete LLM models</li> <li><code>ai_ops.view_mcpserver</code> - View MCP servers (also grants chat access)</li> <li><code>ai_ops.add_mcpserver</code> - Create MCP servers</li> <li><code>ai_ops.change_mcpserver</code> - Edit MCP servers</li> <li><code>ai_ops.delete_mcpserver</code> - Delete MCP servers</li> </ul> <p>Typically, users need <code>view_mcpserver</code> permission to access the chat interface.</p>"},{"location":"user/faq.html#mcp-server-questions","title":"MCP Server Questions","text":""},{"location":"user/faq.html#what-are-mcp-servers-used-for","title":"What are MCP servers used for?","text":"<p>MCP servers extend the AI agent's capabilities by providing:</p> <ul> <li>Additional tools the agent can use</li> <li>Access to external systems and APIs</li> <li>Custom business logic and workflows</li> <li>Integration with internal services</li> </ul> <p>Examples: code execution, file access, database queries, monitoring system integration.</p>"},{"location":"user/faq.html#are-mcp-servers-required","title":"Are MCP servers required?","text":"<p>No, MCP servers are optional. The AI Chat Assistant works without them, providing conversational assistance based on the LLM's training. MCP servers are only needed if you want to extend the agent with additional capabilities.</p>"},{"location":"user/faq.html#how-do-i-know-if-an-mcp-server-is-working","title":"How do I know if an MCP server is working?","text":"<p>Check the MCP server status:</p> <ol> <li>Navigate to AI Platform &gt; Configuration &gt; MCP Servers</li> <li>Check the Status column</li> <li>\"Healthy\" status means the server is working</li> <li>\"Failed\" status indicates connection issues</li> </ol> <p>Health checks run automatically. Failed servers are excluded from agent operations.</p>"},{"location":"user/faq.html#can-i-use-external-mcp-servers","title":"Can I use external MCP servers?","text":"<p>Yes, you can configure both:</p> <ul> <li>Internal MCP servers: Hosted within your infrastructure</li> <li>External MCP servers: Third-party or cloud-hosted services</li> </ul> <p>Set the \"MCP Type\" field accordingly when creating the server configuration.</p>"},{"location":"user/faq.html#what-protocols-do-mcp-servers-use","title":"What protocols do MCP servers use?","text":"<p>The app supports:</p> <ul> <li>HTTP: RESTful MCP servers (most common)</li> <li>STDIO: Process-based MCP servers</li> </ul> <p>Most deployments use HTTP MCP servers.</p>"},{"location":"user/faq.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"user/faq.html#the-chat-interface-isnt-responding-what-should-i-check","title":"The chat interface isn't responding. What should I check?","text":"<ol> <li>Verify LLM Model Configuration:</li> <li>At least one model exists</li> <li>One model is marked as default</li> <li> <p>API credentials are correct</p> </li> <li> <p>Check Nautobot Logs:</p> </li> <li>Look for error messages</li> <li> <p>Check for API key or connection issues</p> </li> <li> <p>Test Azure OpenAI Connectivity:</p> </li> <li>Verify the endpoint URL is accessible</li> <li>Confirm API key has proper permissions</li> <li> <p>Check for Azure service issues</p> </li> <li> <p>Verify Permissions:</p> </li> <li>User has <code>ai_ops.view_mcpserver</code> permission</li> </ol>"},{"location":"user/faq.html#my-mcp-server-shows-failed-status-how-do-i-fix-it","title":"My MCP server shows \"Failed\" status. How do I fix it?","text":"<ol> <li>Verify URL Accessibility:</li> <li>Test the URL from the Nautobot server</li> <li>Ensure network connectivity</li> <li> <p>Check firewall rules</p> </li> <li> <p>Check Health Endpoint:</p> </li> <li>Verify the health check path is correct</li> <li>Test: <code>curl https://mcp-server.example.com/health</code></li> <li> <p>Health check should return HTTP 200</p> </li> <li> <p>Review Server Logs:</p> </li> <li>Check MCP server logs for errors</li> <li>Look for authentication issues</li> <li> <p>Verify server is running</p> </li> <li> <p>Update Status Manually (if needed):</p> </li> <li>Edit the MCP server</li> <li>Change status to \"Maintenance\" while troubleshooting</li> <li>Change back to \"Healthy\" when fixed</li> </ol>"},{"location":"user/faq.html#conversation-history-is-not-persisting-whats-wrong","title":"Conversation history is not persisting. What's wrong?","text":"<ol> <li>Check Redis Configuration:</li> <li>Verify Redis is running</li> <li>Test Redis connectivity</li> <li> <p>Check <code>LANGGRAPH_REDIS_DB</code> setting</p> </li> <li> <p>Review Environment Variables:</p> </li> <li><code>NAUTOBOT_REDIS_HOST</code></li> <li><code>NAUTOBOT_REDIS_PORT</code></li> <li> <p><code>NAUTOBOT_REDIS_PASSWORD</code></p> </li> <li> <p>Check Redis Database:</p> </li> <li>Ensure database number is not in use by another service</li> <li> <p>Default: DB 2 (DB 0: cache, DB 1: Celery)</p> </li> <li> <p>Review Logs:</p> </li> <li>Look for checkpoint-related errors</li> <li>Check Redis connection errors</li> </ol>"},{"location":"user/faq.html#im-getting-azure-openai-rate-limit-errors-what-should-i-do","title":"I'm getting Azure OpenAI rate limit errors. What should I do?","text":"<p>Azure OpenAI has rate limits based on your subscription:</p> <ol> <li>Check Azure Portal:</li> <li>Review your quota and rate limits</li> <li> <p>Monitor current usage</p> </li> <li> <p>Request Quota Increase:</p> </li> <li>Submit a request in Azure Portal</li> <li> <p>Specify your use case and needs</p> </li> <li> <p>Optimize Usage:</p> </li> <li>Use lower temperature for deterministic responses (faster)</li> <li>Configure multiple models to distribute load</li> <li> <p>Implement retry logic in custom integrations</p> </li> <li> <p>Contact Azure Support:</p> </li> <li>For persistent rate limit issues</li> <li>To discuss enterprise quotas</li> </ol>"},{"location":"user/faq.html#the-ai-is-giving-incorrect-or-outdated-information-why","title":"The AI is giving incorrect or outdated information. Why?","text":"<p>LLM models have limitations:</p> <ol> <li>Training Data Cutoff:</li> <li>Models are trained on data up to a certain date</li> <li>They don't have real-time information</li> <li> <p>Check your model's training data date</p> </li> <li> <p>Hallucinations:</p> </li> <li>LLMs can generate plausible but incorrect information</li> <li>Always verify critical information</li> <li> <p>Use MCP servers for real-time data access</p> </li> <li> <p>Context Limitations:</p> </li> <li>Very long conversations may exceed context window</li> <li>Start a new conversation for fresh context</li> <li> <p>Break complex tasks into smaller conversations</p> </li> <li> <p>Model Selection:</p> </li> <li>Different models have different capabilities</li> <li>GPT-4 is generally more accurate than GPT-3.5</li> <li>Adjust model selection based on task requirements</li> </ol>"},{"location":"user/faq.html#performance-questions","title":"Performance Questions","text":""},{"location":"user/faq.html#how-can-i-improve-response-times","title":"How can I improve response times?","text":"<ol> <li>Use Faster Models:</li> <li>GPT-4-turbo is faster than GPT-4</li> <li> <p>GPT-3.5-turbo is fastest but less capable</p> </li> <li> <p>Optimize Temperature:</p> </li> <li>Lower temperature (0.0-0.3) can be faster</li> <li> <p>Higher temperature requires more generation time</p> </li> <li> <p>Reduce MCP Server Count:</p> </li> <li>Fewer MCP servers = faster tool discovery</li> <li> <p>Disable unused MCP servers</p> </li> <li> <p>Monitor Azure Performance:</p> </li> <li>Check Azure OpenAI service status</li> <li>Review your region selection</li> <li>Consider deploying models in multiple regions</li> </ol>"},{"location":"user/faq.html#how-much-does-it-cost-to-run-this-app","title":"How much does it cost to run this app?","text":"<p>Costs depend on Azure OpenAI usage:</p> <ol> <li>Azure OpenAI Charges:</li> <li>Pay per token (input and output)</li> <li>Varies by model (GPT-4 more expensive than GPT-3.5)</li> <li> <p>Check Azure OpenAI pricing page</p> </li> <li> <p>Infrastructure Costs:</p> </li> <li>Nautobot hosting (unchanged)</li> <li>Redis (minimal - shared with existing Nautobot Redis)</li> <li> <p>MCP servers (if self-hosted)</p> </li> <li> <p>Cost Optimization:</p> </li> <li>Use GPT-3.5-turbo for simple queries</li> <li>Reserve GPT-4 for complex tasks</li> <li>Monitor usage in Azure Portal</li> <li>Set up budget alerts</li> </ol>"},{"location":"user/faq.html#advanced-questions","title":"Advanced Questions","text":""},{"location":"user/faq.html#can-i-customize-the-ai-agents-behavior","title":"Can I customize the AI agent's behavior?","text":"<p>The agent's behavior is defined by system prompts in the code:</p> <ul> <li><code>ai_ops/prompts/multi_mcp_system_prompt.py</code></li> <li><code>ai_ops/prompts/system_prompt.py</code></li> </ul> <p>Modifying these requires code changes and app redeployment. See the Developer Guide for details.</p>"},{"location":"user/faq.html#can-i-use-different-llm-providers-besides-azure-openai","title":"Can I use different LLM providers besides Azure OpenAI?","text":"<p>Currently, the app is designed for Azure OpenAI. Supporting other providers would require code modifications:</p> <ul> <li>Modify <code>ai_ops/helpers/get_azure_model.py</code></li> <li>Update model configuration in <code>ai_ops/models.py</code></li> <li>Adjust API calls in agent code</li> </ul> <p>This is not currently supported out-of-the-box.</p>"},{"location":"user/faq.html#how-is-conversation-data-secured","title":"How is conversation data secured?","text":"<p>Security measures:</p> <ol> <li>API Keys: Stored in Nautobot Secrets</li> <li>Conversation Data: Stored in Redis (encrypted in transit)</li> <li>Access Control: Nautobot permission system</li> <li>Audit Trails: All actions logged in Nautobot</li> </ol> <p>Review the External Interactions security section for details.</p>"},{"location":"user/faq.html#can-i-deploy-the-app-in-an-air-gapped-environment","title":"Can I deploy the app in an air-gapped environment?","text":"<p>Partial air-gapped deployment is possible:</p> <p>Possible: - Install app from pip package (downloaded elsewhere) - Use self-hosted MCP servers - Internal Redis</p> <p>Not Possible Without Workarounds: - Requires connectivity to Azure OpenAI endpoints - Azure OpenAI doesn't support on-premises deployment</p> <p>Workarounds: - Use Azure Private Link for Azure OpenAI - Configure proxy for Azure connectivity - Consider Azure Government Cloud for sensitive environments</p>"},{"location":"user/faq.html#how-do-i-backup-conversation-history","title":"How do I backup conversation history?","text":"<p>Conversation history is stored in Redis:</p> <ol> <li>Redis Backup:</li> <li>Use Redis persistence (RDB or AOF)</li> <li>Regular Redis backups</li> <li> <p>Include LANGGRAPH_REDIS_DB in backup scope</p> </li> <li> <p>Cleanup Considerations:</p> </li> <li>Old conversations removed by cleanup job</li> <li> <p>Backup before running cleanup if history is important</p> </li> <li> <p>Alternative Storage:</p> </li> <li>For permanent archival, consider custom development</li> <li>Export conversations to file or database</li> <li>Not built-in to current version</li> </ol>"},{"location":"user/faq.html#getting-help","title":"Getting Help","text":""},{"location":"user/faq.html#where-can-i-find-more-documentation","title":"Where can I find more documentation?","text":"<ul> <li>App Overview - Feature overview</li> <li>Getting Started - Setup guide</li> <li>Use Cases - Usage examples</li> <li>External Interactions - API documentation</li> <li>Developer Guide - Customization and development</li> </ul>"},{"location":"user/faq.html#how-do-i-report-bugs-or-request-features","title":"How do I report bugs or request features?","text":"<p>Open an issue on the GitHub repository:</p> <ul> <li>Repository: kvncampos/nautobot-ai-ops</li> <li>Include version information</li> <li>Provide reproduction steps</li> <li>Include relevant logs (sanitize sensitive data)</li> </ul>"},{"location":"user/faq.html#where-can-i-get-support","title":"Where can I get support?","text":"<ul> <li>Internal Team: See Authors and Maintainers for contact information</li> <li>GitHub Issues: For bugs and feature requests on GitHub</li> <li>Nautobot Community: For general Nautobot questions</li> </ul>"},{"location":"user/faq.html#can-i-contribute-to-the-project","title":"Can I contribute to the project?","text":"<p>Yes! Contributions are welcome:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol> <p>See the Contributing Guide for details.</p>"},{"location":"user/quick_start.html","title":"Quick Start Guide","text":"<p>Get the AI Ops App up and running in 5 minutes!</p>"},{"location":"user/quick_start.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Nautobot 2.4.22+ installed</li> <li>Azure OpenAI subscription with a deployed model</li> <li>Redis server accessible</li> <li>Admin access to Nautobot</li> </ul>"},{"location":"user/quick_start.html#step-1-install-the-app-2-minutes","title":"Step 1: Install the App (2 minutes)","text":"<pre><code># Install from PyPI\npip install ai-ops\n\n# Add to nautobot_config.py\nPLUGINS = [\"ai_ops\"]\n\n# Run migrations\nnautobot-server post_upgrade\n\n# Restart services\nsudo systemctl restart nautobot nautobot-worker\n</code></pre>"},{"location":"user/quick_start.html#step-2-create-a-secret-1-minute","title":"Step 2: Create a Secret (1 minute)","text":"<ol> <li>Navigate to Secrets &gt; Secrets in Nautobot</li> <li>Click + Add</li> <li>Fill in:</li> <li>Name: <code>azure_gpt4_api_key</code></li> <li>Provider: Choose your provider (e.g., Environment Variable)</li> <li>Parameters: Configure according to provider</li> <li>Click Create</li> </ol>"},{"location":"user/quick_start.html#step-3-configure-llm-model-1-minute","title":"Step 3: Configure LLM Model (1 minute)","text":"<ol> <li>Navigate to AI Platform &gt; Configuration &gt; LLM Models</li> <li>Click + Add</li> <li>Fill in:</li> <li>Name: <code>gpt-4o</code> (your Azure deployment name)</li> <li>Description: <code>Production GPT-4o model</code></li> <li>Model Secret Key: <code>azure_gpt4_api_key</code></li> <li>Azure Endpoint: <code>https://your-resource.openai.azure.com/</code></li> <li>API Version: <code>2024-02-15-preview</code></li> <li>Is Default: \u2611\ufe0f Check this box</li> <li>Temperature: <code>0.3</code></li> <li>Click Create</li> </ol>"},{"location":"user/quick_start.html#step-4-test-the-chat-1-minute","title":"Step 4: Test the Chat (1 minute)","text":"<ol> <li>Navigate to AI Platform &gt; Chat &amp; Assistance &gt; AI Chat Assistant</li> <li>Type a test message: <code>Hello! Can you help me?</code></li> <li>Press Enter or click Send</li> <li>You should receive a response from the AI!</li> </ol>"},{"location":"user/quick_start.html#step-5-optional-add-mcp-server","title":"Step 5: Optional - Add MCP Server","text":"<p>If you have an MCP server to connect:</p> <ol> <li>Navigate to AI Platform &gt; Configuration &gt; MCP Servers</li> <li>Click + Add</li> <li>Configure your server details</li> <li>Set Status to Healthy</li> <li>Click Create</li> </ol> <p>The agent will automatically use tools from this server!</p>"},{"location":"user/quick_start.html#thats-it","title":"That's It!","text":"<p>You're now ready to use the AI Ops App. Here's what you can do next:</p>"},{"location":"user/quick_start.html#explore-features","title":"Explore Features","text":"<ul> <li>Try asking complex questions</li> <li>Test multi-turn conversations</li> <li>Explore the REST API</li> </ul>"},{"location":"user/quick_start.html#learn-more","title":"Learn More","text":"<ul> <li>User Guide - Complete feature overview</li> <li>Use Cases - Real-world examples</li> <li>FAQ - Common questions answered</li> </ul>"},{"location":"user/quick_start.html#configure-more","title":"Configure More","text":"<ul> <li>Add additional LLM models for different use cases</li> <li>Connect more MCP servers for extended capabilities</li> <li>Schedule the cleanup job for maintenance</li> </ul>"},{"location":"user/quick_start.html#quick-troubleshooting","title":"Quick Troubleshooting","text":""},{"location":"user/quick_start.html#chat-not-responding","title":"Chat not responding?","text":"<p>Check: 1. Is the LLM Model marked as default? \u2713 2. Does the Secret exist and have the correct API key? \u2713 3. Is the Azure endpoint accessible? \u2713 4. Check Nautobot logs for errors</p> <p>Fix: Navigate to LLM Models, verify configuration, test Azure connectivity</p>"},{"location":"user/quick_start.html#no-llmmodel-instances-exist","title":"\"No LLMModel instances exist\"","text":"<p>Fix: Create at least one LLM Model and mark it as default</p>"},{"location":"user/quick_start.html#mcp-server-shows-failed","title":"MCP Server shows \"Failed\"","text":"<p>Fix:  1. Verify URL is accessible: <code>curl https://your-mcp-server/health</code> 2. Check firewall rules 3. Review MCP server logs</p>"},{"location":"user/quick_start.html#environment-variables-development-only","title":"Environment Variables (Development Only)","text":"<p>For local development, you can skip Steps 2-3 and use environment variables:</p> <pre><code># .env file\nAZURE_OPENAI_API_KEY=your-api-key\nAZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\nAZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o\nAZURE_OPENAI_API_VERSION=2024-02-15-preview\n</code></pre> <p>The app automatically detects LAB environment and uses these variables.</p>"},{"location":"user/quick_start.html#production-checklist","title":"Production Checklist","text":"<p>Before going to production:</p> <ul> <li>[ ] Store API keys in Nautobot Secrets (not environment variables)</li> <li>[ ] Configure Redis properly in <code>nautobot_config.py</code></li> <li>[ ] Set up multiple LLM models for redundancy</li> <li>[ ] Configure MCP servers with health checks</li> <li>[ ] Schedule the cleanup job (weekly recommended)</li> <li>[ ] Test error scenarios (invalid API key, network failures)</li> <li>[ ] Review security settings and permissions</li> <li>[ ] Set up monitoring for Redis and PostgreSQL</li> </ul>"},{"location":"user/quick_start.html#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Full User Guide</li> <li>FAQ: Frequently Asked Questions</li> <li>Issues: GitHub Issues</li> <li>Contact: See Authors and Maintainers</li> </ul>"},{"location":"user/quick_start.html#next-steps","title":"Next Steps","text":"<p>Now that you're up and running:</p> <ol> <li>Explore Use Cases: Check Use Cases for examples</li> <li>Configure API Access: See External Interactions</li> <li>Learn the Architecture: Review Architecture Overview</li> <li>Extend the App: Read Extending Guide</li> </ol> <p>Happy chatting! \ud83e\udd16</p>"}]}