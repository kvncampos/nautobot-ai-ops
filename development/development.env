################################################################################
# DEV File: Store environment information. NOTE: Secrets NOT stored here!
################################################################################
# Nautobot Configuration Environment Variables
NAUTOBOT_ALLOWED_HOSTS=*
NAUTOBOT_BANNER_TOP="Local"
NAUTOBOT_CHANGELOG_RETENTION=0

NAUTOBOT_DEBUG=True
NAUTOBOT_LOG_DEPRECATION_WARNINGS=True
NAUTOBOT_LOG_LEVEL=DEBUG
NAUTOBOT_METRICS_ENABLED=True
NAUTOBOT_NAPALM_TIMEOUT=5
NAUTOBOT_MAX_PAGE_SIZE=0

# Redis Configuration Environment Variables
NAUTOBOT_REDIS_HOST=redis
NAUTOBOT_REDIS_PORT=6379
# Uncomment NAUTOBOT_REDIS_SSL if using SSL
# NAUTOBOT_REDIS_SSL=True

# Nautobot DB Connection Environment Variables
NAUTOBOT_DB_NAME=nautobot
NAUTOBOT_DB_USER=nautobot
NAUTOBOT_DB_HOST=db
NAUTOBOT_DB_TIMEOUT=300

# Use them to overwrite the defaults in nautobot_config.py
# NAUTOBOT_DB_ENGINE=django.db.backends.postgresql
# NAUTOBOT_DB_PORT=5432

# Needed for Postgres should match the values for Nautobot above
POSTGRES_USER=nautobot
POSTGRES_DB=nautobot

# Needed for MYSQL should match the values for Nautobot above
MYSQL_USER=nautobot
MYSQL_DATABASE=nautobot
MYSQL_ROOT_HOST=%

# Use a less verbose log level for Celery Beat
NAUTOBOT_BEAT_LOG_LEVEL=INFO

################################################################################
# Deep Agent Configuration (deepagents framework)
################################################################################

# Redis configuration for deep agent features
# Password is set in creds.env (NAUTOBOT_REDIS_PASSWORD)
# Format: redis://[:password@]host:port[/database]
REDIS_URL=redis://:changeme@redis:6379
# Optional: Separate Redis URLs for different features with different databases
# CHECKPOINT_REDIS_URL=redis://:changeme@redis:6379/1
# STORE_REDIS_URL=redis://:changeme@redis:6379/2  
# SEMANTIC_CACHE_REDIS_URL=redis://:changeme@redis:6379/3

# PostgreSQL connection for checkpointer (alternative to Redis)
# DATABASE_URL=postgresql://nautobot:password@db:5432/nautobot

# Checkpointer Configuration
CHECKPOINT_TTL=3600  # Checkpoint expiration in seconds (1 hour)
CHECKPOINT_POOL_SIZE=10  # Connection pool max size
CHECKPOINT_POOL_MIN_SIZE=2  # Connection pool min size

# Semantic Cache Configuration
SEMANTIC_CACHE_TTL=3600  # Cache TTL in seconds (1 hour)
SEMANTIC_CACHE_THRESHOLD=0.05  # Distance threshold (0-1, lower = stricter)

# Tool Retry Configuration
TOOL_MAX_RETRIES=2  # Number of retries for transient tool errors

# Embedding Configuration (for semantic cache)
# Using host machine's Ollama instance (not running in Docker)
# host.docker.internal allows Docker containers to access services on the host
EMBEDDING_MODEL=mxbai-embed-large  # Default embedding model
EMBEDDING_BASE_URL=http://host.docker.internal:11434  # Host's Ollama endpoint
# EMBEDDING_API_KEY=  # API key if using OpenAI/Azure (not needed for Ollama)

# Langfuse Observability Configuration
# Toggle to enable/disable Langfuse LLM tracing
ENABLE_LANGFUSE=true

# Langfuse Connection Settings
# For docker-compose: Use service name 'langfuse-web'
# For external Langfuse: Use actual URL
# IMPORTANT: Generate these keys from Langfuse UI after first startup:
#   1. Access http://localhost:8000
#   2. Create an account
#   3. Go to Settings > API Keys
#   4. Copy the keys and update creds.env (NOT this file)
# OR use headless initialization (see below)
LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY:-}
LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY:-}
LANGFUSE_HOST=http://langfuse-web:3000

# Langfuse Server Configuration (used by Langfuse Docker container)
LANGFUSE_TELEMETRY_ENABLED=false
LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES=false

# Langfuse Database Configuration
DATABASE_URL=postgresql://nautobot:changeme@db:5432/langfuse

# Langfuse Authentication
NEXTAUTH_URL=http://localhost:8000

# Langfuse ClickHouse Configuration
CLICKHOUSE_MIGRATION_URL=clickhouse://clickhouse:9000
CLICKHOUSE_URL=http://clickhouse:8123
CLICKHOUSE_USER=default
CLICKHOUSE_DB=default
CLICKHOUSE_CLUSTER_ENABLED=false
LANGFUSE_CLICKHOUSE_HOST=clickhouse
LANGFUSE_CLICKHOUSE_PORT=8123

# Langfuse Redis Configuration
REDIS_CONNECTION_STRING=redis://:changeme@redis:6379/0

# Storage Configuration
# Toggle between local MinIO (false) and Azure Blob (true)
LANGFUSE_USE_AZURE_BLOB=false

# Langfuse S3/MinIO Configuration - Event Upload
LANGFUSE_S3_EVENT_UPLOAD_BUCKET=langfuse-events
LANGFUSE_S3_EVENT_UPLOAD_PREFIX=""
LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT=http://minio:9000
LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID=minio
LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY=miniosecret
LANGFUSE_S3_EVENT_UPLOAD_REGION=us-east-1
LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE=true

# Langfuse S3/MinIO Configuration - Media Upload
LANGFUSE_S3_MEDIA_UPLOAD_BUCKET=langfuse-media
LANGFUSE_S3_MEDIA_UPLOAD_PREFIX=""
LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT=http://minio:9000
LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID=minio
LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY=miniosecret
LANGFUSE_S3_MEDIA_UPLOAD_REGION=us-east-1
LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE=true

# Langfuse S3/MinIO Configuration - Batch Export (optional)
LANGFUSE_S3_BATCH_EXPORT_ENABLED=false
LANGFUSE_S3_BATCH_EXPORT_BUCKET=langfuse-batch
LANGFUSE_S3_BATCH_EXPORT_PREFIX=""
LANGFUSE_S3_BATCH_EXPORT_ENDPOINT=http://minio:9000
LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID=minio
LANGFUSE_S3_BATCH_EXPORT_SECRET_ACCESS_KEY=miniosecret
LANGFUSE_S3_BATCH_EXPORT_REGION=us-east-1
LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE=true

# Azure Blob Storage (when LANGFUSE_USE_AZURE_BLOB=true)
# AZURE_BLOB_CONNECTION_STRING=
# LANGFUSE_AZURE_BLOB_CONTAINER_NAME=langfuse

# Optional: Azure AD authentication for PostgreSQL
# DB_AUTH_METHOD=basic  # or "service_principal"
# ARM_CLIENT_ID=
# ARM_CLIENT_SECRET=
# ARM_TENANT_ID=
